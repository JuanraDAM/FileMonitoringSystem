services:
  superset:
    # image: apache/superset:latest
    build:
      context: .
      dockerfile: Dockerfile.superset
    container_name: superset
    ports:
      - "8088:8088"
    environment:
      - SUPERSET_CONFIG_PATH=/app/superset_config.py
    volumes:
      - ./superset_config.py:/app/superset_config.py
    depends_on:
      - superset-db
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8088/health"]
      interval: 30s
      timeout: 10s
      retries: 5
    command: >
      /bin/sh -c "
        superset db upgrade &&
        superset fab create-admin --username admin --firstname Superset --lastname Admin --email admin@superset.com --password 1234 &&
        superset init &&
        superset run -h 0.0.0.0 -p 8088"
    networks:
      - superset-net

  superset-db:
    image: postgres:latest
    container_name: superset-db
    environment:
      POSTGRES_DB: superset
      POSTGRES_USER: superset
      POSTGRES_PASSWORD: superset
    volumes:
      - superset-db-data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    networks:
      - superset-net

  zookeeper:
    image: wurstmeister/zookeeper
    container_name: zookeeper
    ports:
      - "2181:2181"
    networks:
      - superset-net

  kafka:
    image: wurstmeister/kafka
    container_name: kafka
    ports:
      - "9092:9092"
    environment:
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_BROKER_ID: 1
    depends_on:
      - zookeeper
    networks:
      - superset-net

  hadoop-namenode:
    image: bde2020/hadoop-namenode:latest
    container_name: hadoop-namenode
    ports:
      - "9000:9000"
      - "9870:9870"
    environment:
      CLUSTER_NAME: test
      CORE_CONF_fs_defaultFS: hdfs://hadoop-namenode:9000
    volumes:
      - hadoop-namenode:/hadoop/dfs/name
    networks:
      - superset-net

  hadoop-datanode:
    image: bde2020/hadoop-datanode:latest
    container_name: hadoop-datanode
    ports:
      - "9864:9864"
    environment:
      CLUSTER_NAME: test
      CORE_CONF_fs_defaultFS: hdfs://hadoop-namenode:9000
    volumes:
      - hadoop-datanode:/hadoop/dfs/data
    depends_on:
      - hadoop-namenode
    networks:
      - superset-net

  spark-master:
    image: bitnami/spark:3.3.1
    container_name: spark-master
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:
      SPARK_MODE: master
      SPARK_RPC_AUTHENTICATION_ENABLED: "no"
      SPARK_RPC_ENCRYPTION_ENABLED: "no"
      SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED: "no"
      SPARK_SSL_ENABLED: "no"
      SPARK_MASTER_HOST: spark-master
      SPARK_MASTER_PORT: "7077"
      SPARK_MASTER_WEBUI_PORT: "8080"
    ports:
      - "7077:7077"
      - "8080:8080"
    networks:
      - superset-net
    volumes:
      - ./target/scala-2.12:/app/target/scala-2.12

  spark-worker-1:
    image: bitnami/spark:3.3.1
    container_name: spark-worker-1
    depends_on:
      - spark-master
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_WORKER_MEMORY: 12G
      SPARK_WORKER_CORES: "4"
      SPARK_WORKER_PORT: "7078"
      SPARK_WORKER_WEBUI_PORT: "8081"
    ports:
      - "8081:8081"
    networks:
      - superset-net
    volumes:
      - ./target/scala-2.12:/app/target/scala-2.12

  spark-worker-2:
    image: bitnami/spark:3.3.1
    container_name: spark-worker-2
    depends_on:
      - spark-master
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_WORKER_MEMORY: 12G
      SPARK_WORKER_CORES: "4"
      SPARK_WORKER_PORT: "7079"
      SPARK_WORKER_WEBUI_PORT: "8082"
    ports:
      - "8082:8082"
    networks:
      - superset-net
    volumes:
      - ./target/scala-2.12:/app/target/scala-2.12

  validation-engine:
    build:
      context: ..                # contexto: la raÃ­z del repositorio
      dockerfile: docker/Dockerfile.engine
    container_name: validation-engine
    depends_on:
      - spark-master
      - hadoop-namenode
      - superset-db
    environment:
      INPUT_DIR: "/data/bank_accounts"
      OUTPUT_TABLE: "trigger_control"
      POLL_INTERVAL_MS: "10000"
    extra_hosts:
      - "validation-engine:host-gateway"
    networks:
      - superset-net
    volumes:
      - ../src/main/resources/files/bank_accounts:/local_bank_accounts
    command: >
      bash -c "
        hdfs dfs -mkdir -p /data/bank_accounts &&
        hdfs dfs -put -f /local_bank_accounts/* /data/bank_accounts &&
        hdfs dfs -chmod -R 777 /data/bank_accounts &&
        spark-submit \
          --class Main \
          --master spark://spark-master:7077 \
          --deploy-mode client \
          --conf spark.driver.host=validation-engine \
          --conf spark.hadoop.fs.defaultFS=hdfs://hadoop-namenode:9000 \
          /app/app.jar
      "


volumes:
  superset-db-data:
  hadoop-namenode:
  hadoop-datanode:

networks:
  superset-net:
    driver: bridge