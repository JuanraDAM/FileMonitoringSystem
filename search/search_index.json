{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Summary \u00b6 Introducci\u00f3n Especificaci\u00f3n de Requisitos Dise\u00f1o (Diagramas) Implementaci\u00f3n (GIT) Resultado (Manual de usuario) Conclusiones","title":"Home"},{"location":"index.html#summary","text":"Introducci\u00f3n Especificaci\u00f3n de Requisitos Dise\u00f1o (Diagramas) Implementaci\u00f3n (GIT) Resultado (Manual de usuario) Conclusiones","title":"Summary"},{"location":"01-introduccion.html","text":"1. Introducci\u00f3n \u00b6 1.1 Resumen del proyecto \u00b6 El File Monitoring System es una soluci\u00f3n integral para la ingesta, validaci\u00f3n y visualizaci\u00f3n de ficheros bancarios en un entorno Big Data. Consta de tres componentes principales: Un motor de validaciones implementado en Scala y ejecutado sobre Apache Spark , que procesa ficheros almacenados en HDFS , aplica validaciones estructurales, tipol\u00f3gicas, referenciales y de negocio, y almacena los resultados en PostgreSQL . Un backend desarrollado con FastAPI (Python 3.12) que expone una API REST para gestionar usuarios, configuraciones de ficheros, subir/descargar archivos y sincronizarlos con HDFS a trav\u00e9s de WebHDFS . Un frontend construido con React 18+ y Vite , que permite al usuario autenticarse, subir CSV, editar metadatos de parseo, enviar ficheros a HDFS para validaci\u00f3n y visualizar los logs resultantes. La arquitectura est\u00e1 contenida en varios contenedores Docker orquestados mediante Docker Compose , incluyendo cl\u00faster Hadoop (NameNode/DataNode), Spark (Master/Workers), PostgreSQL, Kafka, Superset (opcional) y el propio motor de validaciones. El objetivo es ofrecer una soluci\u00f3n escalable, modular y mantenible para garantizar la calidad e integridad de los datos bancarios. 1.2 Explicaci\u00f3n de la aplicaci\u00f3n \u00b6 El flujo de trabajo global se describe a continuaci\u00f3n: Usuario (frontend) * Se registra o inicia sesi\u00f3n mediante JWT (FastAPI). * Accede a un dashboard donde puede subir un fichero CSV y gestionar metadatos de parseo (delimiter, quote_char, date_format, etc.). * Al pulsar \u201cValidar\u201d, el frontend env\u00eda el CSV al backend y \u00e9ste lo almacena localmente y registra la configuraci\u00f3n en la tabla file_configuration de PostgreSQL. Backend (FastAPI) * Recibe el CSV ( POST /files/upload ) y guarda el archivo en la carpeta local uploaded_files/ . * Crea o actualiza un registro en la tabla file_configuration con par\u00e1metros de parseo. * Cuando el usuario solicita \u201cEnviar a HDFS\u201d ( POST /files/push/{file_name} ), el backend: 1. Verifica que el archivo existe en `uploaded_files/`. 2. Llama a WebHDFS (NameNode) para crear el directorio `/data/bank_accounts` y ajustar permisos (`op=MKDIRS`, `op=SETPERM`). 3. Solicita la creaci\u00f3n del fichero en HDFS (`op=CREATE`), recuerda el redireccionamiento 307 a DataNode, ajusta la URL y sube el contenido. 4. Responde al frontend con un mensaje de \u00e9xito. Motor de validaciones (Scala + Spark) * Se ejecuta en un contenedor Docker con Spark y SBT. En el ENTRYPOINT , al arrancar: 1. Ejecuta `hdfs dfsadmin -safemode leave` para salir de safe mode. 2. Crea la carpeta `/data/bank_accounts` en HDFS (si no existe). 3. Copia los CSV desde un volumen local (`/local_bank_accounts`) a HDFS. 4. Inicia `spark-submit` que corre la clase `Main.scala`. * Main.scala : 1. Configura un bucle de polling que observa el directorio HDFS (`/data/bank_accounts`). 2. Al detectar un fichero, llama a `ExecutionManager.executeFile(path, outputTable)`. 3. Dentro de `ExecutionManager`: * Lee el CSV con `Reader.readFile(...)` en un DataFrame particionado. * Aplica validadores en cascada: 1. **FileSentinel**: validaci\u00f3n estructural (delimitador, headers, n\u00famero de columnas). 2. **TypeValidator**: validaci\u00f3n tipol\u00f3gica (tipos, rangos, formato de fecha, nullability). 3. **ReferentialIntegrityValidator**: integridad referencial (unicidad de claves primarias seg\u00fan metadatos). 4. **FunctionalValidator**: reglas de negocio (formato cuenta, rangos de credit\\_score, balance seg\u00fan estado, etc.). * Si falla alguna validaci\u00f3n, registra el flag correspondiente en la tabla `trigger_control` en PostgreSQL y detiene el procesamiento. * Si pasa todo, registra flag \u201c2\u201d (OK). * Borra el fichero de HDFS para evitar reprocesamientos. Base de datos (PostgreSQL) * Contiene tablas: * users : para gestionar cuentas de usuario (backend y frontend). * file_configuration : configura par\u00e1metros de parseo de cada CSV. * semantic_layer : metadatos de cada campo (tipo de dato, longitud, nullable, PK, formato) utilizados en las validaciones tipol\u00f3gicas y referenciales. * trigger_control : almacena logs de validaci\u00f3n (timestamp, file_config_id, field_name, environment, validation_flag, error_message). * negative_flag_logs (opcional): detalles adicionales de validaciones negativas. Visualizaci\u00f3n de resultados * El usuario, desde el frontend, accede a /logs y obtiene los registros de validaci\u00f3n ( GET /files/logs ). * El backend formatea la fecha ( logged_at ) al formato DD/MM/YYYY, hh:mm:ss en zona Madrid. * El frontend muestra una tabla con campos: ID, file_config_id, file_name, field_name, environment, validation_flag, error_message, fecha. 1.3 Resumen de tecnolog\u00edas utilizadas \u00b6 Scala 2.12 Apache Spark 3.x (Spark Streaming para procesamiento batch/near real\u2010time) HDFS (Hadoop 3.x) SBT (gestor de proyectos Scala) Kryo (serializador personalizado en Spark) Docker & Docker Compose * Contenedores: Spark Master/Workers (bitnami/spark), Hadoop NameNode/DataNode (bde2020), PostgreSQL (postgres\\:latest), Kafka/Zookeeper (wurstmeister), Superset (dockerfile custom). * PostgreSQL 13 (logs y metadatos) * FastAPI (Python 3.12) * Uvicorn (servidor ASGI) * SQLAlchemy Async + AsyncPG * Pydantic Settings * python-jose , passlib[bcrypt] (JWT y hashing de contrase\u00f1as) * requests (llamadas a WebHDFS) * React 18+ (frontend) con Vite * React Router v6 * Axios (con interceptores de token y 401) * Context API (AuthContext) * CSS puro * Superset (opcional, para dashboards de logs) * Git / GitHub (control de versiones) * PlantUML (diagramas UML en bruto para generar ER, clases y secuencias)","title":"Introducci\u00f3n"},{"location":"01-introduccion.html#1-introduccion","text":"","title":"1. Introducci\u00f3n"},{"location":"01-introduccion.html#11-resumen-del-proyecto","text":"El File Monitoring System es una soluci\u00f3n integral para la ingesta, validaci\u00f3n y visualizaci\u00f3n de ficheros bancarios en un entorno Big Data. Consta de tres componentes principales: Un motor de validaciones implementado en Scala y ejecutado sobre Apache Spark , que procesa ficheros almacenados en HDFS , aplica validaciones estructurales, tipol\u00f3gicas, referenciales y de negocio, y almacena los resultados en PostgreSQL . Un backend desarrollado con FastAPI (Python 3.12) que expone una API REST para gestionar usuarios, configuraciones de ficheros, subir/descargar archivos y sincronizarlos con HDFS a trav\u00e9s de WebHDFS . Un frontend construido con React 18+ y Vite , que permite al usuario autenticarse, subir CSV, editar metadatos de parseo, enviar ficheros a HDFS para validaci\u00f3n y visualizar los logs resultantes. La arquitectura est\u00e1 contenida en varios contenedores Docker orquestados mediante Docker Compose , incluyendo cl\u00faster Hadoop (NameNode/DataNode), Spark (Master/Workers), PostgreSQL, Kafka, Superset (opcional) y el propio motor de validaciones. El objetivo es ofrecer una soluci\u00f3n escalable, modular y mantenible para garantizar la calidad e integridad de los datos bancarios.","title":"1.1 Resumen del proyecto"},{"location":"01-introduccion.html#12-explicacion-de-la-aplicacion","text":"El flujo de trabajo global se describe a continuaci\u00f3n: Usuario (frontend) * Se registra o inicia sesi\u00f3n mediante JWT (FastAPI). * Accede a un dashboard donde puede subir un fichero CSV y gestionar metadatos de parseo (delimiter, quote_char, date_format, etc.). * Al pulsar \u201cValidar\u201d, el frontend env\u00eda el CSV al backend y \u00e9ste lo almacena localmente y registra la configuraci\u00f3n en la tabla file_configuration de PostgreSQL. Backend (FastAPI) * Recibe el CSV ( POST /files/upload ) y guarda el archivo en la carpeta local uploaded_files/ . * Crea o actualiza un registro en la tabla file_configuration con par\u00e1metros de parseo. * Cuando el usuario solicita \u201cEnviar a HDFS\u201d ( POST /files/push/{file_name} ), el backend: 1. Verifica que el archivo existe en `uploaded_files/`. 2. Llama a WebHDFS (NameNode) para crear el directorio `/data/bank_accounts` y ajustar permisos (`op=MKDIRS`, `op=SETPERM`). 3. Solicita la creaci\u00f3n del fichero en HDFS (`op=CREATE`), recuerda el redireccionamiento 307 a DataNode, ajusta la URL y sube el contenido. 4. Responde al frontend con un mensaje de \u00e9xito. Motor de validaciones (Scala + Spark) * Se ejecuta en un contenedor Docker con Spark y SBT. En el ENTRYPOINT , al arrancar: 1. Ejecuta `hdfs dfsadmin -safemode leave` para salir de safe mode. 2. Crea la carpeta `/data/bank_accounts` en HDFS (si no existe). 3. Copia los CSV desde un volumen local (`/local_bank_accounts`) a HDFS. 4. Inicia `spark-submit` que corre la clase `Main.scala`. * Main.scala : 1. Configura un bucle de polling que observa el directorio HDFS (`/data/bank_accounts`). 2. Al detectar un fichero, llama a `ExecutionManager.executeFile(path, outputTable)`. 3. Dentro de `ExecutionManager`: * Lee el CSV con `Reader.readFile(...)` en un DataFrame particionado. * Aplica validadores en cascada: 1. **FileSentinel**: validaci\u00f3n estructural (delimitador, headers, n\u00famero de columnas). 2. **TypeValidator**: validaci\u00f3n tipol\u00f3gica (tipos, rangos, formato de fecha, nullability). 3. **ReferentialIntegrityValidator**: integridad referencial (unicidad de claves primarias seg\u00fan metadatos). 4. **FunctionalValidator**: reglas de negocio (formato cuenta, rangos de credit\\_score, balance seg\u00fan estado, etc.). * Si falla alguna validaci\u00f3n, registra el flag correspondiente en la tabla `trigger_control` en PostgreSQL y detiene el procesamiento. * Si pasa todo, registra flag \u201c2\u201d (OK). * Borra el fichero de HDFS para evitar reprocesamientos. Base de datos (PostgreSQL) * Contiene tablas: * users : para gestionar cuentas de usuario (backend y frontend). * file_configuration : configura par\u00e1metros de parseo de cada CSV. * semantic_layer : metadatos de cada campo (tipo de dato, longitud, nullable, PK, formato) utilizados en las validaciones tipol\u00f3gicas y referenciales. * trigger_control : almacena logs de validaci\u00f3n (timestamp, file_config_id, field_name, environment, validation_flag, error_message). * negative_flag_logs (opcional): detalles adicionales de validaciones negativas. Visualizaci\u00f3n de resultados * El usuario, desde el frontend, accede a /logs y obtiene los registros de validaci\u00f3n ( GET /files/logs ). * El backend formatea la fecha ( logged_at ) al formato DD/MM/YYYY, hh:mm:ss en zona Madrid. * El frontend muestra una tabla con campos: ID, file_config_id, file_name, field_name, environment, validation_flag, error_message, fecha.","title":"1.2 Explicaci\u00f3n de la aplicaci\u00f3n"},{"location":"01-introduccion.html#13-resumen-de-tecnologias-utilizadas","text":"Scala 2.12 Apache Spark 3.x (Spark Streaming para procesamiento batch/near real\u2010time) HDFS (Hadoop 3.x) SBT (gestor de proyectos Scala) Kryo (serializador personalizado en Spark) Docker & Docker Compose * Contenedores: Spark Master/Workers (bitnami/spark), Hadoop NameNode/DataNode (bde2020), PostgreSQL (postgres\\:latest), Kafka/Zookeeper (wurstmeister), Superset (dockerfile custom). * PostgreSQL 13 (logs y metadatos) * FastAPI (Python 3.12) * Uvicorn (servidor ASGI) * SQLAlchemy Async + AsyncPG * Pydantic Settings * python-jose , passlib[bcrypt] (JWT y hashing de contrase\u00f1as) * requests (llamadas a WebHDFS) * React 18+ (frontend) con Vite * React Router v6 * Axios (con interceptores de token y 401) * Context API (AuthContext) * CSS puro * Superset (opcional, para dashboards de logs) * Git / GitHub (control de versiones) * PlantUML (diagramas UML en bruto para generar ER, clases y secuencias)","title":"1.3 Resumen de tecnolog\u00edas utilizadas"},{"location":"02-requisitos.html","text":"2. Especificaci\u00f3n de Requisitos \u00b6 2.1 Requisitos funcionales \u00b6 A continuaci\u00f3n se listan los requisitos funcionales unificados, tomando en cuenta los tres componentes (frontend, backend, motor de validaciones): Autenticaci\u00f3n de usuarios * RF1.1: Registro de usuario con email y contrase\u00f1a ( POST /auth/register ). * RF1.2: Login de usuario ( POST /auth/login ), devuelve JWT y redirige al dashboard. * RF1.3: Logout que elimina el token de localStorage y redirige a /login . * RF1.4: Cualquier petici\u00f3n protegida que retorne 401 Unauthorized debe borrar el token y redirigir a /login . Gesti\u00f3n de configuraciones de fichero ( file_configuration ) * RF2.1: Subir CSV al backend ( POST /files/upload ) con multipart/form-data . * RF2.2: Al subir, insertar o actualizar par\u00e1metros en file_configuration (has_header, delimiter, quote_char, escape_char, date_format, timestamp_format, partition_columns). * RF2.3: Listar configuraciones ( GET /files/ ), mostrar ID, file_name, path, has_header, delimiter, quote_char, escape_char, date_format, timestamp_format, partition_columns. * RF2.4: Obtener detalles de una configuraci\u00f3n ( GET /files/{id} ), editar par\u00e1metros ( PATCH /files/{id} ), eliminar configuraci\u00f3n ( DELETE /files/{id} ). * RF2.5: Descargar CSV original desde backend ( GET /files/download/{file_name} ). Sincronizaci\u00f3n con HDFS * RF3.1: Enviar fichero desde backend a HDFS ( POST /files/push/{file_name} ), creando directorio en HDFS ( op=MKDIRS ), ajustando permisos ( op=SETPERM ) y subiendo contenido ( op=CREATE ). * RF3.2: Backend debe manejar redireccionamiento 307 de NameNode a DataNode, adaptando la URL de destino. * RF3.3: Validar existencia local del fichero antes de empujar; si no existe, responder 404. Proceso de validaci\u00f3n (motor Scala/Spark) * RF4.1: Monitorizar HDFS en /data/bank_accounts ; detectar nuevos ficheros en polling batch. * RF4.2: Por cada fichero detectado, procesarlo de forma independiente. * RF4.3: Validaci\u00f3n estructural: * Verificar delimitador, encabezados y n\u00famero de columnas seg\u00fan `file_configuration`. * Flags: * 32: Delimiter mismatch * 33: Header mismatch * 34: Column count per row mismatch * RF4.4: Validaci\u00f3n tipol\u00f3gica: * Comprobar tipos, rangos (fechas, n\u00fameros), formatos de texto seg\u00fan `semantic_layer`. * Flags: * 35: Tipo inv\u00e1lido * 36: Nulo indebido * 37: Longitud excedida * 38: Formato texto inv\u00e1lido * RF4.5: Integridad referencial: * Verificar unicidad de claves primarias seg\u00fan metadatos. * Flag: * 39: Duplicado PK * RF4.6: Validaci\u00f3n de negocio: * Reglas espec\u00edficas del dominio bancario: * Formato de cuenta (`^[A-Za-z0-9]{10}$` \u2192 40). * `credit_score` entre 300 y 850 \u2192 41. * `risk_score` entre 0 y 100 \u2192 42. * Mayor de 18 a\u00f1os (DOB) \u2192 43. * Si `status`=\u201cActive\u201d, `balance`>=0; si \u201cClosed\u201d, `balance`=0 \u2192 44/45. * Si `account_type`=\u201cChecking\u201d, `interest_rate`=0 \u2192 46. * `overdraft_limit`>=0 y v\u00e1lido \u2192 47. * Si `is_joint_account`=\u201cYes\u201d, `num_transactions`>=2 \u2192 48. * Si `num_transactions`=0, `avg_transaction_amount`=0 \u2192 49. * RF4.7: Registrar resultados en trigger_control (timestamp, file_config_id, file_name, field_name, environment, validation_flag, error_message). * RF4.8: Tras procesar (OK o KO), borrar el fichero de HDFS para evitar reprocesamiento. Visualizaci\u00f3n de logs de validaci\u00f3n * RF5.1: Listar logs ( GET /files/logs ) filtrables por environment , from_date y to_date . * RF5.2: Formatear logged_at a DD/MM/YYYY, hh:mm:ss en zona Madrid. * RF5.3: Mostrar tabla con columnas: ID, file_config_id, file_name, field_name, environment, validation_flag, error_message, fecha. UX/UI del frontend * RF6.1: Formularios de registro/login con validaci\u00f3n de campos obligatorios. * RF6.2: Indicadores de carga en botones (\u201cSubiendo\u2026\u201d, \u201cEnviando\u2026\u201d, \u201cGuardando\u2026\u201d, \u201cEliminando\u2026\u201d). * RF6.3: Modal de detalles que se cierra al hacer clic fuera o en bot\u00f3n \u201c\u00d7\u201d. * RF6.4: Alertas ( alert() ) para notificar \u00e9xito o error. * RF6.5: Navbar con enlaces en negrita ( Dashboard , Logs ) y bot\u00f3n Logout en color naranja. 2.2 Requisitos no funcionales \u00b6 Seguridad * RNF1.1: JWT con expiraci\u00f3n configurable; token almacenado en localStorage . * RNF1.2: Backend protege rutas con dependencia get_current_user ; reacciona a 401 borrando token en frontend. * RNF1.3: HDFS: al crear directorios, aplicar chmod -R 777 para permitir lectura/escritura a todos los usuarios. * RNF1.4: Contrase\u00f1as hasheadas con bcrypt. Rendimiento * RNF2.1: El motor de validaciones debe procesar >1M filas por partici\u00f3n en < 1 minuto. * RNF2.2: Uso \u00f3ptimo de particiones en Spark y fetchSize en conexiones JDBC. * RNF2.3: El frontend debe cargar en <2 s en producci\u00f3n b\u00e1sica. Escalabilidad * RNF3.1: Motor de validaciones dise\u00f1ado para aumentar nodos Spark sin cambios en el c\u00f3digo. * RNF3.2: Backend as\u00edncrono (FastAPI + SQLAlchemy Async) para operaciones I/O intensivo. * RNF3.3: Posibilidad de migrar a Spark Structured Streaming en el futuro. Disponibilidad * RNF4.1: El motor debe mantenerse activo 24/7; al iniciar, ejecutar hdfs dfsadmin -safemode leave . * RNF4.2: Contenedores orquestados con Docker Compose deben reiniciarse autom\u00e1ticamente si fallan. Mantenibilidad * RNF5.1: Arquitectura modular (SOLID, Clean Architecture). * RNF5.2: C\u00f3digo documentado con ScalaDoc (motor). * RNF5.3: Separaci\u00f3n de capas (API, servicios, modelos, configuraciones) tanto en backend como en frontend. Compatibilidad * RNF6.1: Frontend soporta navegadores modernos (Chrome, Firefox, Edge). * RNF6.2: No se requiere soporte para Internet Explorer. Tolerancia a fallos * RNF7.1: Capturar excepciones en motor (sin detener el bucle principal) y registrar errores. * RNF7.2: En backend, manejar timeouts y errores de WebHDFS con l\u00f3gica de reintentos b\u00e1sica.","title":"Especificaci\u00f3n de Requisitos"},{"location":"02-requisitos.html#2-especificacion-de-requisitos","text":"","title":"2. Especificaci\u00f3n de Requisitos"},{"location":"02-requisitos.html#21-requisitos-funcionales","text":"A continuaci\u00f3n se listan los requisitos funcionales unificados, tomando en cuenta los tres componentes (frontend, backend, motor de validaciones): Autenticaci\u00f3n de usuarios * RF1.1: Registro de usuario con email y contrase\u00f1a ( POST /auth/register ). * RF1.2: Login de usuario ( POST /auth/login ), devuelve JWT y redirige al dashboard. * RF1.3: Logout que elimina el token de localStorage y redirige a /login . * RF1.4: Cualquier petici\u00f3n protegida que retorne 401 Unauthorized debe borrar el token y redirigir a /login . Gesti\u00f3n de configuraciones de fichero ( file_configuration ) * RF2.1: Subir CSV al backend ( POST /files/upload ) con multipart/form-data . * RF2.2: Al subir, insertar o actualizar par\u00e1metros en file_configuration (has_header, delimiter, quote_char, escape_char, date_format, timestamp_format, partition_columns). * RF2.3: Listar configuraciones ( GET /files/ ), mostrar ID, file_name, path, has_header, delimiter, quote_char, escape_char, date_format, timestamp_format, partition_columns. * RF2.4: Obtener detalles de una configuraci\u00f3n ( GET /files/{id} ), editar par\u00e1metros ( PATCH /files/{id} ), eliminar configuraci\u00f3n ( DELETE /files/{id} ). * RF2.5: Descargar CSV original desde backend ( GET /files/download/{file_name} ). Sincronizaci\u00f3n con HDFS * RF3.1: Enviar fichero desde backend a HDFS ( POST /files/push/{file_name} ), creando directorio en HDFS ( op=MKDIRS ), ajustando permisos ( op=SETPERM ) y subiendo contenido ( op=CREATE ). * RF3.2: Backend debe manejar redireccionamiento 307 de NameNode a DataNode, adaptando la URL de destino. * RF3.3: Validar existencia local del fichero antes de empujar; si no existe, responder 404. Proceso de validaci\u00f3n (motor Scala/Spark) * RF4.1: Monitorizar HDFS en /data/bank_accounts ; detectar nuevos ficheros en polling batch. * RF4.2: Por cada fichero detectado, procesarlo de forma independiente. * RF4.3: Validaci\u00f3n estructural: * Verificar delimitador, encabezados y n\u00famero de columnas seg\u00fan `file_configuration`. * Flags: * 32: Delimiter mismatch * 33: Header mismatch * 34: Column count per row mismatch * RF4.4: Validaci\u00f3n tipol\u00f3gica: * Comprobar tipos, rangos (fechas, n\u00fameros), formatos de texto seg\u00fan `semantic_layer`. * Flags: * 35: Tipo inv\u00e1lido * 36: Nulo indebido * 37: Longitud excedida * 38: Formato texto inv\u00e1lido * RF4.5: Integridad referencial: * Verificar unicidad de claves primarias seg\u00fan metadatos. * Flag: * 39: Duplicado PK * RF4.6: Validaci\u00f3n de negocio: * Reglas espec\u00edficas del dominio bancario: * Formato de cuenta (`^[A-Za-z0-9]{10}$` \u2192 40). * `credit_score` entre 300 y 850 \u2192 41. * `risk_score` entre 0 y 100 \u2192 42. * Mayor de 18 a\u00f1os (DOB) \u2192 43. * Si `status`=\u201cActive\u201d, `balance`>=0; si \u201cClosed\u201d, `balance`=0 \u2192 44/45. * Si `account_type`=\u201cChecking\u201d, `interest_rate`=0 \u2192 46. * `overdraft_limit`>=0 y v\u00e1lido \u2192 47. * Si `is_joint_account`=\u201cYes\u201d, `num_transactions`>=2 \u2192 48. * Si `num_transactions`=0, `avg_transaction_amount`=0 \u2192 49. * RF4.7: Registrar resultados en trigger_control (timestamp, file_config_id, file_name, field_name, environment, validation_flag, error_message). * RF4.8: Tras procesar (OK o KO), borrar el fichero de HDFS para evitar reprocesamiento. Visualizaci\u00f3n de logs de validaci\u00f3n * RF5.1: Listar logs ( GET /files/logs ) filtrables por environment , from_date y to_date . * RF5.2: Formatear logged_at a DD/MM/YYYY, hh:mm:ss en zona Madrid. * RF5.3: Mostrar tabla con columnas: ID, file_config_id, file_name, field_name, environment, validation_flag, error_message, fecha. UX/UI del frontend * RF6.1: Formularios de registro/login con validaci\u00f3n de campos obligatorios. * RF6.2: Indicadores de carga en botones (\u201cSubiendo\u2026\u201d, \u201cEnviando\u2026\u201d, \u201cGuardando\u2026\u201d, \u201cEliminando\u2026\u201d). * RF6.3: Modal de detalles que se cierra al hacer clic fuera o en bot\u00f3n \u201c\u00d7\u201d. * RF6.4: Alertas ( alert() ) para notificar \u00e9xito o error. * RF6.5: Navbar con enlaces en negrita ( Dashboard , Logs ) y bot\u00f3n Logout en color naranja.","title":"2.1 Requisitos funcionales"},{"location":"02-requisitos.html#22-requisitos-no-funcionales","text":"Seguridad * RNF1.1: JWT con expiraci\u00f3n configurable; token almacenado en localStorage . * RNF1.2: Backend protege rutas con dependencia get_current_user ; reacciona a 401 borrando token en frontend. * RNF1.3: HDFS: al crear directorios, aplicar chmod -R 777 para permitir lectura/escritura a todos los usuarios. * RNF1.4: Contrase\u00f1as hasheadas con bcrypt. Rendimiento * RNF2.1: El motor de validaciones debe procesar >1M filas por partici\u00f3n en < 1 minuto. * RNF2.2: Uso \u00f3ptimo de particiones en Spark y fetchSize en conexiones JDBC. * RNF2.3: El frontend debe cargar en <2 s en producci\u00f3n b\u00e1sica. Escalabilidad * RNF3.1: Motor de validaciones dise\u00f1ado para aumentar nodos Spark sin cambios en el c\u00f3digo. * RNF3.2: Backend as\u00edncrono (FastAPI + SQLAlchemy Async) para operaciones I/O intensivo. * RNF3.3: Posibilidad de migrar a Spark Structured Streaming en el futuro. Disponibilidad * RNF4.1: El motor debe mantenerse activo 24/7; al iniciar, ejecutar hdfs dfsadmin -safemode leave . * RNF4.2: Contenedores orquestados con Docker Compose deben reiniciarse autom\u00e1ticamente si fallan. Mantenibilidad * RNF5.1: Arquitectura modular (SOLID, Clean Architecture). * RNF5.2: C\u00f3digo documentado con ScalaDoc (motor). * RNF5.3: Separaci\u00f3n de capas (API, servicios, modelos, configuraciones) tanto en backend como en frontend. Compatibilidad * RNF6.1: Frontend soporta navegadores modernos (Chrome, Firefox, Edge). * RNF6.2: No se requiere soporte para Internet Explorer. Tolerancia a fallos * RNF7.1: Capturar excepciones en motor (sin detener el bucle principal) y registrar errores. * RNF7.2: En backend, manejar timeouts y errores de WebHDFS con l\u00f3gica de reintentos b\u00e1sica.","title":"2.2 Requisitos no funcionales"},{"location":"03-diseno.html","text":"3. Dise\u00f1o (Diagramas) \u00b6 En la carpeta docs/Img/ se incluyen todos los diagramas en PNG o PlantUML; aqu\u00ed se describen y referencian los m\u00e1s relevantes. 3.1 Casos de uso \u00b6 A continuaci\u00f3n se resumen los casos de uso principales para todo el sistema: Registrar Usuario * Actor: Usuario no autenticado. * Flujo: 1. Accede a `/register` (frontend). 2. Completa email y contrase\u00f1a, env\u00eda `POST /auth/register` (backend). 3. Si \u00e9xito (201), frontend redirige a `/login`; si error, muestra mensaje en formulario. Iniciar Sesi\u00f3n * Actor: Usuario no autenticado. * Flujo: 1. Accede a `/login`. 2. Completa credenciales, env\u00eda `POST /auth/login`. 3. Si \u00e9xito (200), backend devuelve JWT; frontend guarda en `localStorage` y redirige a `/dashboard`. 4. Si 401, muestra mensaje \u201cCredenciales inv\u00e1lidas\u201d. Cerrar Sesi\u00f3n * Actor: Usuario autenticado. * Flujo: 1. En navbar, pulsa **Logout**. 2. Frontend elimina token y redirige a `/login`. Subir Fichero CSV * Actor: Usuario autenticado. * Flujo: 1. En `/dashboard`, selecciona CSV y pulsa **Subir Fichero**. 2. Frontend muestra \u201cSubiendo\u2026\u201d, env\u00eda `POST /files/upload` con `multipart/form-data`. 3. Backend guarda el fichero en `uploaded_files/` y crea/actualiza registro en `file_configuration`. 4. Backend responde `{ \"file_config_id\": X }`; frontend muestra alerta \u201cFichero subido ID: X\u201d y refresca lista. Listar Configuraciones * Actor: Usuario autenticado. * Flujo: 1. Al acceder a `/dashboard`, frontend hace `GET /files/`. 2. Backend devuelve array de configuraciones; frontend muestra tabla con cada fila: ID, file\\_name, path, has\\_header, delimiter, etc. Editar Configuraci\u00f3n (Modal) * Actor: Usuario autenticado. * Flujo: 1. En `/dashboard`, pulsa **Detalles** en una fila. 2. Abre modal con campos prellenados (`delimiter`, `quote_char`, `escape_char`, `has_header`, `date_format`, `timestamp_format`, `partition_columns`). 3. Usuario modifica valores y pulsa **Guardar**; frontend muestra \u201cGuardando\u2026\u201d, env\u00eda `PATCH /files/{id}`. 4. Backend actualiza `file_configuration` y responde con objeto actualizado; frontend alerta \u201cConfiguraci\u00f3n actualizada\u201d, cierra modal y refresca lista. Eliminar Configuraci\u00f3n * Actor: Usuario autenticado. * Flujo: 1. En `/dashboard` o modal, pulsa **Eliminar**. 2. Mostrar `window.confirm(\"\u00bfEst\u00e1s seguro?\")`. 3. Si confirma, frontend muestra \u201cEliminando\u2026\u201d, env\u00eda `DELETE /files/{id}`. 4. Backend elimina registro; responde 204; frontend alerta \u201cConfiguraci\u00f3n eliminada\u201d y refresca lista. Descargar Fichero * Actor: Usuario autenticado. * Flujo: 1. En modal, pulsa **Descargar**. 2. Frontend hace `fetch(downloadURL, { headers: { Authorization: Bearer <token> } })`. 3. Recibe Blob, crea `<a>` con `URL.createObjectURL(blob)` y `download=file_name`, ejecuta `a.click()`. 4. Usuario obtiene el fichero en su equipo. Enviar Fichero a HDFS (Backend \u2192 HDFS) * Actor: Usuario autenticado. * Flujo: 1. En `/dashboard`, pulsa **Validar** en una fila; frontend muestra \u201cEnviando\u2026\u201d, env\u00eda `POST /files/push/{file_name}`. 2. Backend: * Verifica que `uploaded_files/{file_name}` exista; si no, responde 404. * Llama a WebHDFS NameNode para crear directorio `/data/bank_accounts` y ajustar permisos (`op=MKDIRS`, `op=SETPERM`). * Inicia creaci\u00f3n de fichero (`op=CREATE`), recibe redirecci\u00f3n 307 con URL de DataNode; ajusta host/puerto y sube contenido. * Devuelve `{ \"message\": \"Pushed <file_name>\" }`. 3. Frontend alerta \u201cEnviado a validar\u201d y refresca lista. Proceso de validaci\u00f3n en Spark (motor) * Actor: Sistema (motor de validaciones). * Flujo: 1. Motor arranca en contenedor Docker; sale de safe mode (`hdfs dfsadmin -safemode leave`), crea carpeta en HDFS y copia CSV. 2. `Main.scala` ejecuta bucle de polling sobre `/data/bank_accounts`. 3. Al detectar fichero, llama a `ExecutionManager.executeFile(path, outputTable)`. * `Reader.readFile(...)` carga CSV como DataFrame. * `FileSentinel.verifyFiles(...)` \u2192 si falla, `logTrigger(flag)` en `trigger_control` (flags 32,33,34). * `TypeValidator.verifyTyping(...)` \u2192 si falla, `logTrigger(...)` (flags 35-38). * `ReferentialIntegrityValidator.verifyIntegrity(...)` \u2192 si falla, `logTrigger(39)`. * `FunctionalValidator.verifyFunctional(...)` \u2192 si falla, `logTrigger(flags 40\u201349)`. * Si todo OK, `logTrigger(2)`. 4. Borra el fichero de HDFS. Consultar Logs de Validaci\u00f3n * Actor: Usuario autenticado. * Flujo: 1. Accede a `/logs`; frontend ejecuta `GET /files/logs?environment=&from_date=&to_date=`. 2. Backend filtra registros en `trigger_control` seg\u00fan par\u00e1metros y devuelve array de logs. 3. Frontend formatea `logged_at` con `toLocaleDateString('es-ES')` y muestra tabla con columnas: ID, file\\_config\\_id, file\\_name, field\\_name, environment, validation\\_flag, error\\_message, fecha. (Diagrams separados en docs/Img/Engine, docs/Img/Backend y docs/Img/Frontend respectivamente) 3.2 Diagrama entidad-relaci\u00f3n \u00b6 Tabla principal: trigger_control id (serial PK) logged_at (timestamp) file_config_id (int FK \u2192 file_configuration.id) file_name (varchar) field_name (varchar, null) environment (varchar) validation_flag (varchar) error_message (varchar, null) Otras tablas relacionadas: file_configuration * id (serial PK) * file_format (varchar) * path (varchar) * file_name (varchar) * has_header (boolean) * delimiter (varchar) * quote_char (varchar) * escape_char (varchar) * date_format (varchar) * timestamp_format (varchar) * partition_columns (varchar, null) * created_by (int FK \u2192 users.id) * created_at (timestamp) semantic_layer * id (serial PK) * file_config_id (int FK \u2192 file_configuration.id) * field_name (varchar) * data_type (varchar) * length (int) * nullable (boolean) * is_pk (boolean) * format (varchar) users (backend/front-end) * id (serial PK) * email (varchar, unique) * hashed_password (varchar) * is_active (boolean, default true) * created_at (timestamp) * updated_at (timestamp) negative_flag_logs (opcional) * id (serial PK) * trigger_id (int FK \u2192 trigger_control.id) * (otros campos de detalle) Relaciones: users 1\u2014* file_configuration (un usuario puede tener varias configuraciones). file_configuration 1\u2014* semantic_layer (cada configuraci\u00f3n define metadatos para m\u00faltiples campos). file_configuration 1\u2014* trigger_control (cada configuraci\u00f3n genera m\u00faltiples logs de validaci\u00f3n). semantic_layer (opcional) 1\u2014* trigger_control (para identificar qu\u00e9 campo en trigger_control.field_name proviene de qu\u00e9 metadato). trigger_control 1\u2014* negative_flag_logs (detalle de validaciones negativas). (Ver docs/Img/ERD.png para la imagen completa.) 3.3 Diagrama de clases del modelo \u00b6 Se incluyen las clases m\u00e1s relevantes en los tres componentes. 3.3.1 Motor de validaciones (Scala) \u00b6 config * DbConfig.scala * DBConnection.scala * SparkSessionProvider.scala models * FileConfigurationCaseClass.scala * SemanticLayerCaseClass.scala services * ExecutionManager.scala * TriggerIdManager.scala utils * Reader.scala * Writer.scala * FileManager.scala validators * FileSentinel.scala * TypeValidator.scala * ReferentialIntegrityValidator.scala * FunctionalValidator.scala Main.scala : configura el bucle de polling e invoca a ExecutionManager . Ejemplo de case class: package models case class FileConfigurationCaseClass( id: Int, file_format: String, path: String, file_name: String, has_header: Boolean, delimiter: String, quote_char: String, escape_char: String, date_format: String, timestamp_format: String, partition_columns: Option[String] ) (Ver docs/Img/Engine/ para la imagen completa.) 3.3.2 Frontend (React) \u00b6 AuthContext.jsx * Propiedades: user , token * M\u00e9todos: login() , register() , logout() * FileConfig (modelo JS) * Propiedades: id , fileName , path , hasHeader , delimiter , quoteChar , escapeChar , dateFormat , timestampFormat , partitionColumns * M\u00e9todos: fetchAll() , create() , update() , delete() , pushToHDFS() , download() * ValidationLog (modelo JS) * Propiedades: id , fileConfigId , fileName , fieldName , environment , validationFlag , errorMessage , loggedAt * M\u00e9todos: fetchAll() * Componentes principales * AppRouter (gestiona rutas con RequireAuth ) * MainLayout (Navbar + <Outlet /> ) * Dashboard (muestra lista de configuraciones y acciones) * FileDetailModal (modal para editar/validar/eliminar/descargar) * LogsPage (muestra logs formateando logged_at ) (Ver docs/Img/Frontend/ para la imagen completa.) 3.3.3 Backend (FastAPI) \u00b6 models ORM (SQLAlchemy Async) * User * `id: Integer PK` * `email: String(255) UNIQUE NOT NULL` * `hashed_password: String(255) NOT NULL` * `is_active: Boolean NOT NULL DEFAULT True` * `created_at: DateTime(timezone=True) DEFAULT now()` * `updated_at: DateTime(timezone=True)` * FileConfiguration * `id: Integer PK` * `file_format: String NOT NULL` * `path: String NOT NULL` * `file_name: String NOT NULL` * `has_header: Boolean NOT NULL` * `delimiter: String(1) NOT NULL` * `quote_char: String(1) NOT NULL` * `escape_char: String(1) NOT NULL` * `date_format: String NOT NULL` * `timestamp_format: String NOT NULL` * `partition_columns: String NULL` * TriggerControl * `id: Integer PK` * `file_config_id: Integer FK \u2192 FileConfiguration.id` * `file_name: String NOT NULL` * `field_name: String NOT NULL` * `environment: String NOT NULL` * `validation_flag: String NOT NULL` * `error_message: Text NULL` * `logged_at: DateTime(timezone=True) DEFAULT now()` * NegativeFlagLog * `id: Integer PK` * `trigger_id: Integer FK \u2192 TriggerControl.id` * (otros campos espec\u00edficos) Clases de servicio * file_service.py * `save_and_register_file(file: UploadFile, db: AsyncSession) \u2192 int` * hdfs_sync.py * `push_file_to_hdfs(file_name: str)` Controladores (routers) * auth.py : /auth/register , /auth/login * files.py : /files/upload , /files/push/{file_name} , /files/download/{file_name} , /files/ , /files/{id} , /files/{id} (GET, PATCH, DELETE), /files/logs * health.py : /health (healthcheck) (Ver docs/Img/Backend/ para la imagen completa.) 3.4 Diagramas de secuencia \u00b6 Se incluyen los diagramas de secuencia m\u00e1s relevantes para cada componente. A continuaci\u00f3n se describen de forma textual; ver docs/ para los PNG o PlantUML. 3.4.1 Secuencia: Subida de fichero (Frontend \u2192 Backend) \u00b6 Usuario \u2192 Dashboard (Frontend): selecciona CSV y pulsa Subir Fichero . Dashboard \u2192 Axios ( POST /files/upload ): env\u00eda multipart/form-data al backend. Backend ( files.py ) \u2192 file_service.save_and_register_file() : * Guarda CSV en uploaded_files/ . * Inserta/actualiza registro en file_configuration . 4. Backend \u2192 Dashboard: responde con { \"file_config_id\": X } (201). 5. Dashboard: muestra alerta \u201cFichero subido ID: X\u201d y llama fetchConfigs() . 3.4.2 Secuencia: Enviar fichero a HDFS (Frontend \u2192 Backend \u2192 HDFS) \u00b6 Usuario \u2192 Dashboard: pulsa Validar . Dashboard \u2192 Axios ( POST /files/push/{file_name} ): solicita al backend empuje a HDFS. Backend ( files.py ) \u2192 hdfs_sync.push_file_to_hdfs(file_name) : * Verifica existencia local en uploaded_files/ . * Llama a WebHDFS NameNode ( MKDIRS , SETPERM ). * Llama CREATE , recibe 307 con header Location apuntando al DataNode. * Ajusta URL para apuntar a hdfs_datanode_host:hdfs_datanode_port . * Abre CSV y hace PUT a DataNode. 4. HDFS DataNode \u2192 Backend: responde 201 Created si \u00e9xito. 5. Backend \u2192 Dashboard: responde { \"message\": \"Pushed <file_name>\" } . 6. Dashboard: muestra alerta \u201cEnviado a validar\u201d y llama fetchConfigs() . 3.4.3 Secuencia: Proceso de validaci\u00f3n en Spark (Motor) \u00b6 Contenedor validation-engine \u2192 HDFS: * Ejecuta hdfs dfsadmin -safemode leave . * hdfs dfs -mkdir -p /data/bank_accounts . * hdfs dfs -put -f /local_bank_accounts/*.csv /data/bank_accounts . * Ajusta permisos hdfs dfs -chmod -R 777 /data/bank_accounts . 2. Contenedor Spark ( spark-submit Main ) \u2192 HDFS.listStatus(): detecta CSV. 3. Main \u2192 ExecutionManager.executeFile(path, outputTable): * Llama Reader.readFile(...) \u2192 devuelve DataFrame particionado. * Llama FileSentinel.verifyFiles(...) \u2192 si falla, logTrigger(flag) y salir. * Llama TypeValidator.verifyTyping(...) \u2192 si falla, logTrigger(flag) y salir. * Llama ReferentialIntegrityValidator.verifyIntegrity(...) \u2192 si falla, logTrigger(39) y salir. * Llama FunctionalValidator.verifyFunctional(...) \u2192 si falla, logTrigger(flags 40\u201349) y salir. * Si todo OK, logTrigger(2) . * Escribe logs en PostgreSQL ( df.write.mode(\"append\").jdbc(...) ). * HDFS.delete(path) . 3.4.4 Secuencia: Listar Logs de Validaci\u00f3n (Frontend \u2192 Backend \u2192 BD) \u00b6 Usuario \u2192 LogsPage (Frontend): al montar, llama getLogs() . LogsPage \u2192 Axios ( GET /files/logs?environment=&from_date=&to_date= ): solicita logs. **Backend ( files.py ) \u2192 consulta trigger_control filtrando por par\u00e1metros. Backend \u2192 LogsPage: devuelve array de objetos JSON con campos de log. LogsPage: recorre cada registro y formatea logged_at con toLocaleDateString('es-ES') . LogsPage: muestra tabla con columnas: ID, file_config_id, file_name, field_name, environment, validation_flag, error_message, fecha.","title":"Dise\u00f1o (Diagramas)"},{"location":"03-diseno.html#3-diseno-diagramas","text":"En la carpeta docs/Img/ se incluyen todos los diagramas en PNG o PlantUML; aqu\u00ed se describen y referencian los m\u00e1s relevantes.","title":"3. Dise\u00f1o (Diagramas)"},{"location":"03-diseno.html#31-casos-de-uso","text":"A continuaci\u00f3n se resumen los casos de uso principales para todo el sistema: Registrar Usuario * Actor: Usuario no autenticado. * Flujo: 1. Accede a `/register` (frontend). 2. Completa email y contrase\u00f1a, env\u00eda `POST /auth/register` (backend). 3. Si \u00e9xito (201), frontend redirige a `/login`; si error, muestra mensaje en formulario. Iniciar Sesi\u00f3n * Actor: Usuario no autenticado. * Flujo: 1. Accede a `/login`. 2. Completa credenciales, env\u00eda `POST /auth/login`. 3. Si \u00e9xito (200), backend devuelve JWT; frontend guarda en `localStorage` y redirige a `/dashboard`. 4. Si 401, muestra mensaje \u201cCredenciales inv\u00e1lidas\u201d. Cerrar Sesi\u00f3n * Actor: Usuario autenticado. * Flujo: 1. En navbar, pulsa **Logout**. 2. Frontend elimina token y redirige a `/login`. Subir Fichero CSV * Actor: Usuario autenticado. * Flujo: 1. En `/dashboard`, selecciona CSV y pulsa **Subir Fichero**. 2. Frontend muestra \u201cSubiendo\u2026\u201d, env\u00eda `POST /files/upload` con `multipart/form-data`. 3. Backend guarda el fichero en `uploaded_files/` y crea/actualiza registro en `file_configuration`. 4. Backend responde `{ \"file_config_id\": X }`; frontend muestra alerta \u201cFichero subido ID: X\u201d y refresca lista. Listar Configuraciones * Actor: Usuario autenticado. * Flujo: 1. Al acceder a `/dashboard`, frontend hace `GET /files/`. 2. Backend devuelve array de configuraciones; frontend muestra tabla con cada fila: ID, file\\_name, path, has\\_header, delimiter, etc. Editar Configuraci\u00f3n (Modal) * Actor: Usuario autenticado. * Flujo: 1. En `/dashboard`, pulsa **Detalles** en una fila. 2. Abre modal con campos prellenados (`delimiter`, `quote_char`, `escape_char`, `has_header`, `date_format`, `timestamp_format`, `partition_columns`). 3. Usuario modifica valores y pulsa **Guardar**; frontend muestra \u201cGuardando\u2026\u201d, env\u00eda `PATCH /files/{id}`. 4. Backend actualiza `file_configuration` y responde con objeto actualizado; frontend alerta \u201cConfiguraci\u00f3n actualizada\u201d, cierra modal y refresca lista. Eliminar Configuraci\u00f3n * Actor: Usuario autenticado. * Flujo: 1. En `/dashboard` o modal, pulsa **Eliminar**. 2. Mostrar `window.confirm(\"\u00bfEst\u00e1s seguro?\")`. 3. Si confirma, frontend muestra \u201cEliminando\u2026\u201d, env\u00eda `DELETE /files/{id}`. 4. Backend elimina registro; responde 204; frontend alerta \u201cConfiguraci\u00f3n eliminada\u201d y refresca lista. Descargar Fichero * Actor: Usuario autenticado. * Flujo: 1. En modal, pulsa **Descargar**. 2. Frontend hace `fetch(downloadURL, { headers: { Authorization: Bearer <token> } })`. 3. Recibe Blob, crea `<a>` con `URL.createObjectURL(blob)` y `download=file_name`, ejecuta `a.click()`. 4. Usuario obtiene el fichero en su equipo. Enviar Fichero a HDFS (Backend \u2192 HDFS) * Actor: Usuario autenticado. * Flujo: 1. En `/dashboard`, pulsa **Validar** en una fila; frontend muestra \u201cEnviando\u2026\u201d, env\u00eda `POST /files/push/{file_name}`. 2. Backend: * Verifica que `uploaded_files/{file_name}` exista; si no, responde 404. * Llama a WebHDFS NameNode para crear directorio `/data/bank_accounts` y ajustar permisos (`op=MKDIRS`, `op=SETPERM`). * Inicia creaci\u00f3n de fichero (`op=CREATE`), recibe redirecci\u00f3n 307 con URL de DataNode; ajusta host/puerto y sube contenido. * Devuelve `{ \"message\": \"Pushed <file_name>\" }`. 3. Frontend alerta \u201cEnviado a validar\u201d y refresca lista. Proceso de validaci\u00f3n en Spark (motor) * Actor: Sistema (motor de validaciones). * Flujo: 1. Motor arranca en contenedor Docker; sale de safe mode (`hdfs dfsadmin -safemode leave`), crea carpeta en HDFS y copia CSV. 2. `Main.scala` ejecuta bucle de polling sobre `/data/bank_accounts`. 3. Al detectar fichero, llama a `ExecutionManager.executeFile(path, outputTable)`. * `Reader.readFile(...)` carga CSV como DataFrame. * `FileSentinel.verifyFiles(...)` \u2192 si falla, `logTrigger(flag)` en `trigger_control` (flags 32,33,34). * `TypeValidator.verifyTyping(...)` \u2192 si falla, `logTrigger(...)` (flags 35-38). * `ReferentialIntegrityValidator.verifyIntegrity(...)` \u2192 si falla, `logTrigger(39)`. * `FunctionalValidator.verifyFunctional(...)` \u2192 si falla, `logTrigger(flags 40\u201349)`. * Si todo OK, `logTrigger(2)`. 4. Borra el fichero de HDFS. Consultar Logs de Validaci\u00f3n * Actor: Usuario autenticado. * Flujo: 1. Accede a `/logs`; frontend ejecuta `GET /files/logs?environment=&from_date=&to_date=`. 2. Backend filtra registros en `trigger_control` seg\u00fan par\u00e1metros y devuelve array de logs. 3. Frontend formatea `logged_at` con `toLocaleDateString('es-ES')` y muestra tabla con columnas: ID, file\\_config\\_id, file\\_name, field\\_name, environment, validation\\_flag, error\\_message, fecha. (Diagrams separados en docs/Img/Engine, docs/Img/Backend y docs/Img/Frontend respectivamente)","title":"3.1 Casos de uso"},{"location":"03-diseno.html#32-diagrama-entidad-relacion","text":"Tabla principal: trigger_control id (serial PK) logged_at (timestamp) file_config_id (int FK \u2192 file_configuration.id) file_name (varchar) field_name (varchar, null) environment (varchar) validation_flag (varchar) error_message (varchar, null) Otras tablas relacionadas: file_configuration * id (serial PK) * file_format (varchar) * path (varchar) * file_name (varchar) * has_header (boolean) * delimiter (varchar) * quote_char (varchar) * escape_char (varchar) * date_format (varchar) * timestamp_format (varchar) * partition_columns (varchar, null) * created_by (int FK \u2192 users.id) * created_at (timestamp) semantic_layer * id (serial PK) * file_config_id (int FK \u2192 file_configuration.id) * field_name (varchar) * data_type (varchar) * length (int) * nullable (boolean) * is_pk (boolean) * format (varchar) users (backend/front-end) * id (serial PK) * email (varchar, unique) * hashed_password (varchar) * is_active (boolean, default true) * created_at (timestamp) * updated_at (timestamp) negative_flag_logs (opcional) * id (serial PK) * trigger_id (int FK \u2192 trigger_control.id) * (otros campos de detalle) Relaciones: users 1\u2014* file_configuration (un usuario puede tener varias configuraciones). file_configuration 1\u2014* semantic_layer (cada configuraci\u00f3n define metadatos para m\u00faltiples campos). file_configuration 1\u2014* trigger_control (cada configuraci\u00f3n genera m\u00faltiples logs de validaci\u00f3n). semantic_layer (opcional) 1\u2014* trigger_control (para identificar qu\u00e9 campo en trigger_control.field_name proviene de qu\u00e9 metadato). trigger_control 1\u2014* negative_flag_logs (detalle de validaciones negativas). (Ver docs/Img/ERD.png para la imagen completa.)","title":"3.2 Diagrama entidad-relaci\u00f3n"},{"location":"03-diseno.html#33-diagrama-de-clases-del-modelo","text":"Se incluyen las clases m\u00e1s relevantes en los tres componentes.","title":"3.3 Diagrama de clases del modelo"},{"location":"03-diseno.html#331-motor-de-validaciones-scala","text":"config * DbConfig.scala * DBConnection.scala * SparkSessionProvider.scala models * FileConfigurationCaseClass.scala * SemanticLayerCaseClass.scala services * ExecutionManager.scala * TriggerIdManager.scala utils * Reader.scala * Writer.scala * FileManager.scala validators * FileSentinel.scala * TypeValidator.scala * ReferentialIntegrityValidator.scala * FunctionalValidator.scala Main.scala : configura el bucle de polling e invoca a ExecutionManager . Ejemplo de case class: package models case class FileConfigurationCaseClass( id: Int, file_format: String, path: String, file_name: String, has_header: Boolean, delimiter: String, quote_char: String, escape_char: String, date_format: String, timestamp_format: String, partition_columns: Option[String] ) (Ver docs/Img/Engine/ para la imagen completa.)","title":"3.3.1 Motor de validaciones (Scala)"},{"location":"03-diseno.html#332-frontend-react","text":"AuthContext.jsx * Propiedades: user , token * M\u00e9todos: login() , register() , logout() * FileConfig (modelo JS) * Propiedades: id , fileName , path , hasHeader , delimiter , quoteChar , escapeChar , dateFormat , timestampFormat , partitionColumns * M\u00e9todos: fetchAll() , create() , update() , delete() , pushToHDFS() , download() * ValidationLog (modelo JS) * Propiedades: id , fileConfigId , fileName , fieldName , environment , validationFlag , errorMessage , loggedAt * M\u00e9todos: fetchAll() * Componentes principales * AppRouter (gestiona rutas con RequireAuth ) * MainLayout (Navbar + <Outlet /> ) * Dashboard (muestra lista de configuraciones y acciones) * FileDetailModal (modal para editar/validar/eliminar/descargar) * LogsPage (muestra logs formateando logged_at ) (Ver docs/Img/Frontend/ para la imagen completa.)","title":"3.3.2 Frontend (React)"},{"location":"03-diseno.html#333-backend-fastapi","text":"models ORM (SQLAlchemy Async) * User * `id: Integer PK` * `email: String(255) UNIQUE NOT NULL` * `hashed_password: String(255) NOT NULL` * `is_active: Boolean NOT NULL DEFAULT True` * `created_at: DateTime(timezone=True) DEFAULT now()` * `updated_at: DateTime(timezone=True)` * FileConfiguration * `id: Integer PK` * `file_format: String NOT NULL` * `path: String NOT NULL` * `file_name: String NOT NULL` * `has_header: Boolean NOT NULL` * `delimiter: String(1) NOT NULL` * `quote_char: String(1) NOT NULL` * `escape_char: String(1) NOT NULL` * `date_format: String NOT NULL` * `timestamp_format: String NOT NULL` * `partition_columns: String NULL` * TriggerControl * `id: Integer PK` * `file_config_id: Integer FK \u2192 FileConfiguration.id` * `file_name: String NOT NULL` * `field_name: String NOT NULL` * `environment: String NOT NULL` * `validation_flag: String NOT NULL` * `error_message: Text NULL` * `logged_at: DateTime(timezone=True) DEFAULT now()` * NegativeFlagLog * `id: Integer PK` * `trigger_id: Integer FK \u2192 TriggerControl.id` * (otros campos espec\u00edficos) Clases de servicio * file_service.py * `save_and_register_file(file: UploadFile, db: AsyncSession) \u2192 int` * hdfs_sync.py * `push_file_to_hdfs(file_name: str)` Controladores (routers) * auth.py : /auth/register , /auth/login * files.py : /files/upload , /files/push/{file_name} , /files/download/{file_name} , /files/ , /files/{id} , /files/{id} (GET, PATCH, DELETE), /files/logs * health.py : /health (healthcheck) (Ver docs/Img/Backend/ para la imagen completa.)","title":"3.3.3 Backend (FastAPI)"},{"location":"03-diseno.html#34-diagramas-de-secuencia","text":"Se incluyen los diagramas de secuencia m\u00e1s relevantes para cada componente. A continuaci\u00f3n se describen de forma textual; ver docs/ para los PNG o PlantUML.","title":"3.4 Diagramas de secuencia"},{"location":"03-diseno.html#341-secuencia-subida-de-fichero-frontend-backend","text":"Usuario \u2192 Dashboard (Frontend): selecciona CSV y pulsa Subir Fichero . Dashboard \u2192 Axios ( POST /files/upload ): env\u00eda multipart/form-data al backend. Backend ( files.py ) \u2192 file_service.save_and_register_file() : * Guarda CSV en uploaded_files/ . * Inserta/actualiza registro en file_configuration . 4. Backend \u2192 Dashboard: responde con { \"file_config_id\": X } (201). 5. Dashboard: muestra alerta \u201cFichero subido ID: X\u201d y llama fetchConfigs() .","title":"3.4.1 Secuencia: Subida de fichero (Frontend \u2192 Backend)"},{"location":"03-diseno.html#342-secuencia-enviar-fichero-a-hdfs-frontend-backend-hdfs","text":"Usuario \u2192 Dashboard: pulsa Validar . Dashboard \u2192 Axios ( POST /files/push/{file_name} ): solicita al backend empuje a HDFS. Backend ( files.py ) \u2192 hdfs_sync.push_file_to_hdfs(file_name) : * Verifica existencia local en uploaded_files/ . * Llama a WebHDFS NameNode ( MKDIRS , SETPERM ). * Llama CREATE , recibe 307 con header Location apuntando al DataNode. * Ajusta URL para apuntar a hdfs_datanode_host:hdfs_datanode_port . * Abre CSV y hace PUT a DataNode. 4. HDFS DataNode \u2192 Backend: responde 201 Created si \u00e9xito. 5. Backend \u2192 Dashboard: responde { \"message\": \"Pushed <file_name>\" } . 6. Dashboard: muestra alerta \u201cEnviado a validar\u201d y llama fetchConfigs() .","title":"3.4.2 Secuencia: Enviar fichero a HDFS (Frontend \u2192 Backend \u2192 HDFS)"},{"location":"03-diseno.html#343-secuencia-proceso-de-validacion-en-spark-motor","text":"Contenedor validation-engine \u2192 HDFS: * Ejecuta hdfs dfsadmin -safemode leave . * hdfs dfs -mkdir -p /data/bank_accounts . * hdfs dfs -put -f /local_bank_accounts/*.csv /data/bank_accounts . * Ajusta permisos hdfs dfs -chmod -R 777 /data/bank_accounts . 2. Contenedor Spark ( spark-submit Main ) \u2192 HDFS.listStatus(): detecta CSV. 3. Main \u2192 ExecutionManager.executeFile(path, outputTable): * Llama Reader.readFile(...) \u2192 devuelve DataFrame particionado. * Llama FileSentinel.verifyFiles(...) \u2192 si falla, logTrigger(flag) y salir. * Llama TypeValidator.verifyTyping(...) \u2192 si falla, logTrigger(flag) y salir. * Llama ReferentialIntegrityValidator.verifyIntegrity(...) \u2192 si falla, logTrigger(39) y salir. * Llama FunctionalValidator.verifyFunctional(...) \u2192 si falla, logTrigger(flags 40\u201349) y salir. * Si todo OK, logTrigger(2) . * Escribe logs en PostgreSQL ( df.write.mode(\"append\").jdbc(...) ). * HDFS.delete(path) .","title":"3.4.3 Secuencia: Proceso de validaci\u00f3n en Spark (Motor)"},{"location":"03-diseno.html#344-secuencia-listar-logs-de-validacion-frontend-backend-bd","text":"Usuario \u2192 LogsPage (Frontend): al montar, llama getLogs() . LogsPage \u2192 Axios ( GET /files/logs?environment=&from_date=&to_date= ): solicita logs. **Backend ( files.py ) \u2192 consulta trigger_control filtrando por par\u00e1metros. Backend \u2192 LogsPage: devuelve array de objetos JSON con campos de log. LogsPage: recorre cada registro y formatea logged_at con toLocaleDateString('es-ES') . LogsPage: muestra tabla con columnas: ID, file_config_id, file_name, field_name, environment, validation_flag, error_message, fecha.","title":"3.4.4 Secuencia: Listar Logs de Validaci\u00f3n (Frontend \u2192 Backend \u2192 BD)"},{"location":"04-implementacion.html","text":"4. Implementaci\u00f3n (GIT) \u00b6 4.1 Diagrama de arquitectura \u00b6 --- config: layout: dagre --- flowchart LR subgraph Usuario[\"Usuario\"] Browser[\"User Browser\"] end subgraph Frontend[\"Frontend\"] React[\"React SPA<br>(Web App)\"] end subgraph API[\"API\"] FastAPI[\"FastAPI Backend<br>(Python)\"] end subgraph Persistencia[\"Persistencia\"] Postgres[\"PostgreSQL DB\"] HDFS[\"HDFS Cluster\"] end subgraph Procesamiento[\"Procesamiento\"] SparkMaster[\"\ud83d\udd37 Spark Master\"] SparkWorker1[\"\ud83d\udd36 Spark Worker 1\"] SparkWorker2[\"\ud83d\udd36 Spark Worker 2\"] ValidationEngine[\"Validation Engine<br>(Scala + Spark)\"] end subgraph subGraph7[\"Contenedores Docker\"] Superset[\"Superset BI\"] end Browser -- HTTP requests --> React React -- REST/GraphQL<br>JSON over HTTP --> FastAPI FastAPI -- SQLAlchemy (INSERT/SELECT/UPDATE) --> Postgres FastAPI -- WebHDFS / HDFS Client --> HDFS FastAPI -- SELECT * FROM TABLE --> Postgres ValidationEngine -- Leer CSV/Parquet --> HDFS ValidationEngine -- INSERT logs en trigger_control --> Postgres ValidationEngine -- Spark job submission --> SparkMaster SparkMaster --> SparkWorker1 & SparkWorker2 SparkMaster -- HDFS FS Default --- HDFS SparkWorker1 -- HDFS FS Default --- HDFS SparkWorker2 -- HDFS FS Default --- HDFS Superset -- SQLAlchemy --> Postgres style Usuario fill:#f9f,stroke:#333,stroke-width:1px style Frontend fill:#ccf,stroke:#333,stroke-width:1px style API fill:#cfc,stroke:#333,stroke-width:1px style Persistencia fill:#ffe599,stroke:#333,stroke-width:1px style Procesamiento fill:#f4cccc,stroke:#333,stroke-width:1px (C\u00f3digo de diagrama mermaid, renderizable en https://mermaid.js.org/) Frontend (React + Vite) Usuario interact\u00faa con la UI, llama rutas REST al backend y muestra datos. Backend (FastAPI) Exposici\u00f3n de endpoints /auth , /files , /files/logs . Usa SQLAlchemy Async + AsyncPG para comunicarse con PostgreSQL. Utiliza requests para llamar a WebHDFS (NameNode/DataNode). PostgreSQL Almacena users , file_configuration , trigger_control , negative_flag_logs . Superset (opcional) tambi\u00e9n se conecta aqu\u00ed para dashboards. Motor Scala + Spark Se ejecuta como contenedor independiente, se conecta a HDFS ( hdfs://hadoop-namenode:9000 ). Procesa ficheros mediante Spark Streaming en modo batch y escribe logs en PostgreSQL mediante JDBC. HDFS (NameNode/DataNode) Almacena CSV en /data/bank_accounts . Motor de validaciones lee ficheros desde aqu\u00ed y borra tras procesar. (Ver docs/diagrama_arquitectura.png para imagen detallada.) 4.2 Tecnolog\u00edas \u00b6 A continuaci\u00f3n se especifican todas las tecnolog\u00edas y dependencias utilizadas en cada componente: 4.2.1 Motor de validaciones (Scala + Spark) \u00b6 Scala 2.12 Apache Spark 3.x HDFS (Hadoop 3.x) SBT (build tool de Scala) Kryo (serializaci\u00f3n para Spark) Docker (OpenJDK 11-slim, bitnami/spark) Docker Compose (orquestaci\u00f3n) 4.2.2 Backend (FastAPI) \u00b6 Python 3.12 FastAPI >= 0.100.0 Uvicorn[standard] >= 0.23.0 SQLAlchemy >= 2.0 (Async) asyncpg >= 0.28 Pydantic Settings >= 2.0 python-jose >= 3.3 passlib[bcrypt] >= 1.7 requests >= 2.31 python-dotenv >= 1.0 Docker (para ejecutar contenedores de backend en producci\u00f3n si se desea) 4.2.3 Frontend (React) \u00b6 React 18+ React Router v6 Axios Context API (AuthContext) Vite CSS puro 4.2.4 Base de datos y otros \u00b6 PostgreSQL 13 Kafka (opcional) Zookeeper (opcional) Superset (opcional, para dashboards) PlantUML (diagramas de referencia) 4.3 C\u00f3digo (Explicaci\u00f3n de las partes m\u00e1s interesantes) \u00b6 Se destacan las implementaciones clave de cada componente. 4.3.1 Motor de validaciones (Scala) \u00b6 SparkSessionProvider.scala * Configura la SparkSession con par\u00e1metros personalizados: ```scala val spark = SparkSession.builder() .appName(\"ValidationEngine\") .master(\"spark://spark-master:7077\") .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") .config(\"spark.kryo.registrator\", \"com.mycompany.KryoRegistrator\") .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://hadoop-namenode:9000\") .getOrCreate() ``` * Ajusta particionamiento y directorios locales a trav\u00e9s de SPARK_LOCAL_DIRS . ExecutionManager.scala * Orquesta todas las validaciones: ```scala def executeFile(path: String, outputTable: String): Unit = { val df = Reader.readFile(spark, path) if (!FileSentinel.verifyFiles(df, metadata)) { logTrigger(flag, metadata, path) return } if (!TypeValidator.verifyTyping(df, metadata)) { logTrigger(flag, metadata, path) return } if (!ReferentialIntegrityValidator.verifyIntegrity(df, metadata)) { logTrigger(flag, metadata, path) return } FunctionalValidator.verifyFunctional(df, metadata) match { case Some(error) => logTrigger(errorFlag, metadata, path) return case None => logTrigger(2, metadata, path) } // Registrar en PostgreSQL Writer.writeToJdbc(df, outputTable, dbConfig) // Borrar de HDFS spark.sparkContext.hadoopConfiguration .delete(new Path(path), false) } ``` * logTrigger(flag, metadata, path) : escribe un DataFrame con columns ( logged_at , file_config_id , file_name , field_name , environment , validation_flag , error_message ) y hace df.write.mode(\"append\").jdbc(...) . Multi-Stage Dockerfile ( docker/Dockerfile.engine ) ```dockerfile # Stage 1: Build con OpenJDK y SBT FROM openjdk:11-slim AS builder WORKDIR /app RUN apt-get update && apt-get install -y curl gnupg && \\ echo \"deb https://repo.scala-sbt.org/scalasbt/debian all main\" > /etc/apt/sources.list.d/sbt.list && \\ curl -sL \"https://keyserver.ubuntu.com/pks/lookup?op=get&search=0x99E82A75642AC823\" | apt-key add - && \\ apt-get update && apt-get install -y sbt && rm -rf /var/lib/apt/lists/* COPY ../project project/ COPY ../build.sbt build.sbt COPY ../src src/ COPY ../db.properties db.properties COPY ../src/main/resources/application.conf src/main/resources/ RUN sbt clean assembly # Stage 2: Runtime Spark FROM bitnami/spark:3.3.1 WORKDIR /app COPY --from=builder /app/target/scala-2.12/Fin_de_Grado-assembly-0.1.0-SNAPSHOT.jar app.jar COPY ../db.properties db.properties COPY ../src/main/resources/application.conf application.conf ENV INPUT_DIR=/data/bank_accounts \\ OUTPUT_TABLE=trigger_control \\ POLL_INTERVAL_MS=10000 \\ SPARK_LOCAL_DIRS=/tmp/spark_local ENTRYPOINT spark-submit \\ --class Main \\ --master spark://spark-master:7077 \\ --deploy-mode client \\ --conf spark.driver.host=validation-engine \\ --conf spark.hadoop.fs.defaultFS=hdfs://hadoop-namenode:9000 \\ /app/app.jar ``` * Stage 1: instala SBT, compila fat JAR con todas las dependencias. * Stage 2: parte de una imagen oficial de Spark; copia JAR, db.properties y application.conf ; expone variables de entorno y lanza spark-submit . Script de reconstrucci\u00f3n ( scripts/rebuild_and_run.sh ) ```bash #!/usr/bin/env bash set -euo pipefail # Construir el JAR con SBT sbt clean assembly # Crear red si no existe docker network create superset-net || true # Reconstruir y ejecutar solo el contenedor de validation-engine cd docker docker-compose build validation-engine docker-compose up --abort-on-container-exit validation-engine ``` * Compila el JAR, crea la red superset-net y levanta \u00fanicamente el contenedor del motor de validaciones (asume que los dem\u00e1s servicios est\u00e1n corriendo). 4.3.2 Backend (FastAPI) \u00b6 Configuraci\u00f3n ( app/core/config.py ) ```python from pydantic import BaseSettings class Settings(BaseSettings): postgres_user: str postgres_password: str postgres_host: str postgres_port: int postgres_db: str hdfs_host: str hdfs_port: int hdfs_dir: str hdfs_user: str hdfs_datanode_host: str hdfs_datanode_port: int upload_dir: str jwt_secret_key: str jwt_algorithm: str access_token_expire_minutes: int class Config: env_file = \".env\" ``` * Carga variables desde .env y construye database_url = f\"postgresql+asyncpg://{user}:{password}@{host}:{port}/{db}\" . Seguridad ( app/core/security.py ) ```python from jose import JWTError, jwt from passlib.context import CryptContext from fastapi import Depends, HTTPException, status from fastapi.security import OAuth2PasswordBearer from sqlalchemy.ext.asyncio import AsyncSession from .config import Settings from app.db.models.user import UserModel pwd_context = CryptContext(schemes=[\"bcrypt\"], deprecated=\"auto\") oauth2_scheme = OAuth2PasswordBearer(tokenUrl=\"/auth/login\") def verify_password(plain_password, hashed_password): return pwd_context.verify(plain_password, hashed_password) def get_password_hash(password): return pwd_context.hash(password) def create_access_token(subject: str, expires_delta=None): to_encode = {\"sub\": subject} # agregar expiraci\u00f3n... encoded_jwt = jwt.encode(to_encode, Settings().jwt_secret_key, algorithm=Settings().jwt_algorithm) return encoded_jwt async def get_current_user(token: str = Depends(oauth2_scheme), db: AsyncSession = Depends(get_db)): credentials_exception = HTTPException( status_code=status.HTTP_401_UNAUTHORIZED, detail=\"No autenticado\", headers={\"WWW-Authenticate\": \"Bearer\"}, ) try: payload = jwt.decode(token, Settings().jwt_secret_key, algorithms=[Settings().jwt_algorithm]) user_id: str = payload.get(\"sub\") if user_id is None: raise credentials_exception except JWTError: raise credentials_exception user = await db.get(UserModel, int(user_id)) if user is None: raise credentials_exception return user ``` * Define hashing de contrase\u00f1as, creaci\u00f3n de JWT, y dependencia get_current_user que valida el token y extrae sub . Servicio de ficheros ( app/services/file_service.py ) ```python import os from sqlalchemy.ext.asyncio import AsyncSession from app.db.models.file_configuration import FileConfigurationModel async def save_and_register_file(file, db: AsyncSession) -> int: upload_dir = Settings().upload_dir os.makedirs(upload_dir, exist_ok=True) file_path = os.path.join(upload_dir, file.filename) with open(file_path, \"wb\") as f: content = await file.read() f.write(content) result = await db.execute(select(FileConfigurationModel).where(FileConfigurationModel.file_name == file.filename)) existing = result.scalar_one_or_none() if existing: # actualizar par\u00e1metros si vienen en request existing.some_field = ... await db.commit() return existing.id else: new_cfg = FileConfigurationModel( file_format=\"csv\", path=upload_dir, file_name=file.filename, has_header=True, delimiter=\",\", quote_char='\"', escape_char=\"\\\", date_format=\"yyyy-MM-dd\", timestamp_format=\"yyyy-MM-dd HH:mm:ss\", partition_columns=None ) db.add(new_cfg) await db.flush() return new_cfg.id ``` * Guarda el CSV en disco local y crea/actualiza el registro en file_configuration . Servicio HDFS ( app/services/hdfs_sync.py ) ```python import requests from fastapi import HTTPException from .config import Settings def push_file_to_hdfs(file_name: str): settings = Settings() local_path = os.path.join(settings.upload_dir, file_name) if not os.path.exists(local_path): raise HTTPException(status_code=404, detail=f\"Local not found: {local_path}\") # 1) MKDIRS mkdir_url = f\"http://{settings.hdfs_host}:{settings.hdfs_port}/webhdfs/v1{settings.hdfs_dir}?op=MKDIRS&user.name={settings.hdfs_user}\" resp = requests.put(mkdir_url) resp.raise_for_status() # 2) SETPERM perm_url = f\"http://{settings.hdfs_host}:{settings.hdfs_port}/webhdfs/v1{settings.hdfs_dir}?op=SETPERM&permission=777&user.name={settings.hdfs_user}\" resp = requests.put(perm_url) resp.raise_for_status() # 3) CREATE (NameNode \u2192 307 \u2192 Location \u2192 DataNode) create_url = f\"http://{settings.hdfs_host}:{settings.hdfs_port}/webhdfs/v1{settings.hdfs_dir}/{file_name}?op=CREATE&overwrite=true&user.name={settings.hdfs_user}\" resp = requests.put(create_url, allow_redirects=False) location = resp.headers.get(\"Location\") if not location: raise HTTPException(status_code=500, detail=\"HDFS create failed: no redirect\") # Ajustar URL al DataNode upload_url = location.replace(f\"{settings.hdfs_host}:{settings.hdfs_port}\", f\"{settings.hdfs_datanode_host}:{settings.hdfs_datanode_port}\") with open(local_path, \"rb\") as f: resp2 = requests.put(upload_url, data=f) resp2.raise_for_status() ``` * Gestiona la secuencia WebHDFS (MKDIRS, SETPERM, CREATE \u2192 307 \u2192 PUT a DataNode). Rutas de archivos ( app/api/files.py ) ```python from fastapi import APIRouter, Depends, UploadFile, File, HTTPException from sqlalchemy.ext.asyncio import AsyncSession from app.services.file_service import save_and_register_file from app.services.hdfs_sync import push_file_to_hdfs from app.db.session import get_db router = APIRouter(prefix=\"/files\", tags=[\"files\"]) @router.post(\"/upload\", status_code=201) async def upload_file(file: UploadFile = File(...), db: AsyncSession = Depends(get_db)): try: file_id = await save_and_register_file(file, db) return {\"file_config_id\": file_id} except Exception as e: raise HTTPException(status_code=500, detail=str(e)) @router.post(\"/push/{file_name}\") async def push_to_hdfs(file_name: str, user=Depends(get_current_user)): try: push_file_to_hdfs(file_name) return {\"message\": f\"Pushed {file_name}\"} except HTTPException as he: raise he except Exception as e: raise HTTPException(status_code=500, detail=str(e)) @router.get(\"/\") async def list_configs(db: AsyncSession = Depends(get_db)): result = await db.execute(select(FileConfigurationModel)) configs = result.scalars().all() return configs @router.get(\"/{id}\") async def get_config(id: int, db: AsyncSession = Depends(get_db)): cfg = await db.get(FileConfigurationModel, id) if not cfg: raise HTTPException(status_code=404, detail=\"Not found\") return cfg @router.patch(\"/{id}\") async def update_config(id: int, payload: FileConfigUpdate, db: AsyncSession = Depends(get_db)): cfg = await db.get(FileConfigurationModel, id) if not cfg: raise HTTPException(status_code=404, detail=\"Not found\") for key, val in payload.dict(exclude_unset=True).items(): setattr(cfg, key, val) await db.commit() return cfg @router.delete(\"/{id}\", status_code=204) async def delete_config(id: int, db: AsyncSession = Depends(get_db)): cfg = await db.get(FileConfigurationModel, id) if not cfg: raise HTTPException(status_code=404, detail=\"Not found\") await db.delete(cfg) await db.commit() @router.get(\"/download/{file_name}\") async def download_file(file_name: str): file_path = os.path.join(Settings().upload_dir, file_name) if not os.path.exists(file_path): raise HTTPException(status_code=404, detail=\"File not found\") return FileResponse(file_path, media_type=\"text/csv\", filename=file_name) @router.get(\"/logs\") async def get_logs(environment: str = None, from_date: date = None, to_date: date = None, db: AsyncSession = Depends(get_db)): query = select(TriggerControlModel) if environment: query = query.where(TriggerControlModel.environment == environment) if from_date: query = query.where(TriggerControlModel.logged_at >= from_date) if to_date: query = query.where(TriggerControlModel.logged_at <= to_date) result = await db.execute(query.order_by(TriggerControlModel.logged_at.desc())) logs = result.scalars().all() return logs ``` * Endpoints para CRUD de configuraciones, subida, push a HDFS, descarga y consulta de logs. 4.3.3 Frontend (React + Vite) \u00b6 Configuraci\u00f3n de Axios ( src/api/axiosConfig.js ) ```js import axios from 'axios'; const api = axios.create({ baseURL: 'http://localhost:8000', }); // Interceptor de petici\u00f3n: a\u00f1ade token si existe api.interceptors.request.use(config => { const token = localStorage.getItem('access_token'); if (token) { config.headers.Authorization = Bearer ${token} ; } return config; }); // Interceptor de respuesta: si 401, borrar token y redirigir api.interceptors.response.use( response => response, error => { if (error.response && error.response.status === 401) { localStorage.removeItem('access_token'); alert('Tu sesi\u00f3n ha expirado. Por favor, inicia sesi\u00f3n de nuevo.'); window.location.href = '/login'; } return Promise.reject(error); } ); export default api; ``` * Gestiona token en cada petici\u00f3n y maneja 401 autom\u00e1ticamente. AuthContext.jsx ```jsx import React, { createContext, useState, useEffect } from 'react'; import { useNavigate } from 'react-router-dom'; import api from '../api/axiosConfig'; export const AuthContext = createContext(); export const AuthProvider = ({ children }) => { const [token, setToken] = useState(localStorage.getItem('access_token')); const navigate = useNavigate(); useEffect(() => { if (token) { localStorage.setItem('access_token', token); } else { localStorage.removeItem('access_token'); } }, [token]); const login = async (email, password) => { const res = await api.post('/auth/login', { email, password }); setToken(res.data.access_token); navigate('/dashboard'); }; const register = async (email, password) => { await api.post('/auth/register', { email, password }); navigate('/login'); }; const logout = () => { setToken(null); navigate('/login'); }; return ( {children} ); }; ``` * Gestiona estado global de autenticaci\u00f3n y m\u00e9todos login() , register() , logout() . AppRouter.jsx ```jsx import React, { useContext } from 'react'; import { BrowserRouter, Routes, Route, Navigate, useLocation } from 'react-router-dom'; import { AuthContext } from '../contexts/AuthContext'; import LoginPage from '../pages/LoginPage'; import RegisterPage from '../pages/RegisterPage'; import Dashboard from '../pages/Dashboard'; import LogsPage from '../pages/LogsPage'; import MainLayout from '../layouts/MainLayout'; const RequireAuth = ({ children }) => { const { token } = useContext(AuthContext); const location = useLocation(); return token ? children : ; }; const AppRouter = () => ( } /> } /> \\ \\ }> } /> } /> } /> } /> ); export default AppRouter; ``` * Define rutas p\u00fablicas ( /login , /register ) y privadas ( /dashboard , /logs ) con RequireAuth . Dashboard.jsx ```jsx import React, { useState, useEffect, useContext } from 'react'; import { AuthContext } from '../contexts/AuthContext'; import api from '../api/axiosConfig'; import FileDetailModal from '../components/FileDetailModal'; const Dashboard = () => { const [configs, setConfigs] = useState([]); const [selectedFile, setSelectedFile] = useState(null); const [uploading, setUploading] = useState(false); const [showModal, setShowModal] = useState(false); const [currentConfig, setCurrentConfig] = useState(null); const fetchConfigs = async () => { const res = await api.get('/files/'); setConfigs(res.data); }; useEffect(() => { fetchConfigs(); }, []); const handleUpload = async () => { if (!selectedFile) return; setUploading(true); const form = new FormData(); form.append('file', selectedFile); const res = await api.post('/files/upload', form); alert( Fichero subido ID: ${res.data.file_config_id} ); fetchConfigs(); setUploading(false); }; const handleValidate = async (fileName) => { await api.post( /files/push/${fileName} ); alert('Enviado a validar'); fetchConfigs(); }; const handleDelete = async (id) => { if (!window.confirm('\u00bfEst\u00e1s seguro?')) return; await api.delete( /files/${id} ); alert('Configuraci\u00f3n eliminada'); fetchConfigs(); }; return ( Dashboard setSelectedFile(e.target.files[0])} /> {uploading ? 'Subiendo...' : 'Subir Fichero'} ID Fichero Has Header Delimiter Acciones {configs.map(cfg => ( {cfg.id} {cfg.file_name} {cfg.has_header ? 'S\u00ed' : 'No'} {cfg.delimiter} handleValidate(cfg.file_name)}>Enviando\u2026 { setCurrentConfig(cfg); setShowModal(true); }}>Detalles handleDelete(cfg.id)}>Eliminar ))} {showModal && ( setShowModal(false)} onRefresh={fetchConfigs} /> )} ); }; export default Dashboard; ``` * Subida de fichero, lista de configuraciones con acciones (Validar, Detalles, Eliminar). FileDetailModal.jsx ```jsx import React, { useState } from 'react'; import api from '../api/axiosConfig'; const FileDetailModal = ({ config, onClose, onRefresh }) => { const [delimiter, setDelimiter] = useState(config.delimiter); const [quoteChar, setQuoteChar] = useState(config.quote_char); const [escapeChar, setEscapeChar] = useState(config.escape_char); const [hasHeader, setHasHeader] = useState(config.has_header); const [dateFormat, setDateFormat] = useState(config.date_format); const [timestampFormat, setTimestampFormat] = useState(config.timestamp_format); const [partitionColumns, setPartitionColumns] = useState(config.partition_columns); const [saving, setSaving] = useState(false); const [validating, setValidating] = useState(false); const [deleting, setDeleting] = useState(false); const save = async () => { setSaving(true); await api.patch( /files/${config.id} , { delimiter, quote_char: quoteChar, escape_char: escapeChar, has_header: hasHeader, date_format: dateFormat, timestamp_format: timestampFormat, partition_columns: partitionColumns }); alert('Configuraci\u00f3n actualizada'); setSaving(false); onClose(); onRefresh(); }; const validate = async () => { setValidating(true); await api.post( /files/push/${config.file_name} ); alert('Enviado a validar'); setValidating(false); onClose(); onRefresh(); }; const downloadFile = async () => { const res = await fetch( http://localhost:8000/files/download/${config.file_name} , { headers: { Authorization: Bearer ${localStorage.getItem('access_token')} } }); const blob = await res.blob(); const url = URL.createObjectURL(blob); const a = document.createElement('a'); a.href = url; a.download = config.file_name; a.click(); URL.revokeObjectURL(url); }; const remove = async () => { if (!window.confirm('\u00bfEst\u00e1s seguro?')) return; setDeleting(true); await api.delete( /files/${config.id} ); alert('Configuraci\u00f3n eliminada'); setDeleting(false); onClose(); onRefresh(); }; const overlayStyle = { position: 'fixed', top: 0, left: 0, right: 0, bottom: 0, backgroundColor: 'rgba(0,0,0,0.5)', display: 'flex', justifyContent: 'center', alignItems: 'center' }; const modalStyle = { background: '#fff', padding: '1rem', borderRadius: '8px', position: 'relative', width: '500px' }; return ( e.stopPropagation()}> Detalles Fichero Delimiter: setDelimiter(e.target.value)}> Comma Semicolon Tab Pipe Quote Character: setQuoteChar(e.target.value)}> Double Single None Escape Character: setEscapeChar(e.target.value)}> Backslash Double None Has Header: setHasHeader(e.target.checked)} /> Date Format: setDateFormat(e.target.value)}> YYYY-MM-DD DD/MM/YYYY MM-DD-YYYY Timestamp Format: setTimestampFormat(e.target.value)}> YYYY-MM-DD HH:mm:ss ISO 8601 Timestamp MS Partition Columns: setPartitionColumns(e.target.value)} /> {saving ? 'Guardando...' : 'Guardar'} {validating ? 'Enviando\u2026' : 'Validar'} Descargar {deleting ? 'Eliminando\u2026' : 'Eliminar'} \u00d7 ); }; export default FileDetailModal; ``` * Modal para editar par\u00e1metros, validar, descargar y eliminar configuraci\u00f3n. LogsPage.jsx ```jsx import React, { useState, useEffect } from 'react'; import api from '../api/axiosConfig'; const LogsPage = () => { const [logs, setLogs] = useState([]); const fetchLogs = async () => { const res = await api.get('/files/logs'); setLogs(res.data); }; useEffect(() => { fetchLogs(); }, []); return ( Logs de Validaci\u00f3n ID Config ID Fichero Campo Entorno Flag Mensaje Fecha {logs.map(l => ( {l.id} {l.file_config_id} {l.file_name} {l.field_name} {l.environment} {l.validation_flag} {l.error_message} {new Date(l.logged_at).toLocaleDateString('es-ES')} ))} ); }; export default LogsPage; ``` * Muestra los logs de validaci\u00f3n y formatea la fecha a espa\u00f1ol. 4.4 Organizaci\u00f3n del proyecto. Patr\u00f3n \u00b6 Se propone la siguiente estructura monol\u00edtica (tres carpetas principales), aunque cada componente puede colocarse en repositorios separados seg\u00fan conveniencia: FileMonitoringSystem/ \u251c\u2500\u2500 backend/ \u2190 C\u00f3digo FastAPI \u2502 \u251c\u2500\u2500 app/ \u2502 \u2502 \u251c\u2500\u2500 api/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 auth.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 files.py \u2502 \u2502 \u2502 \u2514\u2500\u2500 health.py \u2502 \u2502 \u251c\u2500\u2500 core/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 config.py \u2502 \u2502 \u2502 \u2514\u2500\u2500 security.py \u2502 \u2502 \u251c\u2500\u2500 db/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 base.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 session.py \u2502 \u2502 \u2502 \u2514\u2500\u2500 models/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 user.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 file_configuration.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 semantic_layer.py \u2190 metadatos de campos \u2502 \u2502 \u2502 \u251c\u2500\u2500 trigger_control.py \u2502 \u2502 \u2502 \u2514\u2500\u2500 negative_flag_logs.py \u2502 \u2502 \u251c\u2500\u2500 services/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 file_service.py \u2502 \u2502 \u2502 \u2514\u2500\u2500 hdfs_sync.py \u2502 \u2502 \u2514\u2500\u2500 schemas/ \u2502 \u2502 \u251c\u2500\u2500 auth.py \u2502 \u2502 \u251c\u2500\u2500 files.py \u2502 \u2502 \u2514\u2500\u2500 logs.py \u2502 \u251c\u2500\u2500 main.py \u2502 \u251c\u2500\u2500 .env \u2502 \u251c\u2500\u2500 requirements.txt \u2502 \u2514\u2500\u2500 test_api.sh \u2502 \u251c\u2500\u2500 frontend/ \u2190 C\u00f3digo React + Vite \u2502 \u251c\u2500\u2500 public/ \u2502 \u2502 \u2514\u2500\u2500 index.html \u2502 \u251c\u2500\u2500 src/ \u2502 \u2502 \u251c\u2500\u2500 api/ \u2502 \u2502 \u2502 \u2514\u2500\u2500 axiosConfig.js \u2502 \u2502 \u251c\u2500\u2500 components/ \u2502 \u2502 \u2502 \u2514\u2500\u2500 FileDetailModal.jsx \u2502 \u2502 \u251c\u2500\u2500 contexts/ \u2502 \u2502 \u2502 \u2514\u2500\u2500 AuthContext.jsx \u2502 \u2502 \u251c\u2500\u2500 layouts/ \u2502 \u2502 \u2502 \u2514\u2500\u2500 MainLayout.jsx \u2502 \u2502 \u251c\u2500\u2500 pages/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 Dashboard.jsx \u2502 \u2502 \u2502 \u251c\u2500\u2500 LoginPage.jsx \u2502 \u2502 \u2502 \u251c\u2500\u2500 RegisterPage.jsx \u2502 \u2502 \u2502 \u2514\u2500\u2500 LogsPage.jsx \u2502 \u2502 \u251c\u2500\u2500 routes/ \u2502 \u2502 \u2502 \u2514\u2500\u2500 AppRouter.jsx \u2502 \u2502 \u251c\u2500\u2500 styles/ \u2502 \u2502 \u2502 \u2514\u2500\u2500 index.css \u2502 \u2502 \u2514\u2500\u2500 main.jsx \u2502 \u251c\u2500\u2500 package.json \u2502 \u2514\u2500\u2500 README.md \u2502 \u251c\u2500\u2500 engine/ \u2190 Motor de validaciones Scala + Spark \u2502 \u251c\u2500\u2500 docker/ \u2502 \u2502 \u251c\u2500\u2500 Dockerfile.engine \u2502 \u2502 \u251c\u2500\u2500 Dockerfile.superset \u2502 \u2502 \u2514\u2500\u2500 docker-compose.yml \u2502 \u251c\u2500\u2500 docs/ \u2190 Diagramas PNG \u2502 \u2502 \u251c\u2500\u2500 diagrama_casos_de_uso.png \u2502 \u2502 \u251c\u2500\u2500 ERD.png \u2502 \u2502 \u251c\u2500\u2500 diagrama_clases.png \u2502 \u2502 \u251c\u2500\u2500 diagrama_secuencia.png \u2502 \u2502 \u2514\u2500\u2500 ... \u2502 \u251c\u2500\u2500 scripts/ \u2502 \u2502 \u2514\u2500\u2500 rebuild_and_run.sh \u2502 \u251c\u2500\u2500 src/ \u2502 \u2502 \u251c\u2500\u2500 main/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 resources/ \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 application.conf \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 db.properties \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 files/ \u2190 Scripts Python para generar CSV de prueba \u2502 \u2502 \u2502 \u2514\u2500\u2500 scala/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 Main.scala \u2502 \u2502 \u2502 \u251c\u2500\u2500 config/ \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 DbConfig.scala \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 DBConnection.scala \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 SparkSessionProvider.scala \u2502 \u2502 \u2502 \u251c\u2500\u2500 models/ \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 FileConfigurationCaseClass.scala \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 SemanticLayerCaseClass.scala \u2502 \u2502 \u2502 \u251c\u2500\u2500 services/ \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 ExecutionManager.scala \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 TriggerIdManager.scala \u2502 \u2502 \u2502 \u251c\u2500\u2500 utils/ \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 Reader.scala \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 Writer.scala \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 FileManager.scala \u2502 \u2502 \u2502 \u2514\u2500\u2500 validators/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 FileSentinel.scala \u2502 \u2502 \u2502 \u251c\u2500\u2500 TypeValidator.scala \u2502 \u2502 \u2502 \u251c\u2500\u2500 ReferentialIntegrityValidator.scala \u2502 \u2502 \u2502 \u2514\u2500\u2500 FunctionalValidator.scala \u2502 \u2502 \u251c\u2500\u2500 test/scala/ \u2190 Pruebas unitarias (ScalaTest) \u2502 \u2502 \u2514\u2500\u2500 build.sbt \u2502 \u2514\u2500\u2500 README-cluster.md \u2190 Gu\u00eda de cluster Hadoop/Spark/Kafka/Superset \u2502 \u251c\u2500\u2500 docs/ \u2190 Carpeta MkDocs (documentaci\u00f3n \u201clibro\u201d) \u2502 \u251c\u2500\u2500 01-introduccion.md \u2502 \u251c\u2500\u2500 02-requisitos.md \u2502 \u251c\u2500\u2500 03-diseno.md \u2502 \u251c\u2500\u2500 04-implementacion.md \u2502 \u251c\u2500\u2500 05-resultado.md \u2502 \u251c\u2500\u2500 06-conclusiones.md \u2502 \u2514\u2500\u2500 img/ \u2502 \u251c\u2500\u2500 diagrama_casos_de_uso.png \u2502 \u251c\u2500\u2500 ERD.png \u2502 \u251c\u2500\u2500 diagrama_clases.png \u2502 \u251c\u2500\u2500 diagrama_secuencia.png \u2502 \u2514\u2500\u2500 ... (otros PNG) \u2502 \u251c\u2500\u2500 mkdocs.yml \u2190 Configuraci\u00f3n de MkDocs \u251c\u2500\u2500 .github/ \u2502 \u2514\u2500\u2500 workflows/ \u2502 \u2514\u2500\u2500 mkdocs-deploy.yml \u2190 GitHub Action para desplegar en Pages \u2502 \u2514\u2500\u2500 README-cluster.md \u2190 Gu\u00eda de cluster (si quieres) El patr\u00f3n es feature-based : cada carpeta agrupa archivos seg\u00fan responsabilidad (API, servicios, modelos, diagramas, etc.). Cada componente puede versionarse por separado y luego combinarse en un monorepo si se desea.","title":"Implementaci\u00f3n (GIT)"},{"location":"04-implementacion.html#4-implementacion-git","text":"","title":"4. Implementaci\u00f3n (GIT)"},{"location":"04-implementacion.html#41-diagrama-de-arquitectura","text":"--- config: layout: dagre --- flowchart LR subgraph Usuario[\"Usuario\"] Browser[\"User Browser\"] end subgraph Frontend[\"Frontend\"] React[\"React SPA<br>(Web App)\"] end subgraph API[\"API\"] FastAPI[\"FastAPI Backend<br>(Python)\"] end subgraph Persistencia[\"Persistencia\"] Postgres[\"PostgreSQL DB\"] HDFS[\"HDFS Cluster\"] end subgraph Procesamiento[\"Procesamiento\"] SparkMaster[\"\ud83d\udd37 Spark Master\"] SparkWorker1[\"\ud83d\udd36 Spark Worker 1\"] SparkWorker2[\"\ud83d\udd36 Spark Worker 2\"] ValidationEngine[\"Validation Engine<br>(Scala + Spark)\"] end subgraph subGraph7[\"Contenedores Docker\"] Superset[\"Superset BI\"] end Browser -- HTTP requests --> React React -- REST/GraphQL<br>JSON over HTTP --> FastAPI FastAPI -- SQLAlchemy (INSERT/SELECT/UPDATE) --> Postgres FastAPI -- WebHDFS / HDFS Client --> HDFS FastAPI -- SELECT * FROM TABLE --> Postgres ValidationEngine -- Leer CSV/Parquet --> HDFS ValidationEngine -- INSERT logs en trigger_control --> Postgres ValidationEngine -- Spark job submission --> SparkMaster SparkMaster --> SparkWorker1 & SparkWorker2 SparkMaster -- HDFS FS Default --- HDFS SparkWorker1 -- HDFS FS Default --- HDFS SparkWorker2 -- HDFS FS Default --- HDFS Superset -- SQLAlchemy --> Postgres style Usuario fill:#f9f,stroke:#333,stroke-width:1px style Frontend fill:#ccf,stroke:#333,stroke-width:1px style API fill:#cfc,stroke:#333,stroke-width:1px style Persistencia fill:#ffe599,stroke:#333,stroke-width:1px style Procesamiento fill:#f4cccc,stroke:#333,stroke-width:1px (C\u00f3digo de diagrama mermaid, renderizable en https://mermaid.js.org/) Frontend (React + Vite) Usuario interact\u00faa con la UI, llama rutas REST al backend y muestra datos. Backend (FastAPI) Exposici\u00f3n de endpoints /auth , /files , /files/logs . Usa SQLAlchemy Async + AsyncPG para comunicarse con PostgreSQL. Utiliza requests para llamar a WebHDFS (NameNode/DataNode). PostgreSQL Almacena users , file_configuration , trigger_control , negative_flag_logs . Superset (opcional) tambi\u00e9n se conecta aqu\u00ed para dashboards. Motor Scala + Spark Se ejecuta como contenedor independiente, se conecta a HDFS ( hdfs://hadoop-namenode:9000 ). Procesa ficheros mediante Spark Streaming en modo batch y escribe logs en PostgreSQL mediante JDBC. HDFS (NameNode/DataNode) Almacena CSV en /data/bank_accounts . Motor de validaciones lee ficheros desde aqu\u00ed y borra tras procesar. (Ver docs/diagrama_arquitectura.png para imagen detallada.)","title":"4.1 Diagrama de arquitectura"},{"location":"04-implementacion.html#42-tecnologias","text":"A continuaci\u00f3n se especifican todas las tecnolog\u00edas y dependencias utilizadas en cada componente:","title":"4.2 Tecnolog\u00edas"},{"location":"04-implementacion.html#421-motor-de-validaciones-scala-spark","text":"Scala 2.12 Apache Spark 3.x HDFS (Hadoop 3.x) SBT (build tool de Scala) Kryo (serializaci\u00f3n para Spark) Docker (OpenJDK 11-slim, bitnami/spark) Docker Compose (orquestaci\u00f3n)","title":"4.2.1 Motor de validaciones (Scala + Spark)"},{"location":"04-implementacion.html#422-backend-fastapi","text":"Python 3.12 FastAPI >= 0.100.0 Uvicorn[standard] >= 0.23.0 SQLAlchemy >= 2.0 (Async) asyncpg >= 0.28 Pydantic Settings >= 2.0 python-jose >= 3.3 passlib[bcrypt] >= 1.7 requests >= 2.31 python-dotenv >= 1.0 Docker (para ejecutar contenedores de backend en producci\u00f3n si se desea)","title":"4.2.2 Backend (FastAPI)"},{"location":"04-implementacion.html#423-frontend-react","text":"React 18+ React Router v6 Axios Context API (AuthContext) Vite CSS puro","title":"4.2.3 Frontend (React)"},{"location":"04-implementacion.html#424-base-de-datos-y-otros","text":"PostgreSQL 13 Kafka (opcional) Zookeeper (opcional) Superset (opcional, para dashboards) PlantUML (diagramas de referencia)","title":"4.2.4 Base de datos y otros"},{"location":"04-implementacion.html#43-codigo-explicacion-de-las-partes-mas-interesantes","text":"Se destacan las implementaciones clave de cada componente.","title":"4.3 C\u00f3digo (Explicaci\u00f3n de las partes m\u00e1s interesantes)"},{"location":"04-implementacion.html#431-motor-de-validaciones-scala","text":"SparkSessionProvider.scala * Configura la SparkSession con par\u00e1metros personalizados: ```scala val spark = SparkSession.builder() .appName(\"ValidationEngine\") .master(\"spark://spark-master:7077\") .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") .config(\"spark.kryo.registrator\", \"com.mycompany.KryoRegistrator\") .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://hadoop-namenode:9000\") .getOrCreate() ``` * Ajusta particionamiento y directorios locales a trav\u00e9s de SPARK_LOCAL_DIRS . ExecutionManager.scala * Orquesta todas las validaciones: ```scala def executeFile(path: String, outputTable: String): Unit = { val df = Reader.readFile(spark, path) if (!FileSentinel.verifyFiles(df, metadata)) { logTrigger(flag, metadata, path) return } if (!TypeValidator.verifyTyping(df, metadata)) { logTrigger(flag, metadata, path) return } if (!ReferentialIntegrityValidator.verifyIntegrity(df, metadata)) { logTrigger(flag, metadata, path) return } FunctionalValidator.verifyFunctional(df, metadata) match { case Some(error) => logTrigger(errorFlag, metadata, path) return case None => logTrigger(2, metadata, path) } // Registrar en PostgreSQL Writer.writeToJdbc(df, outputTable, dbConfig) // Borrar de HDFS spark.sparkContext.hadoopConfiguration .delete(new Path(path), false) } ``` * logTrigger(flag, metadata, path) : escribe un DataFrame con columns ( logged_at , file_config_id , file_name , field_name , environment , validation_flag , error_message ) y hace df.write.mode(\"append\").jdbc(...) . Multi-Stage Dockerfile ( docker/Dockerfile.engine ) ```dockerfile # Stage 1: Build con OpenJDK y SBT FROM openjdk:11-slim AS builder WORKDIR /app RUN apt-get update && apt-get install -y curl gnupg && \\ echo \"deb https://repo.scala-sbt.org/scalasbt/debian all main\" > /etc/apt/sources.list.d/sbt.list && \\ curl -sL \"https://keyserver.ubuntu.com/pks/lookup?op=get&search=0x99E82A75642AC823\" | apt-key add - && \\ apt-get update && apt-get install -y sbt && rm -rf /var/lib/apt/lists/* COPY ../project project/ COPY ../build.sbt build.sbt COPY ../src src/ COPY ../db.properties db.properties COPY ../src/main/resources/application.conf src/main/resources/ RUN sbt clean assembly # Stage 2: Runtime Spark FROM bitnami/spark:3.3.1 WORKDIR /app COPY --from=builder /app/target/scala-2.12/Fin_de_Grado-assembly-0.1.0-SNAPSHOT.jar app.jar COPY ../db.properties db.properties COPY ../src/main/resources/application.conf application.conf ENV INPUT_DIR=/data/bank_accounts \\ OUTPUT_TABLE=trigger_control \\ POLL_INTERVAL_MS=10000 \\ SPARK_LOCAL_DIRS=/tmp/spark_local ENTRYPOINT spark-submit \\ --class Main \\ --master spark://spark-master:7077 \\ --deploy-mode client \\ --conf spark.driver.host=validation-engine \\ --conf spark.hadoop.fs.defaultFS=hdfs://hadoop-namenode:9000 \\ /app/app.jar ``` * Stage 1: instala SBT, compila fat JAR con todas las dependencias. * Stage 2: parte de una imagen oficial de Spark; copia JAR, db.properties y application.conf ; expone variables de entorno y lanza spark-submit . Script de reconstrucci\u00f3n ( scripts/rebuild_and_run.sh ) ```bash #!/usr/bin/env bash set -euo pipefail # Construir el JAR con SBT sbt clean assembly # Crear red si no existe docker network create superset-net || true # Reconstruir y ejecutar solo el contenedor de validation-engine cd docker docker-compose build validation-engine docker-compose up --abort-on-container-exit validation-engine ``` * Compila el JAR, crea la red superset-net y levanta \u00fanicamente el contenedor del motor de validaciones (asume que los dem\u00e1s servicios est\u00e1n corriendo).","title":"4.3.1 Motor de validaciones (Scala)"},{"location":"04-implementacion.html#432-backend-fastapi","text":"Configuraci\u00f3n ( app/core/config.py ) ```python from pydantic import BaseSettings class Settings(BaseSettings): postgres_user: str postgres_password: str postgres_host: str postgres_port: int postgres_db: str hdfs_host: str hdfs_port: int hdfs_dir: str hdfs_user: str hdfs_datanode_host: str hdfs_datanode_port: int upload_dir: str jwt_secret_key: str jwt_algorithm: str access_token_expire_minutes: int class Config: env_file = \".env\" ``` * Carga variables desde .env y construye database_url = f\"postgresql+asyncpg://{user}:{password}@{host}:{port}/{db}\" . Seguridad ( app/core/security.py ) ```python from jose import JWTError, jwt from passlib.context import CryptContext from fastapi import Depends, HTTPException, status from fastapi.security import OAuth2PasswordBearer from sqlalchemy.ext.asyncio import AsyncSession from .config import Settings from app.db.models.user import UserModel pwd_context = CryptContext(schemes=[\"bcrypt\"], deprecated=\"auto\") oauth2_scheme = OAuth2PasswordBearer(tokenUrl=\"/auth/login\") def verify_password(plain_password, hashed_password): return pwd_context.verify(plain_password, hashed_password) def get_password_hash(password): return pwd_context.hash(password) def create_access_token(subject: str, expires_delta=None): to_encode = {\"sub\": subject} # agregar expiraci\u00f3n... encoded_jwt = jwt.encode(to_encode, Settings().jwt_secret_key, algorithm=Settings().jwt_algorithm) return encoded_jwt async def get_current_user(token: str = Depends(oauth2_scheme), db: AsyncSession = Depends(get_db)): credentials_exception = HTTPException( status_code=status.HTTP_401_UNAUTHORIZED, detail=\"No autenticado\", headers={\"WWW-Authenticate\": \"Bearer\"}, ) try: payload = jwt.decode(token, Settings().jwt_secret_key, algorithms=[Settings().jwt_algorithm]) user_id: str = payload.get(\"sub\") if user_id is None: raise credentials_exception except JWTError: raise credentials_exception user = await db.get(UserModel, int(user_id)) if user is None: raise credentials_exception return user ``` * Define hashing de contrase\u00f1as, creaci\u00f3n de JWT, y dependencia get_current_user que valida el token y extrae sub . Servicio de ficheros ( app/services/file_service.py ) ```python import os from sqlalchemy.ext.asyncio import AsyncSession from app.db.models.file_configuration import FileConfigurationModel async def save_and_register_file(file, db: AsyncSession) -> int: upload_dir = Settings().upload_dir os.makedirs(upload_dir, exist_ok=True) file_path = os.path.join(upload_dir, file.filename) with open(file_path, \"wb\") as f: content = await file.read() f.write(content) result = await db.execute(select(FileConfigurationModel).where(FileConfigurationModel.file_name == file.filename)) existing = result.scalar_one_or_none() if existing: # actualizar par\u00e1metros si vienen en request existing.some_field = ... await db.commit() return existing.id else: new_cfg = FileConfigurationModel( file_format=\"csv\", path=upload_dir, file_name=file.filename, has_header=True, delimiter=\",\", quote_char='\"', escape_char=\"\\\", date_format=\"yyyy-MM-dd\", timestamp_format=\"yyyy-MM-dd HH:mm:ss\", partition_columns=None ) db.add(new_cfg) await db.flush() return new_cfg.id ``` * Guarda el CSV en disco local y crea/actualiza el registro en file_configuration . Servicio HDFS ( app/services/hdfs_sync.py ) ```python import requests from fastapi import HTTPException from .config import Settings def push_file_to_hdfs(file_name: str): settings = Settings() local_path = os.path.join(settings.upload_dir, file_name) if not os.path.exists(local_path): raise HTTPException(status_code=404, detail=f\"Local not found: {local_path}\") # 1) MKDIRS mkdir_url = f\"http://{settings.hdfs_host}:{settings.hdfs_port}/webhdfs/v1{settings.hdfs_dir}?op=MKDIRS&user.name={settings.hdfs_user}\" resp = requests.put(mkdir_url) resp.raise_for_status() # 2) SETPERM perm_url = f\"http://{settings.hdfs_host}:{settings.hdfs_port}/webhdfs/v1{settings.hdfs_dir}?op=SETPERM&permission=777&user.name={settings.hdfs_user}\" resp = requests.put(perm_url) resp.raise_for_status() # 3) CREATE (NameNode \u2192 307 \u2192 Location \u2192 DataNode) create_url = f\"http://{settings.hdfs_host}:{settings.hdfs_port}/webhdfs/v1{settings.hdfs_dir}/{file_name}?op=CREATE&overwrite=true&user.name={settings.hdfs_user}\" resp = requests.put(create_url, allow_redirects=False) location = resp.headers.get(\"Location\") if not location: raise HTTPException(status_code=500, detail=\"HDFS create failed: no redirect\") # Ajustar URL al DataNode upload_url = location.replace(f\"{settings.hdfs_host}:{settings.hdfs_port}\", f\"{settings.hdfs_datanode_host}:{settings.hdfs_datanode_port}\") with open(local_path, \"rb\") as f: resp2 = requests.put(upload_url, data=f) resp2.raise_for_status() ``` * Gestiona la secuencia WebHDFS (MKDIRS, SETPERM, CREATE \u2192 307 \u2192 PUT a DataNode). Rutas de archivos ( app/api/files.py ) ```python from fastapi import APIRouter, Depends, UploadFile, File, HTTPException from sqlalchemy.ext.asyncio import AsyncSession from app.services.file_service import save_and_register_file from app.services.hdfs_sync import push_file_to_hdfs from app.db.session import get_db router = APIRouter(prefix=\"/files\", tags=[\"files\"]) @router.post(\"/upload\", status_code=201) async def upload_file(file: UploadFile = File(...), db: AsyncSession = Depends(get_db)): try: file_id = await save_and_register_file(file, db) return {\"file_config_id\": file_id} except Exception as e: raise HTTPException(status_code=500, detail=str(e)) @router.post(\"/push/{file_name}\") async def push_to_hdfs(file_name: str, user=Depends(get_current_user)): try: push_file_to_hdfs(file_name) return {\"message\": f\"Pushed {file_name}\"} except HTTPException as he: raise he except Exception as e: raise HTTPException(status_code=500, detail=str(e)) @router.get(\"/\") async def list_configs(db: AsyncSession = Depends(get_db)): result = await db.execute(select(FileConfigurationModel)) configs = result.scalars().all() return configs @router.get(\"/{id}\") async def get_config(id: int, db: AsyncSession = Depends(get_db)): cfg = await db.get(FileConfigurationModel, id) if not cfg: raise HTTPException(status_code=404, detail=\"Not found\") return cfg @router.patch(\"/{id}\") async def update_config(id: int, payload: FileConfigUpdate, db: AsyncSession = Depends(get_db)): cfg = await db.get(FileConfigurationModel, id) if not cfg: raise HTTPException(status_code=404, detail=\"Not found\") for key, val in payload.dict(exclude_unset=True).items(): setattr(cfg, key, val) await db.commit() return cfg @router.delete(\"/{id}\", status_code=204) async def delete_config(id: int, db: AsyncSession = Depends(get_db)): cfg = await db.get(FileConfigurationModel, id) if not cfg: raise HTTPException(status_code=404, detail=\"Not found\") await db.delete(cfg) await db.commit() @router.get(\"/download/{file_name}\") async def download_file(file_name: str): file_path = os.path.join(Settings().upload_dir, file_name) if not os.path.exists(file_path): raise HTTPException(status_code=404, detail=\"File not found\") return FileResponse(file_path, media_type=\"text/csv\", filename=file_name) @router.get(\"/logs\") async def get_logs(environment: str = None, from_date: date = None, to_date: date = None, db: AsyncSession = Depends(get_db)): query = select(TriggerControlModel) if environment: query = query.where(TriggerControlModel.environment == environment) if from_date: query = query.where(TriggerControlModel.logged_at >= from_date) if to_date: query = query.where(TriggerControlModel.logged_at <= to_date) result = await db.execute(query.order_by(TriggerControlModel.logged_at.desc())) logs = result.scalars().all() return logs ``` * Endpoints para CRUD de configuraciones, subida, push a HDFS, descarga y consulta de logs.","title":"4.3.2 Backend (FastAPI)"},{"location":"04-implementacion.html#433-frontend-react-vite","text":"Configuraci\u00f3n de Axios ( src/api/axiosConfig.js ) ```js import axios from 'axios'; const api = axios.create({ baseURL: 'http://localhost:8000', }); // Interceptor de petici\u00f3n: a\u00f1ade token si existe api.interceptors.request.use(config => { const token = localStorage.getItem('access_token'); if (token) { config.headers.Authorization = Bearer ${token} ; } return config; }); // Interceptor de respuesta: si 401, borrar token y redirigir api.interceptors.response.use( response => response, error => { if (error.response && error.response.status === 401) { localStorage.removeItem('access_token'); alert('Tu sesi\u00f3n ha expirado. Por favor, inicia sesi\u00f3n de nuevo.'); window.location.href = '/login'; } return Promise.reject(error); } ); export default api; ``` * Gestiona token en cada petici\u00f3n y maneja 401 autom\u00e1ticamente. AuthContext.jsx ```jsx import React, { createContext, useState, useEffect } from 'react'; import { useNavigate } from 'react-router-dom'; import api from '../api/axiosConfig'; export const AuthContext = createContext(); export const AuthProvider = ({ children }) => { const [token, setToken] = useState(localStorage.getItem('access_token')); const navigate = useNavigate(); useEffect(() => { if (token) { localStorage.setItem('access_token', token); } else { localStorage.removeItem('access_token'); } }, [token]); const login = async (email, password) => { const res = await api.post('/auth/login', { email, password }); setToken(res.data.access_token); navigate('/dashboard'); }; const register = async (email, password) => { await api.post('/auth/register', { email, password }); navigate('/login'); }; const logout = () => { setToken(null); navigate('/login'); }; return ( {children} ); }; ``` * Gestiona estado global de autenticaci\u00f3n y m\u00e9todos login() , register() , logout() . AppRouter.jsx ```jsx import React, { useContext } from 'react'; import { BrowserRouter, Routes, Route, Navigate, useLocation } from 'react-router-dom'; import { AuthContext } from '../contexts/AuthContext'; import LoginPage from '../pages/LoginPage'; import RegisterPage from '../pages/RegisterPage'; import Dashboard from '../pages/Dashboard'; import LogsPage from '../pages/LogsPage'; import MainLayout from '../layouts/MainLayout'; const RequireAuth = ({ children }) => { const { token } = useContext(AuthContext); const location = useLocation(); return token ? children : ; }; const AppRouter = () => ( } /> } /> \\ \\ }> } /> } /> } /> } /> ); export default AppRouter; ``` * Define rutas p\u00fablicas ( /login , /register ) y privadas ( /dashboard , /logs ) con RequireAuth . Dashboard.jsx ```jsx import React, { useState, useEffect, useContext } from 'react'; import { AuthContext } from '../contexts/AuthContext'; import api from '../api/axiosConfig'; import FileDetailModal from '../components/FileDetailModal'; const Dashboard = () => { const [configs, setConfigs] = useState([]); const [selectedFile, setSelectedFile] = useState(null); const [uploading, setUploading] = useState(false); const [showModal, setShowModal] = useState(false); const [currentConfig, setCurrentConfig] = useState(null); const fetchConfigs = async () => { const res = await api.get('/files/'); setConfigs(res.data); }; useEffect(() => { fetchConfigs(); }, []); const handleUpload = async () => { if (!selectedFile) return; setUploading(true); const form = new FormData(); form.append('file', selectedFile); const res = await api.post('/files/upload', form); alert( Fichero subido ID: ${res.data.file_config_id} ); fetchConfigs(); setUploading(false); }; const handleValidate = async (fileName) => { await api.post( /files/push/${fileName} ); alert('Enviado a validar'); fetchConfigs(); }; const handleDelete = async (id) => { if (!window.confirm('\u00bfEst\u00e1s seguro?')) return; await api.delete( /files/${id} ); alert('Configuraci\u00f3n eliminada'); fetchConfigs(); }; return (","title":"4.3.3 Frontend (React + Vite)"},{"location":"04-implementacion.html#44-organizacion-del-proyecto-patron","text":"Se propone la siguiente estructura monol\u00edtica (tres carpetas principales), aunque cada componente puede colocarse en repositorios separados seg\u00fan conveniencia: FileMonitoringSystem/ \u251c\u2500\u2500 backend/ \u2190 C\u00f3digo FastAPI \u2502 \u251c\u2500\u2500 app/ \u2502 \u2502 \u251c\u2500\u2500 api/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 auth.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 files.py \u2502 \u2502 \u2502 \u2514\u2500\u2500 health.py \u2502 \u2502 \u251c\u2500\u2500 core/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 config.py \u2502 \u2502 \u2502 \u2514\u2500\u2500 security.py \u2502 \u2502 \u251c\u2500\u2500 db/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 base.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 session.py \u2502 \u2502 \u2502 \u2514\u2500\u2500 models/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 user.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 file_configuration.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 semantic_layer.py \u2190 metadatos de campos \u2502 \u2502 \u2502 \u251c\u2500\u2500 trigger_control.py \u2502 \u2502 \u2502 \u2514\u2500\u2500 negative_flag_logs.py \u2502 \u2502 \u251c\u2500\u2500 services/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 file_service.py \u2502 \u2502 \u2502 \u2514\u2500\u2500 hdfs_sync.py \u2502 \u2502 \u2514\u2500\u2500 schemas/ \u2502 \u2502 \u251c\u2500\u2500 auth.py \u2502 \u2502 \u251c\u2500\u2500 files.py \u2502 \u2502 \u2514\u2500\u2500 logs.py \u2502 \u251c\u2500\u2500 main.py \u2502 \u251c\u2500\u2500 .env \u2502 \u251c\u2500\u2500 requirements.txt \u2502 \u2514\u2500\u2500 test_api.sh \u2502 \u251c\u2500\u2500 frontend/ \u2190 C\u00f3digo React + Vite \u2502 \u251c\u2500\u2500 public/ \u2502 \u2502 \u2514\u2500\u2500 index.html \u2502 \u251c\u2500\u2500 src/ \u2502 \u2502 \u251c\u2500\u2500 api/ \u2502 \u2502 \u2502 \u2514\u2500\u2500 axiosConfig.js \u2502 \u2502 \u251c\u2500\u2500 components/ \u2502 \u2502 \u2502 \u2514\u2500\u2500 FileDetailModal.jsx \u2502 \u2502 \u251c\u2500\u2500 contexts/ \u2502 \u2502 \u2502 \u2514\u2500\u2500 AuthContext.jsx \u2502 \u2502 \u251c\u2500\u2500 layouts/ \u2502 \u2502 \u2502 \u2514\u2500\u2500 MainLayout.jsx \u2502 \u2502 \u251c\u2500\u2500 pages/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 Dashboard.jsx \u2502 \u2502 \u2502 \u251c\u2500\u2500 LoginPage.jsx \u2502 \u2502 \u2502 \u251c\u2500\u2500 RegisterPage.jsx \u2502 \u2502 \u2502 \u2514\u2500\u2500 LogsPage.jsx \u2502 \u2502 \u251c\u2500\u2500 routes/ \u2502 \u2502 \u2502 \u2514\u2500\u2500 AppRouter.jsx \u2502 \u2502 \u251c\u2500\u2500 styles/ \u2502 \u2502 \u2502 \u2514\u2500\u2500 index.css \u2502 \u2502 \u2514\u2500\u2500 main.jsx \u2502 \u251c\u2500\u2500 package.json \u2502 \u2514\u2500\u2500 README.md \u2502 \u251c\u2500\u2500 engine/ \u2190 Motor de validaciones Scala + Spark \u2502 \u251c\u2500\u2500 docker/ \u2502 \u2502 \u251c\u2500\u2500 Dockerfile.engine \u2502 \u2502 \u251c\u2500\u2500 Dockerfile.superset \u2502 \u2502 \u2514\u2500\u2500 docker-compose.yml \u2502 \u251c\u2500\u2500 docs/ \u2190 Diagramas PNG \u2502 \u2502 \u251c\u2500\u2500 diagrama_casos_de_uso.png \u2502 \u2502 \u251c\u2500\u2500 ERD.png \u2502 \u2502 \u251c\u2500\u2500 diagrama_clases.png \u2502 \u2502 \u251c\u2500\u2500 diagrama_secuencia.png \u2502 \u2502 \u2514\u2500\u2500 ... \u2502 \u251c\u2500\u2500 scripts/ \u2502 \u2502 \u2514\u2500\u2500 rebuild_and_run.sh \u2502 \u251c\u2500\u2500 src/ \u2502 \u2502 \u251c\u2500\u2500 main/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 resources/ \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 application.conf \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 db.properties \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 files/ \u2190 Scripts Python para generar CSV de prueba \u2502 \u2502 \u2502 \u2514\u2500\u2500 scala/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 Main.scala \u2502 \u2502 \u2502 \u251c\u2500\u2500 config/ \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 DbConfig.scala \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 DBConnection.scala \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 SparkSessionProvider.scala \u2502 \u2502 \u2502 \u251c\u2500\u2500 models/ \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 FileConfigurationCaseClass.scala \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 SemanticLayerCaseClass.scala \u2502 \u2502 \u2502 \u251c\u2500\u2500 services/ \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 ExecutionManager.scala \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 TriggerIdManager.scala \u2502 \u2502 \u2502 \u251c\u2500\u2500 utils/ \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 Reader.scala \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 Writer.scala \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 FileManager.scala \u2502 \u2502 \u2502 \u2514\u2500\u2500 validators/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 FileSentinel.scala \u2502 \u2502 \u2502 \u251c\u2500\u2500 TypeValidator.scala \u2502 \u2502 \u2502 \u251c\u2500\u2500 ReferentialIntegrityValidator.scala \u2502 \u2502 \u2502 \u2514\u2500\u2500 FunctionalValidator.scala \u2502 \u2502 \u251c\u2500\u2500 test/scala/ \u2190 Pruebas unitarias (ScalaTest) \u2502 \u2502 \u2514\u2500\u2500 build.sbt \u2502 \u2514\u2500\u2500 README-cluster.md \u2190 Gu\u00eda de cluster Hadoop/Spark/Kafka/Superset \u2502 \u251c\u2500\u2500 docs/ \u2190 Carpeta MkDocs (documentaci\u00f3n \u201clibro\u201d) \u2502 \u251c\u2500\u2500 01-introduccion.md \u2502 \u251c\u2500\u2500 02-requisitos.md \u2502 \u251c\u2500\u2500 03-diseno.md \u2502 \u251c\u2500\u2500 04-implementacion.md \u2502 \u251c\u2500\u2500 05-resultado.md \u2502 \u251c\u2500\u2500 06-conclusiones.md \u2502 \u2514\u2500\u2500 img/ \u2502 \u251c\u2500\u2500 diagrama_casos_de_uso.png \u2502 \u251c\u2500\u2500 ERD.png \u2502 \u251c\u2500\u2500 diagrama_clases.png \u2502 \u251c\u2500\u2500 diagrama_secuencia.png \u2502 \u2514\u2500\u2500 ... (otros PNG) \u2502 \u251c\u2500\u2500 mkdocs.yml \u2190 Configuraci\u00f3n de MkDocs \u251c\u2500\u2500 .github/ \u2502 \u2514\u2500\u2500 workflows/ \u2502 \u2514\u2500\u2500 mkdocs-deploy.yml \u2190 GitHub Action para desplegar en Pages \u2502 \u2514\u2500\u2500 README-cluster.md \u2190 Gu\u00eda de cluster (si quieres) El patr\u00f3n es feature-based : cada carpeta agrupa archivos seg\u00fan responsabilidad (API, servicios, modelos, diagramas, etc.). Cada componente puede versionarse por separado y luego combinarse en un monorepo si se desea.","title":"4.4 Organizaci\u00f3n del proyecto. Patr\u00f3n"},{"location":"05-resultado.html","text":"5. Resultado (Manual de usuario) \u00b6 5.1 Requisitos previos \u00b6 Requisitos comunes: Docker Desktop (Linux, macOS o Windows) con 160 GB libres en disco. Docker Compose (versi\u00f3n \u2265 1.27). Node.js v16+ y npm (para frontend). Python 3.12 , pip (para backend). Clonar el repositorio: bash git clone https://github.com/JuanraDAM/FileMonitoringSystem.git cd FileMonitoringSystem 5.1.1 Motor de validaciones \u00b6 Espacio en disco para contenedores Hadoop/Spark y datos HDFS. Puertos: * 9000 (NameNode) * 9870 (NameNode UI) * 9864 (DataNode) * 7077 (Spark Master) * 8080 (Spark Master UI) * 8081, 8082 (Spark Workers UI) * SBT instalado localmente si se desea compilar sin Docker. 5.1.2 Backend (FastAPI) \u00b6 Crear entorno virtual Python 3.12: bash python3.12 -m venv backend/.venv source backend/.venv/bin/activate pip install --upgrade pip pip install -r backend/requirements.txt * Archivo .env en backend/ con credenciales y configuraci\u00f3n de HDFS. 5.1.3 Frontend (React) \u00b6 Desde frontend/ : bash cd frontend npm install * No hay variables de entorno adicionales, asume que el backend corre en http://localhost:8000 . 5.2 Pasos de uso \u00b6 A continuaci\u00f3n se describen los pasos para levantar cada componente y usar el sistema completo. 5.2.1 Levantar infraestructura con Docker Compose \u00b6 Ir a la carpeta del motor : bash cd engine/docker Crear la red Docker (si no existe): bash docker network create superset-net || true Levantar todos los servicios : bash docker-compose up -d Esto arrancar\u00e1: * PostgreSQL (para Superset y tablas de validaci\u00f3n). * Superset (opcional, UI de dashboards). * Zookeeper + Kafka (opcional). * Hadoop NameNode + DataNode. * Spark Master + Spark Workers. * Contenedor validation-engine (motor de validaciones). Verificar estado : bash docker-compose ps * Esperar a que contenedores se vuelvan healthy . * Si HDFS arranca en safe mode, validation-engine ejecutar\u00e1 autom\u00e1ticamente hdfs dfsadmin -safemode leave . 5.2.2 Inicializar el backend (FastAPI) \u00b6 Entrar a la carpeta backend/ : bash cd ../../backend source .venv/bin/activate 2. Ejecutar migraciones o crear tablas manualmente (no incluidas en el proyecto, usar tu propia estrategia). 3. Levantar el servidor local: bash uvicorn main:app --reload --host 0.0.0.0 --port 8000 * La API estar\u00e1 disponible en http://localhost:8000 . * Endpoints principales: * `/auth/register` * `/auth/login` * `/files/upload` * `/files/push/{file_name}` * `/files/download/{file_name}` * `/files/` (GET, POST, PATCH, DELETE) * `/files/logs` 5.2.3 Inicializar el frontend (React) \u00b6 En otra terminal, entrar a frontend/ : bash cd ../frontend 2. Instalar dependencias (si no se hizo antes): bash npm install 3. Levantar el servidor de desarrollo: bash npm run dev 4. Abrir el navegador en http://localhost:5173 . 5.2.4 Flujo de usuario \u00b6 Registro/Login * Acceder a http://localhost:5173/register para crear cuenta. * Luego http://localhost:5173/login para iniciar sesi\u00f3n. Dashboard * Subir CSV: seleccionar un archivo y pulsar Subir Fichero . * Aparecer\u00e1 alerta con el ID de configuraci\u00f3n. * La tabla mostrar\u00e1 todas las configuraciones; cada fila ofrece botones: * **Validar**: env\u00eda el CSV a HDFS y lanza validaci\u00f3n en Spark. * **Detalles**: abre modal para editar metadatos, validar, descargar o eliminar. * **Eliminar**: borra configuraci\u00f3n de la base de datos. Visualizar logs * Acceder a http://localhost:5173/logs para ver resultados de validaci\u00f3n. * La tabla mostrar\u00e1 los \u00faltimos logs, con fecha formateada a DD/MM/YYYY . Superset (opcional) * Superset corre en http://localhost:8088 . * Iniciar sesi\u00f3n con credenciales creadas en docker-compose.yml (admin / 1234). * Configurar una conexi\u00f3n a PostgreSQL ( superset-db:5432 ) para leer tablas trigger_control y crear dashboards de calidad de datos. 5.3 Configuraci\u00f3n adicional \u00b6 Variables de entorno (backend) * En backend/.env : ``` # PostgreSQL POSTGRES_USER=superset POSTGRES_PASSWORD=superset POSTGRES_HOST=postgres POSTGRES_PORT=5432 POSTGRES_DB=superset # HDFS HDFS_HOST=hadoop-namenode HDFS_PORT=9870 HDFS_DIR=/data/bank_accounts HDFS_USER=hdfs HDFS_DATANODE_HOST=hadoop-datanode HDFS_DATANODE_PORT=9864 # Carpeta local de uploads UPLOAD_DIR=uploaded_files # JWT JWT_SECRET_KEY=TuSecretoUltraSeguro123! JWT_ALGORITHM=HS256 ACCESS_TOKEN_EXPIRE_MINUTES=60 ``` Permisos en HDFS * El contenedor validation-engine se encarga de ajustar permisos con hdfs dfs -chmod -R 777 /data/bank_accounts . * Si se prefiere, desde cualquier contenedor con cliente Hadoop: ```bash hdfs dfs -chmod -R 777 /data/bank_accounts ``` Descargas y backups * Para exportar la base de datos PostgreSQL: ```bash docker exec -t superset-db pg_dumpall -c -U superset > dump_$(date +%F).sql ``` * Para restaurar: ```bash docker exec -i superset-db psql -U superset < dump_YYYY-MM-DD.sql ``` Personalizar Spark * Variables de entorno para Spark Master/Workers (en docker-compose.yml ): * `SPARK_WORKER_MEMORY`, `SPARK_WORKER_CORES`, `SPARK_LOCAL_DIRS`. * Ajustar spark.executor.memory y spark.driver.memory en spark-submit si se procesan CSV muy grandes.","title":"Resultado (Manual de usuario)"},{"location":"05-resultado.html#5-resultado-manual-de-usuario","text":"","title":"5. Resultado (Manual de usuario)"},{"location":"05-resultado.html#51-requisitos-previos","text":"Requisitos comunes: Docker Desktop (Linux, macOS o Windows) con 160 GB libres en disco. Docker Compose (versi\u00f3n \u2265 1.27). Node.js v16+ y npm (para frontend). Python 3.12 , pip (para backend). Clonar el repositorio: bash git clone https://github.com/JuanraDAM/FileMonitoringSystem.git cd FileMonitoringSystem","title":"5.1 Requisitos previos"},{"location":"05-resultado.html#511-motor-de-validaciones","text":"Espacio en disco para contenedores Hadoop/Spark y datos HDFS. Puertos: * 9000 (NameNode) * 9870 (NameNode UI) * 9864 (DataNode) * 7077 (Spark Master) * 8080 (Spark Master UI) * 8081, 8082 (Spark Workers UI) * SBT instalado localmente si se desea compilar sin Docker.","title":"5.1.1 Motor de validaciones"},{"location":"05-resultado.html#512-backend-fastapi","text":"Crear entorno virtual Python 3.12: bash python3.12 -m venv backend/.venv source backend/.venv/bin/activate pip install --upgrade pip pip install -r backend/requirements.txt * Archivo .env en backend/ con credenciales y configuraci\u00f3n de HDFS.","title":"5.1.2 Backend (FastAPI)"},{"location":"05-resultado.html#513-frontend-react","text":"Desde frontend/ : bash cd frontend npm install * No hay variables de entorno adicionales, asume que el backend corre en http://localhost:8000 .","title":"5.1.3 Frontend (React)"},{"location":"05-resultado.html#52-pasos-de-uso","text":"A continuaci\u00f3n se describen los pasos para levantar cada componente y usar el sistema completo.","title":"5.2 Pasos de uso"},{"location":"05-resultado.html#521-levantar-infraestructura-con-docker-compose","text":"Ir a la carpeta del motor : bash cd engine/docker Crear la red Docker (si no existe): bash docker network create superset-net || true Levantar todos los servicios : bash docker-compose up -d Esto arrancar\u00e1: * PostgreSQL (para Superset y tablas de validaci\u00f3n). * Superset (opcional, UI de dashboards). * Zookeeper + Kafka (opcional). * Hadoop NameNode + DataNode. * Spark Master + Spark Workers. * Contenedor validation-engine (motor de validaciones). Verificar estado : bash docker-compose ps * Esperar a que contenedores se vuelvan healthy . * Si HDFS arranca en safe mode, validation-engine ejecutar\u00e1 autom\u00e1ticamente hdfs dfsadmin -safemode leave .","title":"5.2.1 Levantar infraestructura con Docker Compose"},{"location":"05-resultado.html#522-inicializar-el-backend-fastapi","text":"Entrar a la carpeta backend/ : bash cd ../../backend source .venv/bin/activate 2. Ejecutar migraciones o crear tablas manualmente (no incluidas en el proyecto, usar tu propia estrategia). 3. Levantar el servidor local: bash uvicorn main:app --reload --host 0.0.0.0 --port 8000 * La API estar\u00e1 disponible en http://localhost:8000 . * Endpoints principales: * `/auth/register` * `/auth/login` * `/files/upload` * `/files/push/{file_name}` * `/files/download/{file_name}` * `/files/` (GET, POST, PATCH, DELETE) * `/files/logs`","title":"5.2.2 Inicializar el backend (FastAPI)"},{"location":"05-resultado.html#523-inicializar-el-frontend-react","text":"En otra terminal, entrar a frontend/ : bash cd ../frontend 2. Instalar dependencias (si no se hizo antes): bash npm install 3. Levantar el servidor de desarrollo: bash npm run dev 4. Abrir el navegador en http://localhost:5173 .","title":"5.2.3 Inicializar el frontend (React)"},{"location":"05-resultado.html#524-flujo-de-usuario","text":"Registro/Login * Acceder a http://localhost:5173/register para crear cuenta. * Luego http://localhost:5173/login para iniciar sesi\u00f3n. Dashboard * Subir CSV: seleccionar un archivo y pulsar Subir Fichero . * Aparecer\u00e1 alerta con el ID de configuraci\u00f3n. * La tabla mostrar\u00e1 todas las configuraciones; cada fila ofrece botones: * **Validar**: env\u00eda el CSV a HDFS y lanza validaci\u00f3n en Spark. * **Detalles**: abre modal para editar metadatos, validar, descargar o eliminar. * **Eliminar**: borra configuraci\u00f3n de la base de datos. Visualizar logs * Acceder a http://localhost:5173/logs para ver resultados de validaci\u00f3n. * La tabla mostrar\u00e1 los \u00faltimos logs, con fecha formateada a DD/MM/YYYY . Superset (opcional) * Superset corre en http://localhost:8088 . * Iniciar sesi\u00f3n con credenciales creadas en docker-compose.yml (admin / 1234). * Configurar una conexi\u00f3n a PostgreSQL ( superset-db:5432 ) para leer tablas trigger_control y crear dashboards de calidad de datos.","title":"5.2.4 Flujo de usuario"},{"location":"05-resultado.html#53-configuracion-adicional","text":"Variables de entorno (backend) * En backend/.env : ``` # PostgreSQL POSTGRES_USER=superset POSTGRES_PASSWORD=superset POSTGRES_HOST=postgres POSTGRES_PORT=5432 POSTGRES_DB=superset # HDFS HDFS_HOST=hadoop-namenode HDFS_PORT=9870 HDFS_DIR=/data/bank_accounts HDFS_USER=hdfs HDFS_DATANODE_HOST=hadoop-datanode HDFS_DATANODE_PORT=9864 # Carpeta local de uploads UPLOAD_DIR=uploaded_files # JWT JWT_SECRET_KEY=TuSecretoUltraSeguro123! JWT_ALGORITHM=HS256 ACCESS_TOKEN_EXPIRE_MINUTES=60 ``` Permisos en HDFS * El contenedor validation-engine se encarga de ajustar permisos con hdfs dfs -chmod -R 777 /data/bank_accounts . * Si se prefiere, desde cualquier contenedor con cliente Hadoop: ```bash hdfs dfs -chmod -R 777 /data/bank_accounts ``` Descargas y backups * Para exportar la base de datos PostgreSQL: ```bash docker exec -t superset-db pg_dumpall -c -U superset > dump_$(date +%F).sql ``` * Para restaurar: ```bash docker exec -i superset-db psql -U superset < dump_YYYY-MM-DD.sql ``` Personalizar Spark * Variables de entorno para Spark Master/Workers (en docker-compose.yml ): * `SPARK_WORKER_MEMORY`, `SPARK_WORKER_CORES`, `SPARK_LOCAL_DIRS`. * Ajustar spark.executor.memory y spark.driver.memory en spark-submit si se procesan CSV muy grandes.","title":"5.3 Configuraci\u00f3n adicional"},{"location":"06-conclusiones.html","text":"6. Conclusiones \u00b6 6.1 Dificultades \u00b6 ClusterID incompatibles (HDFS) * Al cambiar o recrear im\u00e1genes de Hadoop, DataNode arroja error de \u201cIncompatible clusterIDs\u201d. * Soluci\u00f3n: borrar vol\u00famenes hadoop-namenode y hdfs-data-datanode antes de levantar. Safe Mode en HDFS * HDFS arranca en \u201csafe mode\u201d si detecta inconsistencias o falta de bloques. * Se incorpor\u00f3 hdfs dfsadmin -safemode leave en el contenedor validation-engine para forzar la salida. Manejo de redirecci\u00f3n en WebHDFS * WebHDFS devuelve un redirect 307 con URL del DataNode; ajustar host y puerto es cr\u00edtico. * Se debi\u00f3 detectar y procesar manualmente el header Location antes de subir el contenido. Tama\u00f1o de ficheros grandes * Al probar con CSV de varios GB, los DataNode se quedaban sin espacio y se exclu\u00edan. * Se recomend\u00f3 montar /tmp de Spark Workers en vol\u00famenes dedicados ( docker_validation_tmp ) o usar tmpfs . Coordinaci\u00f3n entre frontend y backend (token y rutas) * La gesti\u00f3n de 401 Unauthorized requiri\u00f3 interceptores en Axios para borrar token y redirigir a login. * Asegurarse de que todas las peticiones protegidas incluyeran el header Authorization . Configuraci\u00f3n de permisos HDFS desde FastAPI * Ejecutar comandos Hadoop desde contenedor Python (shell) generaba a veces problemas de path. * Se opt\u00f3 por usar HTTP WebHDFS en lugar de comandos nativos en Python. SQLAlchemy Async y migraciones * Cambios frecuentes en el modelo de datos (por ejemplo, TIMESTAMP a TIMESTAMPTZ ) implicaron migraciones manuales. * No se incluy\u00f3 un sistema de migraciones (Alembic), por lo que hubo que recrear tablas en producci\u00f3n. Paginaci\u00f3n y filtros en logs * Al crecer el n\u00famero de logs, la consulta simple ( SELECT * FROM trigger_control ) se volv\u00eda lenta. * Se implementaron filtros por fecha y entorno, pero falta paginaci\u00f3n y l\u00edmites por defecto. Interfaz de usuario b\u00e1sica * Al usar CSS puro, cost\u00f3 dise\u00f1ar un estilo consistente y responsivo. * Se recomend\u00f3 integrar un framework CSS (Tailwind, Material UI) para mejorar UX/UI. 6.2 Mejoras \u00b6 Migrar a Spark Structured Streaming * Pasar de polling batch a procesamiento near\u2010real\u2010time con Structured Streaming y checkpoints. Orquestaci\u00f3n con Kubernetes * Desplegar Hadoop, Spark y el motor de validaciones en un cl\u00faster Kubernetes usando Helm charts y StatefulSets. * Facilitar elasticidad y escalado autom\u00e1tico de nodos. Monitorizaci\u00f3n con Prometheus & Grafana * Exponer m\u00e9tricas de Spark, HDFS y PostgreSQL. * Crear dashboards de latencia, tasas de error y uso de recursos. Alertas autom\u00e1ticas * Enviar notificaciones por correo o webhook cuando ocurran validaciones negativas (flags cr\u00edticos). Paginaci\u00f3n y filtros avanzados en frontend * A\u00f1adir paginaci\u00f3n en listas de configuraciones y logs. * Filtros por fecha, entorno, flag de validaci\u00f3n, nombre de fichero. Soporte para formatos adicionales * A\u00f1adir validaci\u00f3n y lectura de CSV comprimidos, Parquet, Avro u ORC. * Implementar validaciones basadas en schemas Avro o JSON Schema. Carga fragmentada (chunked upload) * Para CSV muy grandes, usar multipart upload o chunking en frontend/backend para evitar timeouts. Integraci\u00f3n de notificaciones en tiempo real * Usar WebSockets o WebPubSub para notificar al frontend sobre el estado de la validaci\u00f3n en tiempo real. Internacionalizaci\u00f3n (i18n) * Permitir cambiar idioma en frontend (ES, EN). * Formateo de fechas y mensajes localizados. Documentaci\u00f3n de API con Swagger/Redoc * FastAPI ya genera documentaci\u00f3n autom\u00e1tica; extenderla con ejemplos de petici\u00f3n/respuesta y c\u00f3digo de errores. Anexo: Glosario de Flags \u00b6 Rango Significado 30 Error de lectura (I/O, CSV mal formado) 32 Delimiter mismatch (n\u00famero de columnas esperado) 33 Header mismatch (encabezados distintos) 34 Column count per row mismatch 35 Tipo inv\u00e1lido 36 Nulo indebido 37 Longitud excedida 38 Formato texto inv\u00e1lido 39 Duplicado PK (integridad referencial) 40 Formato inv\u00e1lido ( account_number ) 41 Fuera de rango ( credit_score ) 42 Fuera de rango ( risk_score ) 43 Menor de edad ( date_of_birth ) 44 Negativo en \u201cActive\u201d ( balance ) 45 Balance \u2260 0 en \u201cClosed\u201d 46 Interest \u2260 0 en \u201cChecking\u201d 47 Overdraft inv\u00e1lido 48 Pocas transacciones en joint 49 Avg tx \u2260 0 con 0 tx 1.13, 1.21, 1.31, 1.41 Flags de validaci\u00f3n \u201ctodo OK\u201d (sin errores) 2 OK final (todos los validadores pasaron) 99 Sin configuraci\u00f3n en file_configuration","title":"Conclusiones"},{"location":"06-conclusiones.html#6-conclusiones","text":"","title":"6. Conclusiones"},{"location":"06-conclusiones.html#61-dificultades","text":"ClusterID incompatibles (HDFS) * Al cambiar o recrear im\u00e1genes de Hadoop, DataNode arroja error de \u201cIncompatible clusterIDs\u201d. * Soluci\u00f3n: borrar vol\u00famenes hadoop-namenode y hdfs-data-datanode antes de levantar. Safe Mode en HDFS * HDFS arranca en \u201csafe mode\u201d si detecta inconsistencias o falta de bloques. * Se incorpor\u00f3 hdfs dfsadmin -safemode leave en el contenedor validation-engine para forzar la salida. Manejo de redirecci\u00f3n en WebHDFS * WebHDFS devuelve un redirect 307 con URL del DataNode; ajustar host y puerto es cr\u00edtico. * Se debi\u00f3 detectar y procesar manualmente el header Location antes de subir el contenido. Tama\u00f1o de ficheros grandes * Al probar con CSV de varios GB, los DataNode se quedaban sin espacio y se exclu\u00edan. * Se recomend\u00f3 montar /tmp de Spark Workers en vol\u00famenes dedicados ( docker_validation_tmp ) o usar tmpfs . Coordinaci\u00f3n entre frontend y backend (token y rutas) * La gesti\u00f3n de 401 Unauthorized requiri\u00f3 interceptores en Axios para borrar token y redirigir a login. * Asegurarse de que todas las peticiones protegidas incluyeran el header Authorization . Configuraci\u00f3n de permisos HDFS desde FastAPI * Ejecutar comandos Hadoop desde contenedor Python (shell) generaba a veces problemas de path. * Se opt\u00f3 por usar HTTP WebHDFS en lugar de comandos nativos en Python. SQLAlchemy Async y migraciones * Cambios frecuentes en el modelo de datos (por ejemplo, TIMESTAMP a TIMESTAMPTZ ) implicaron migraciones manuales. * No se incluy\u00f3 un sistema de migraciones (Alembic), por lo que hubo que recrear tablas en producci\u00f3n. Paginaci\u00f3n y filtros en logs * Al crecer el n\u00famero de logs, la consulta simple ( SELECT * FROM trigger_control ) se volv\u00eda lenta. * Se implementaron filtros por fecha y entorno, pero falta paginaci\u00f3n y l\u00edmites por defecto. Interfaz de usuario b\u00e1sica * Al usar CSS puro, cost\u00f3 dise\u00f1ar un estilo consistente y responsivo. * Se recomend\u00f3 integrar un framework CSS (Tailwind, Material UI) para mejorar UX/UI.","title":"6.1 Dificultades"},{"location":"06-conclusiones.html#62-mejoras","text":"Migrar a Spark Structured Streaming * Pasar de polling batch a procesamiento near\u2010real\u2010time con Structured Streaming y checkpoints. Orquestaci\u00f3n con Kubernetes * Desplegar Hadoop, Spark y el motor de validaciones en un cl\u00faster Kubernetes usando Helm charts y StatefulSets. * Facilitar elasticidad y escalado autom\u00e1tico de nodos. Monitorizaci\u00f3n con Prometheus & Grafana * Exponer m\u00e9tricas de Spark, HDFS y PostgreSQL. * Crear dashboards de latencia, tasas de error y uso de recursos. Alertas autom\u00e1ticas * Enviar notificaciones por correo o webhook cuando ocurran validaciones negativas (flags cr\u00edticos). Paginaci\u00f3n y filtros avanzados en frontend * A\u00f1adir paginaci\u00f3n en listas de configuraciones y logs. * Filtros por fecha, entorno, flag de validaci\u00f3n, nombre de fichero. Soporte para formatos adicionales * A\u00f1adir validaci\u00f3n y lectura de CSV comprimidos, Parquet, Avro u ORC. * Implementar validaciones basadas en schemas Avro o JSON Schema. Carga fragmentada (chunked upload) * Para CSV muy grandes, usar multipart upload o chunking en frontend/backend para evitar timeouts. Integraci\u00f3n de notificaciones en tiempo real * Usar WebSockets o WebPubSub para notificar al frontend sobre el estado de la validaci\u00f3n en tiempo real. Internacionalizaci\u00f3n (i18n) * Permitir cambiar idioma en frontend (ES, EN). * Formateo de fechas y mensajes localizados. Documentaci\u00f3n de API con Swagger/Redoc * FastAPI ya genera documentaci\u00f3n autom\u00e1tica; extenderla con ejemplos de petici\u00f3n/respuesta y c\u00f3digo de errores.","title":"6.2 Mejoras"},{"location":"06-conclusiones.html#anexo-glosario-de-flags","text":"Rango Significado 30 Error de lectura (I/O, CSV mal formado) 32 Delimiter mismatch (n\u00famero de columnas esperado) 33 Header mismatch (encabezados distintos) 34 Column count per row mismatch 35 Tipo inv\u00e1lido 36 Nulo indebido 37 Longitud excedida 38 Formato texto inv\u00e1lido 39 Duplicado PK (integridad referencial) 40 Formato inv\u00e1lido ( account_number ) 41 Fuera de rango ( credit_score ) 42 Fuera de rango ( risk_score ) 43 Menor de edad ( date_of_birth ) 44 Negativo en \u201cActive\u201d ( balance ) 45 Balance \u2260 0 en \u201cClosed\u201d 46 Interest \u2260 0 en \u201cChecking\u201d 47 Overdraft inv\u00e1lido 48 Pocas transacciones en joint 49 Avg tx \u2260 0 con 0 tx 1.13, 1.21, 1.31, 1.41 Flags de validaci\u00f3n \u201ctodo OK\u201d (sin errores) 2 OK final (todos los validadores pasaron) 99 Sin configuraci\u00f3n en file_configuration","title":"Anexo: Glosario de Flags"}]}