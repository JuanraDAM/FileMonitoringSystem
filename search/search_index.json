{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"01-introduccion.html","title":"Introducci\u00f3n","text":""},{"location":"01-introduccion.html#1-introduccion","title":"1. Introducci\u00f3n","text":""},{"location":"01-introduccion.html#11-resumen-del-proyecto","title":"1.1 Resumen del proyecto","text":"<p>El File Monitoring System es una soluci\u00f3n integral para la ingesta, validaci\u00f3n y visualizaci\u00f3n de ficheros bancarios en un entorno Big Data. Consta de tres componentes principales:</p> <ol> <li>Un motor de validaciones implementado en Scala y ejecutado sobre Apache Spark, que procesa ficheros almacenados en HDFS, aplica validaciones estructurales, tipol\u00f3gicas, referenciales y de negocio, y almacena los resultados en PostgreSQL.</li> <li>Un backend desarrollado con FastAPI (Python 3.12) que expone una API REST para gestionar usuarios, configuraciones de ficheros, subir/descargar archivos y sincronizarlos con HDFS a trav\u00e9s de WebHDFS.</li> <li>Un frontend construido con React 18+ y Vite, que permite al usuario autenticarse, subir CSV, editar metadatos de parseo, enviar ficheros a HDFS para validaci\u00f3n y visualizar los logs resultantes.</li> </ol> <p>La arquitectura est\u00e1 contenida en varios contenedores Docker orquestados mediante Docker Compose, incluyendo cl\u00faster Hadoop (NameNode/DataNode), Spark (Master/Workers), PostgreSQL, Kafka, Superset (opcional) y el propio motor de validaciones. El objetivo es ofrecer una soluci\u00f3n escalable, modular y mantenible para garantizar la calidad e integridad de los datos bancarios.</p>"},{"location":"01-introduccion.html#12-explicacion-de-la-aplicacion","title":"1.2 Explicaci\u00f3n de la aplicaci\u00f3n","text":"<p>El flujo de trabajo global se describe a continuaci\u00f3n:</p> <ol> <li> <p>Usuario (frontend)</p> <ul> <li>Se registra o inicia sesi\u00f3n mediante JWT (FastAPI).</li> <li>Accede a un dashboard donde puede subir un fichero CSV y gestionar metadatos de parseo (delimiter, quote_char, date_format, etc.).</li> <li>Al pulsar \u201cValidar\u201d, el frontend env\u00eda el CSV al backend y \u00e9ste lo almacena localmente y registra la configuraci\u00f3n en la tabla <code>file_configuration</code> de PostgreSQL.</li> </ul> </li> <li> <p>Backend (FastAPI)</p> <ul> <li>Recibe el CSV (<code>POST /files/upload</code>) y guarda el archivo en la carpeta local <code>uploaded_files/</code>.</li> <li>Crea o actualiza un registro en la tabla <code>file_configuration</code> con par\u00e1metros de parseo.</li> <li> <p>Cuando el usuario solicita \u201cEnviar a HDFS\u201d (<code>POST /files/push/{file_name}</code>), el backend:</p> <ol> <li>Verifica que el archivo existe en <code>uploaded_files/</code>.</li> <li>Llama a WebHDFS (NameNode) para crear el directorio <code>/data/bank_accounts</code> y ajustar permisos (<code>op=MKDIRS</code>, <code>op=SETPERM</code>).</li> <li>Solicita la creaci\u00f3n del fichero en HDFS (<code>op=CREATE</code>), recuerda el redireccionamiento 307 a DataNode, ajusta la URL y sube el contenido.</li> <li>Responde al frontend con un mensaje de \u00e9xito.</li> </ol> </li> </ul> </li> <li> <p>Motor de validaciones (Scala + Spark)</p> <ul> <li> <p>Se ejecuta en un contenedor Docker con Spark y SBT. En el <code>ENTRYPOINT</code>, al arrancar:</p> <ol> <li>Ejecuta <code>hdfs dfsadmin -safemode leave</code> para salir de safe mode.</li> <li>Crea la carpeta <code>/data/bank_accounts</code> en HDFS (si no existe).</li> <li>Copia los CSV desde un volumen local (<code>/local_bank_accounts</code>) a HDFS.</li> <li> <p>Inicia <code>spark-submit</code> que corre la clase <code>Main.scala</code>.</p> <ul> <li>Main.scala:</li> </ul> </li> <li> <p>Configura un bucle de polling que observa el directorio HDFS (<code>/data/bank_accounts</code>).</p> </li> <li>Al detectar un fichero, llama a <code>ExecutionManager.executeFile(path, outputTable)</code>.</li> <li> <p>Dentro de <code>ExecutionManager</code>:</p> <ul> <li>Lee el CSV con <code>Reader.readFile(...)</code> en un DataFrame particionado.</li> <li> <p>Aplica validadores en cascada:</p> <ol> <li>FileSentinel: validaci\u00f3n estructural (delimitador, headers, n\u00famero de columnas).</li> <li>TypeValidator: validaci\u00f3n tipol\u00f3gica (tipos, rangos, formato de fecha, nullability).</li> <li>ReferentialIntegrityValidator: integridad referencial (unicidad de claves primarias seg\u00fan metadatos).</li> <li>FunctionalValidator: reglas de negocio (formato cuenta, rangos de credit_score, balance seg\u00fan estado, etc.).             * Si falla alguna validaci\u00f3n, registra el flag correspondiente en la tabla <code>trigger_control</code> en PostgreSQL y detiene el procesamiento.             * Si pasa todo, registra flag \u201c2\u201d (OK).             * Borra el fichero de HDFS para evitar reprocesamientos.</li> </ol> </li> </ul> </li> </ol> </li> </ul> </li> <li> <p>Base de datos (PostgreSQL)</p> <ul> <li>Contiene tablas:<ul> <li><code>users</code>: para gestionar cuentas de usuario (backend y frontend).</li> <li><code>file_configuration</code>: configura par\u00e1metros de parseo de cada CSV.</li> <li><code>semantic_layer</code>: metadatos de cada campo (tipo de dato, longitud, nullable, PK, formato) utilizados en las validaciones tipol\u00f3gicas y referenciales.</li> <li><code>trigger_control</code>: almacena logs de validaci\u00f3n (timestamp, file_config_id, field_name, environment, validation_flag, error_message).</li> <li><code>negative_flag_logs</code> (opcional): detalles adicionales de validaciones negativas.</li> </ul> </li> </ul> </li> <li> <p>Visualizaci\u00f3n de resultados</p> <ul> <li>El usuario, desde el frontend, accede a <code>/logs</code> y obtiene los registros de validaci\u00f3n (<code>GET /files/logs</code>).</li> <li>El backend formatea la fecha (<code>logged_at</code>) al formato <code>DD/MM/YYYY, hh:mm:ss</code> en zona Madrid.</li> <li>El frontend muestra una tabla con campos: ID, file_config_id, file_name, field_name, environment, validation_flag, error_message, fecha.</li> </ul> </li> </ol>"},{"location":"01-introduccion.html#13-resumen-de-tecnologias-utilizadas","title":"1.3 Resumen de tecnolog\u00edas utilizadas","text":"<ul> <li>Scala 2.12</li> <li>Apache Spark 3.x (Spark Streaming para procesamiento batch/near real\u2010time)</li> <li>HDFS (Hadoop 3.x)</li> <li>SBT (gestor de proyectos Scala)</li> <li>Kryo (serializador personalizado en Spark)</li> <li> <p>Docker &amp; Docker Compose</p> <ul> <li>Contenedores: Spark Master/Workers (bitnami/spark), Hadoop NameNode/DataNode (bde2020), PostgreSQL (postgres\\:latest), Kafka/Zookeeper (wurstmeister), Superset (dockerfile custom).</li> <li>PostgreSQL 13 (logs y metadatos)</li> <li> <p>FastAPI (Python 3.12)</p> </li> <li> <p>Uvicorn (servidor ASGI)</p> </li> <li>SQLAlchemy Async + AsyncPG</li> <li>Pydantic Settings</li> <li>python-jose, passlib[bcrypt] (JWT y hashing de contrase\u00f1as)</li> <li>requests (llamadas a WebHDFS)</li> <li> <p>React 18+ (frontend) con Vite</p> </li> <li> <p>React Router v6</p> </li> <li>Axios (con interceptores de token y 401)</li> <li>Context API (AuthContext)</li> <li>CSS puro</li> <li>Superset (opcional, para dashboards de logs)</li> <li>Git / GitHub (control de versiones)</li> <li>PlantUML (diagramas UML en bruto para generar ER, clases y secuencias)</li> </ul> </li> </ul>"},{"location":"02-requisitos.html","title":"Especificaci\u00f3n de Requisitos","text":""},{"location":"02-requisitos.html#2-especificacion-de-requisitos","title":"2. Especificaci\u00f3n de Requisitos","text":""},{"location":"02-requisitos.html#21-requisitos-funcionales","title":"2.1 Requisitos funcionales","text":"<p>A continuaci\u00f3n se listan los requisitos funcionales unificados, tomando en cuenta los tres componentes (frontend, backend, motor de validaciones):</p> <ol> <li> <p>Autenticaci\u00f3n de usuarios</p> <ul> <li>RF1.1: Registro de usuario con email y contrase\u00f1a (<code>POST /auth/register</code>).</li> <li>RF1.2: Login de usuario (<code>POST /auth/login</code>), devuelve JWT y redirige al dashboard.</li> <li>RF1.3: Logout que elimina el token de <code>localStorage</code> y redirige a <code>/login</code>.</li> <li>RF1.4: Cualquier petici\u00f3n protegida que retorne 401 Unauthorized debe borrar el token y redirigir a <code>/login</code>.</li> </ul> </li> <li> <p>Gesti\u00f3n de configuraciones de fichero (<code>file_configuration</code>)</p> <ul> <li>RF2.1: Subir CSV al backend (<code>POST /files/upload</code>) con <code>multipart/form-data</code>.</li> <li>RF2.2: Al subir, insertar o actualizar par\u00e1metros en <code>file_configuration</code> (has_header, delimiter, quote_char, escape_char, date_format, timestamp_format, partition_columns).</li> <li>RF2.3: Listar configuraciones (<code>GET /files/</code>), mostrar ID, file_name, path, has_header, delimiter, quote_char, escape_char, date_format, timestamp_format, partition_columns.</li> <li>RF2.4: Obtener detalles de una configuraci\u00f3n (<code>GET /files/{id}</code>), editar par\u00e1metros (<code>PATCH /files/{id}</code>), eliminar configuraci\u00f3n (<code>DELETE /files/{id}</code>).</li> <li>RF2.5: Descargar CSV original desde backend (<code>GET /files/download/{file_name}</code>).</li> </ul> </li> <li> <p>Sincronizaci\u00f3n con HDFS</p> <ul> <li>RF3.1: Enviar fichero desde backend a HDFS (<code>POST /files/push/{file_name}</code>), creando directorio en HDFS (<code>op=MKDIRS</code>), ajustando permisos (<code>op=SETPERM</code>) y subiendo contenido (<code>op=CREATE</code>).</li> <li>RF3.2: Backend debe manejar redireccionamiento 307 de NameNode a DataNode, adaptando la URL de destino.</li> <li>RF3.3: Validar existencia local del fichero antes de empujar; si no existe, responder 404.</li> </ul> </li> <li> <p>Proceso de validaci\u00f3n (motor Scala/Spark)</p> <ul> <li>RF4.1: Monitorizar HDFS en <code>/data/bank_accounts</code>; detectar nuevos ficheros en polling batch.</li> <li>RF4.2: Por cada fichero detectado, procesarlo de forma independiente.</li> <li> <p>RF4.3: Validaci\u00f3n estructural:</p> <ul> <li>Verificar delimitador, encabezados y n\u00famero de columnas seg\u00fan <code>file_configuration</code>.</li> <li> <p>Flags:</p> <ul> <li>32: Delimiter mismatch</li> <li>33: Header mismatch</li> <li>34: Column count per row mismatch<ul> <li>RF4.4: Validaci\u00f3n tipol\u00f3gica:</li> </ul> </li> </ul> </li> <li> <p>Comprobar tipos, rangos (fechas, n\u00fameros), formatos de texto seg\u00fan <code>semantic_layer</code>.</p> </li> <li> <p>Flags:</p> <ul> <li>35: Tipo inv\u00e1lido</li> <li>36: Nulo indebido</li> <li>37: Longitud excedida</li> <li>38: Formato texto inv\u00e1lido<ul> <li>RF4.5: Integridad referencial:</li> </ul> </li> </ul> </li> <li> <p>Verificar unicidad de claves primarias seg\u00fan metadatos.</p> </li> <li> <p>Flag:</p> <ul> <li>39: Duplicado PK<ul> <li>RF4.6: Validaci\u00f3n de negocio:</li> </ul> </li> </ul> </li> <li> <p>Reglas espec\u00edficas del dominio bancario:</p> <ul> <li>Formato de cuenta (<code>^[A-Za-z0-9]{10}$</code> \u2192 40).</li> <li><code>credit_score</code> entre 300 y 850 \u2192 41.</li> <li><code>risk_score</code> entre 0 y 100 \u2192 42.</li> <li>Mayor de 18 a\u00f1os (DOB) \u2192 43.</li> <li>Si <code>status</code>=\u201cActive\u201d, <code>balance</code>&gt;=0; si \u201cClosed\u201d, <code>balance</code>=0 \u2192 44/45.</li> <li>Si <code>account_type</code>=\u201cChecking\u201d, <code>interest_rate</code>=0 \u2192 46.</li> <li><code>overdraft_limit</code>&gt;=0 y v\u00e1lido \u2192 47.</li> <li>Si <code>is_joint_account</code>=\u201cYes\u201d, <code>num_transactions</code>&gt;=2 \u2192 48.</li> <li>Si <code>num_transactions</code>=0, <code>avg_transaction_amount</code>=0 \u2192 49.<ul> <li>RF4.7: Registrar resultados en <code>trigger_control</code> (timestamp, file_config_id, file_name, field_name, environment, validation_flag, error_message).</li> <li>RF4.8: Tras procesar (OK o KO), borrar el fichero de HDFS para evitar reprocesamiento.</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> <li> <p>Visualizaci\u00f3n de logs de validaci\u00f3n</p> <ul> <li>RF5.1: Listar logs (<code>GET /files/logs</code>) filtrables por <code>environment</code>, <code>from_date</code> y <code>to_date</code>.</li> <li>RF5.2: Formatear <code>logged_at</code> a <code>DD/MM/YYYY, hh:mm:ss</code> en zona Madrid.</li> <li>RF5.3: Mostrar tabla con columnas: ID, file_config_id, file_name, field_name, environment, validation_flag, error_message, fecha.</li> </ul> </li> <li> <p>UX/UI del frontend</p> <ul> <li>RF6.1: Formularios de registro/login con validaci\u00f3n de campos obligatorios.</li> <li>RF6.2: Indicadores de carga en botones (\u201cSubiendo\u2026\u201d, \u201cEnviando\u2026\u201d, \u201cGuardando\u2026\u201d, \u201cEliminando\u2026\u201d).</li> <li>RF6.3: Modal de detalles que se cierra al hacer clic fuera o en bot\u00f3n \u201c\u00d7\u201d.</li> <li>RF6.4: Alertas (<code>alert()</code>) para notificar \u00e9xito o error.</li> <li>RF6.5: Navbar con enlaces en negrita (Dashboard, Logs) y bot\u00f3n Logout en color naranja.</li> </ul> </li> </ol>"},{"location":"02-requisitos.html#22-requisitos-no-funcionales","title":"2.2 Requisitos no funcionales","text":"<ol> <li> <p>Seguridad</p> <ul> <li>RNF1.1: JWT con expiraci\u00f3n configurable; token almacenado en <code>localStorage</code>.</li> <li>RNF1.2: Backend protege rutas con dependencia <code>get_current_user</code>; reacciona a 401 borrando token en frontend.</li> <li>RNF1.3: HDFS: al crear directorios, aplicar <code>chmod -R 777</code> para permitir lectura/escritura a todos los usuarios.</li> <li>RNF1.4: Contrase\u00f1as hasheadas con bcrypt.</li> </ul> </li> <li> <p>Rendimiento</p> <ul> <li>RNF2.1: El motor de validaciones debe procesar &gt;1M filas por partici\u00f3n en &lt; 1 minuto.</li> <li>RNF2.2: Uso \u00f3ptimo de particiones en Spark y <code>fetchSize</code> en conexiones JDBC.</li> <li>RNF2.3: El frontend debe cargar en &lt;2 s en producci\u00f3n b\u00e1sica.</li> </ul> </li> <li> <p>Escalabilidad</p> <ul> <li>RNF3.1: Motor de validaciones dise\u00f1ado para aumentar nodos Spark sin cambios en el c\u00f3digo.</li> <li>RNF3.2: Backend as\u00edncrono (FastAPI + SQLAlchemy Async) para operaciones I/O intensivo.</li> <li>RNF3.3: Posibilidad de migrar a Spark Structured Streaming en el futuro.</li> </ul> </li> <li> <p>Disponibilidad</p> <ul> <li>RNF4.1: El motor debe mantenerse activo 24/7; al iniciar, ejecutar <code>hdfs dfsadmin -safemode leave</code>.</li> <li>RNF4.2: Contenedores orquestados con Docker Compose deben reiniciarse autom\u00e1ticamente si fallan.</li> </ul> </li> <li> <p>Mantenibilidad</p> <ul> <li>RNF5.1: Arquitectura modular (SOLID, Clean Architecture).</li> <li>RNF5.2: C\u00f3digo documentado con ScalaDoc (motor).</li> <li>RNF5.3: Separaci\u00f3n de capas (API, servicios, modelos, configuraciones) tanto en backend como en frontend.</li> </ul> </li> <li> <p>Compatibilidad</p> <ul> <li>RNF6.1: Frontend soporta navegadores modernos (Chrome, Firefox, Edge).</li> <li>RNF6.2: No se requiere soporte para Internet Explorer.</li> </ul> </li> <li> <p>Tolerancia a fallos</p> <ul> <li>RNF7.1: Capturar excepciones en motor (sin detener el bucle principal) y registrar errores.</li> <li>RNF7.2: En backend, manejar timeouts y errores de WebHDFS con l\u00f3gica de reintentos b\u00e1sica.</li> </ul> </li> </ol>"},{"location":"03-diseno.html","title":"03 diseno","text":""},{"location":"03-diseno.html#3-diseno-diagramas","title":"3. Dise\u00f1o (Diagramas)","text":"<p>En la carpeta <code>docs/Img/</code> se incluyen todos los diagramas en PNG o PlantUML; aqu\u00ed se describen y referencian los m\u00e1s relevantes.</p>"},{"location":"03-diseno.html#31-casos-de-uso","title":"3.1 Casos de uso","text":"<p>A continuaci\u00f3n se resumen los casos de uso principales para todo el sistema:</p> <ol> <li> <p>Registrar Usuario</p> <ul> <li>Actor: Usuario no autenticado.</li> <li> <p>Flujo:</p> <ol> <li>Accede a <code>/register</code> (frontend).</li> <li>Completa email y contrase\u00f1a, env\u00eda <code>POST /auth/register</code> (backend).</li> <li>Si \u00e9xito (201), frontend redirige a <code>/login</code>; si error, muestra mensaje en formulario.</li> </ol> </li> </ul> </li> <li> <p>Iniciar Sesi\u00f3n</p> <ul> <li>Actor: Usuario no autenticado.</li> <li> <p>Flujo:</p> <ol> <li>Accede a <code>/login</code>.</li> <li>Completa credenciales, env\u00eda <code>POST /auth/login</code>.</li> <li>Si \u00e9xito (200), backend devuelve JWT; frontend guarda en <code>localStorage</code> y redirige a <code>/dashboard</code>.</li> <li>Si 401, muestra mensaje \u201cCredenciales inv\u00e1lidas\u201d.</li> </ol> </li> </ul> </li> <li> <p>Cerrar Sesi\u00f3n</p> <ul> <li>Actor: Usuario autenticado.</li> <li> <p>Flujo:</p> <ol> <li>En navbar, pulsa Logout.</li> <li>Frontend elimina token y redirige a <code>/login</code>.</li> </ol> </li> </ul> </li> <li> <p>Subir Fichero CSV</p> <ul> <li>Actor: Usuario autenticado.</li> <li> <p>Flujo:</p> <ol> <li>En <code>/dashboard</code>, selecciona CSV y pulsa Subir Fichero.</li> <li>Frontend muestra \u201cSubiendo\u2026\u201d, env\u00eda <code>POST /files/upload</code> con <code>multipart/form-data</code>.</li> <li>Backend guarda el fichero en <code>uploaded_files/</code> y crea/actualiza registro en <code>file_configuration</code>.</li> <li>Backend responde <code>{ \"file_config_id\": X }</code>; frontend muestra alerta \u201cFichero subido ID: X\u201d y refresca lista.</li> </ol> </li> </ul> </li> <li> <p>Listar Configuraciones</p> <ul> <li>Actor: Usuario autenticado.</li> <li> <p>Flujo:</p> <ol> <li>Al acceder a <code>/dashboard</code>, frontend hace <code>GET /files/</code>.</li> <li>Backend devuelve array de configuraciones; frontend muestra tabla con cada fila: ID, file_name, path, has_header, delimiter, etc.</li> </ol> </li> </ul> </li> <li> <p>Editar Configuraci\u00f3n (Modal)</p> <ul> <li>Actor: Usuario autenticado.</li> <li> <p>Flujo:</p> <ol> <li>En <code>/dashboard</code>, pulsa Detalles en una fila.</li> <li>Abre modal con campos prellenados (<code>delimiter</code>, <code>quote_char</code>, <code>escape_char</code>, <code>has_header</code>, <code>date_format</code>, <code>timestamp_format</code>, <code>partition_columns</code>).</li> <li>Usuario modifica valores y pulsa Guardar; frontend muestra \u201cGuardando\u2026\u201d, env\u00eda <code>PATCH /files/{id}</code>.</li> <li>Backend actualiza <code>file_configuration</code> y responde con objeto actualizado; frontend alerta \u201cConfiguraci\u00f3n actualizada\u201d, cierra modal y refresca lista.</li> </ol> </li> </ul> </li> <li> <p>Eliminar Configuraci\u00f3n</p> <ul> <li>Actor: Usuario autenticado.</li> <li> <p>Flujo:</p> <ol> <li>En <code>/dashboard</code> o modal, pulsa Eliminar.</li> <li>Mostrar <code>window.confirm(\"\u00bfEst\u00e1s seguro?\")</code>.</li> <li>Si confirma, frontend muestra \u201cEliminando\u2026\u201d, env\u00eda <code>DELETE /files/{id}</code>.</li> <li>Backend elimina registro; responde 204; frontend alerta \u201cConfiguraci\u00f3n eliminada\u201d y refresca lista.</li> </ol> </li> </ul> </li> <li> <p>Descargar Fichero</p> <ul> <li>Actor: Usuario autenticado.</li> <li> <p>Flujo:</p> <ol> <li>En modal, pulsa Descargar.</li> <li>Frontend hace <code>fetch(downloadURL, { headers: { Authorization: Bearer &lt;token&gt; } })</code>.</li> <li>Recibe Blob, crea <code>&lt;a&gt;</code> con <code>URL.createObjectURL(blob)</code> y <code>download=file_name</code>, ejecuta <code>a.click()</code>.</li> <li>Usuario obtiene el fichero en su equipo.</li> </ol> </li> </ul> </li> <li> <p>Enviar Fichero a HDFS (Backend \u2192 HDFS)</p> <ul> <li>Actor: Usuario autenticado.</li> <li> <p>Flujo:</p> <ol> <li>En <code>/dashboard</code>, pulsa Validar en una fila; frontend muestra \u201cEnviando\u2026\u201d, env\u00eda <code>POST /files/push/{file_name}</code>.</li> <li> <p>Backend:</p> <ul> <li>Verifica que <code>uploaded_files/{file_name}</code> exista; si no, responde 404.</li> <li>Llama a WebHDFS NameNode para crear directorio <code>/data/bank_accounts</code> y ajustar permisos (<code>op=MKDIRS</code>, <code>op=SETPERM</code>).</li> <li>Inicia creaci\u00f3n de fichero (<code>op=CREATE</code>), recibe redirecci\u00f3n 307 con URL de DataNode; ajusta host/puerto y sube contenido.</li> <li>Devuelve <code>{ \"message\": \"Pushed &lt;file_name&gt;\" }</code>.         3. Frontend alerta \u201cEnviado a validar\u201d y refresca lista.</li> </ul> </li> </ol> </li> </ul> </li> <li> <p>Proceso de validaci\u00f3n en Spark (motor)</p> <ul> <li>Actor: Sistema (motor de validaciones).</li> <li> <p>Flujo:</p> <ol> <li>Motor arranca en contenedor Docker; sale de safe mode (<code>hdfs dfsadmin -safemode leave</code>), crea carpeta en HDFS y copia CSV.</li> <li><code>Main.scala</code> ejecuta bucle de polling sobre <code>/data/bank_accounts</code>.</li> <li> <p>Al detectar fichero, llama a <code>ExecutionManager.executeFile(path, outputTable)</code>.</p> <ul> <li><code>Reader.readFile(...)</code> carga CSV como DataFrame.</li> <li><code>FileSentinel.verifyFiles(...)</code> \u2192 si falla, <code>logTrigger(flag)</code> en <code>trigger_control</code> (flags 32,33,34).</li> <li><code>TypeValidator.verifyTyping(...)</code> \u2192 si falla, <code>logTrigger(...)</code> (flags 35-38).</li> <li><code>ReferentialIntegrityValidator.verifyIntegrity(...)</code> \u2192 si falla, <code>logTrigger(39)</code>.</li> <li><code>FunctionalValidator.verifyFunctional(...)</code> \u2192 si falla, <code>logTrigger(flags 40\u201349)</code>.</li> <li>Si todo OK, <code>logTrigger(2)</code>.         4. Borra el fichero de HDFS.</li> </ul> </li> </ol> </li> </ul> </li> <li> <p>Consultar Logs de Validaci\u00f3n</p> <ul> <li>Actor: Usuario autenticado.</li> <li> <p>Flujo:</p> <ol> <li>Accede a <code>/logs</code>; frontend ejecuta <code>GET /files/logs?environment=&amp;from_date=&amp;to_date=</code>.</li> <li>Backend filtra registros en <code>trigger_control</code> seg\u00fan par\u00e1metros y devuelve array de logs.</li> <li>Frontend formatea <code>logged_at</code> con <code>toLocaleDateString('es-ES')</code> y muestra tabla con columnas: ID, file_config_id, file_name, field_name, environment, validation_flag, error_message, fecha.</li> </ol> </li> </ul> </li> </ol> <p>(Diagrams separados en docs/Img/Engine, docs/Img/Backend y docs/Img/Frontend respectivamente)</p>"},{"location":"03-diseno.html#32-diagrama-entidad-relacion","title":"3.2 Diagrama entidad-relaci\u00f3n","text":"<p>Tabla principal: <code>trigger_control</code></p> <ul> <li><code>id (serial PK)</code></li> <li><code>logged_at (timestamp)</code></li> <li><code>file_config_id (int FK \u2192 file_configuration.id)</code></li> <li><code>file_name (varchar)</code></li> <li><code>field_name (varchar, null)</code></li> <li><code>environment (varchar)</code></li> <li><code>validation_flag (varchar)</code></li> <li><code>error_message (varchar, null)</code></li> </ul> <p>Otras tablas relacionadas:</p> <ul> <li> <p><code>file_configuration</code></p> <ul> <li><code>id (serial PK)</code></li> <li><code>file_format (varchar)</code></li> <li><code>path (varchar)</code></li> <li><code>file_name (varchar)</code></li> <li><code>has_header (boolean)</code></li> <li><code>delimiter (varchar)</code></li> <li><code>quote_char (varchar)</code></li> <li><code>escape_char (varchar)</code></li> <li><code>date_format (varchar)</code></li> <li><code>timestamp_format (varchar)</code></li> <li><code>partition_columns (varchar, null)</code></li> <li><code>created_by (int FK \u2192 users.id)</code></li> <li><code>created_at (timestamp)</code></li> </ul> </li> <li> <p><code>semantic_layer</code></p> <ul> <li><code>id (serial PK)</code></li> <li><code>file_config_id (int FK \u2192 file_configuration.id)</code></li> <li><code>field_name (varchar)</code></li> <li><code>data_type (varchar)</code></li> <li><code>length (int)</code></li> <li><code>nullable (boolean)</code></li> <li><code>is_pk (boolean)</code></li> <li><code>format (varchar)</code></li> </ul> </li> <li> <p><code>users</code> (backend/front-end)</p> <ul> <li><code>id (serial PK)</code></li> <li><code>email (varchar, unique)</code></li> <li><code>hashed_password (varchar)</code></li> <li><code>is_active (boolean, default true)</code></li> <li><code>created_at (timestamp)</code></li> <li><code>updated_at (timestamp)</code></li> </ul> </li> <li> <p><code>negative_flag_logs</code> (opcional)</p> <ul> <li><code>id (serial PK)</code></li> <li><code>trigger_id (int FK \u2192 trigger_control.id)</code></li> <li>(otros campos de detalle)</li> </ul> </li> </ul> <p>Relaciones:</p> <ul> <li><code>users</code> 1\u2014* <code>file_configuration</code> (un usuario puede tener varias configuraciones).</li> <li><code>file_configuration</code> 1\u2014* <code>semantic_layer</code> (cada configuraci\u00f3n define metadatos para m\u00faltiples campos).</li> <li><code>file_configuration</code> 1\u2014* <code>trigger_control</code> (cada configuraci\u00f3n genera m\u00faltiples logs de validaci\u00f3n).</li> <li><code>semantic_layer</code> (opcional) 1\u2014* <code>trigger_control</code> (para identificar qu\u00e9 campo en <code>trigger_control.field_name</code> proviene de qu\u00e9 metadato).</li> <li><code>trigger_control</code> 1\u2014* <code>negative_flag_logs</code> (detalle de validaciones negativas).</li> </ul> <p></p> <p>(Ver <code>docs/Img/ERD.png</code> para la imagen completa.)</p>"},{"location":"03-diseno.html#33-diagrama-de-clases-del-modelo","title":"3.3 Diagrama de clases del modelo","text":"<p>Se incluyen las clases m\u00e1s relevantes en los tres componentes.</p>"},{"location":"03-diseno.html#331-motor-de-validaciones-scala","title":"3.3.1 Motor de validaciones (Scala)","text":"<ul> <li> <p>config</p> <ul> <li><code>DbConfig.scala</code></li> <li><code>DBConnection.scala</code></li> <li><code>SparkSessionProvider.scala</code></li> </ul> </li> <li> <p>models</p> <ul> <li><code>FileConfigurationCaseClass.scala</code></li> <li><code>SemanticLayerCaseClass.scala</code></li> </ul> </li> <li> <p>services</p> <ul> <li><code>ExecutionManager.scala</code></li> <li><code>TriggerIdManager.scala</code></li> </ul> </li> <li> <p>utils</p> <ul> <li><code>Reader.scala</code></li> <li><code>Writer.scala</code></li> <li><code>FileManager.scala</code></li> </ul> </li> <li> <p>validators</p> <ul> <li><code>FileSentinel.scala</code></li> <li><code>TypeValidator.scala</code></li> <li><code>ReferentialIntegrityValidator.scala</code></li> <li><code>FunctionalValidator.scala</code></li> </ul> </li> <li> <p>Main.scala: configura el bucle de polling e invoca a <code>ExecutionManager</code>.</p> </li> </ul> <p>Ejemplo de case class:</p> <pre><code>package models\n\ncase class FileConfigurationCaseClass(\n  id: Int,\n  file_format: String,\n  path: String,\n  file_name: String,\n  has_header: Boolean,\n  delimiter: String,\n  quote_char: String,\n  escape_char: String,\n  date_format: String,\n  timestamp_format: String,\n  partition_columns: Option[String]\n)\n</code></pre> <p>(Ver <code>docs/Img/Engine/</code> para la imagen completa.)</p>"},{"location":"03-diseno.html#332-frontend-react","title":"3.3.2 Frontend (React)","text":"<ul> <li> <p>AuthContext.jsx</p> <ul> <li>Propiedades: <code>user</code>, <code>token</code></li> <li>M\u00e9todos: <code>login()</code>, <code>register()</code>, <code>logout()</code></li> <li> <p>FileConfig (modelo JS)</p> </li> <li> <p>Propiedades: <code>id</code>, <code>fileName</code>, <code>path</code>, <code>hasHeader</code>, <code>delimiter</code>, <code>quoteChar</code>, <code>escapeChar</code>, <code>dateFormat</code>, <code>timestampFormat</code>, <code>partitionColumns</code></p> </li> <li>M\u00e9todos: <code>fetchAll()</code>, <code>create()</code>, <code>update()</code>, <code>delete()</code>, <code>pushToHDFS()</code>, <code>download()</code></li> <li> <p>ValidationLog (modelo JS)</p> </li> <li> <p>Propiedades: <code>id</code>, <code>fileConfigId</code>, <code>fileName</code>, <code>fieldName</code>, <code>environment</code>, <code>validationFlag</code>, <code>errorMessage</code>, <code>loggedAt</code></p> </li> <li>M\u00e9todos: <code>fetchAll()</code></li> <li> <p>Componentes principales</p> </li> <li> <p><code>AppRouter</code> (gestiona rutas con <code>RequireAuth</code>)</p> </li> <li><code>MainLayout</code> (Navbar + <code>&lt;Outlet /&gt;</code>)</li> <li><code>Dashboard</code> (muestra lista de configuraciones y acciones)</li> <li><code>FileDetailModal</code> (modal para editar/validar/eliminar/descargar)</li> <li><code>LogsPage</code> (muestra logs formateando <code>logged_at</code>)</li> </ul> </li> </ul> <p>(Ver <code>docs/Img/Frontend/</code> para la imagen completa.)</p>"},{"location":"03-diseno.html#333-backend-fastapi","title":"3.3.3 Backend (FastAPI)","text":"<ul> <li> <p>models ORM (SQLAlchemy Async)</p> <ul> <li> <p><code>User</code></p> <ul> <li><code>id: Integer PK</code></li> <li><code>email: String(255) UNIQUE NOT NULL</code></li> <li><code>hashed_password: String(255) NOT NULL</code></li> <li><code>is_active: Boolean NOT NULL DEFAULT True</code></li> <li><code>created_at: DateTime(timezone=True) DEFAULT now()</code></li> <li> <p><code>updated_at: DateTime(timezone=True)</code></p> <ul> <li><code>FileConfiguration</code></li> </ul> </li> <li> <p><code>id: Integer PK</code></p> </li> <li><code>file_format: String NOT NULL</code></li> <li><code>path: String NOT NULL</code></li> <li><code>file_name: String NOT NULL</code></li> <li><code>has_header: Boolean NOT NULL</code></li> <li><code>delimiter: String(1) NOT NULL</code></li> <li><code>quote_char: String(1) NOT NULL</code></li> <li><code>escape_char: String(1) NOT NULL</code></li> <li><code>date_format: String NOT NULL</code></li> <li><code>timestamp_format: String NOT NULL</code></li> <li> <p><code>partition_columns: String NULL</code></p> <ul> <li><code>TriggerControl</code></li> </ul> </li> <li> <p><code>id: Integer PK</code></p> </li> <li><code>file_config_id: Integer FK \u2192 FileConfiguration.id</code></li> <li><code>file_name: String NOT NULL</code></li> <li><code>field_name: String NOT NULL</code></li> <li><code>environment: String NOT NULL</code></li> <li><code>validation_flag: String NOT NULL</code></li> <li><code>error_message: Text NULL</code></li> <li> <p><code>logged_at: DateTime(timezone=True) DEFAULT now()</code></p> <ul> <li><code>NegativeFlagLog</code></li> </ul> </li> <li> <p><code>id: Integer PK</code></p> </li> <li><code>trigger_id: Integer FK \u2192 TriggerControl.id</code></li> <li>(otros campos espec\u00edficos)</li> </ul> </li> </ul> </li> <li> <p>Clases de servicio</p> <ul> <li> <p><code>file_service.py</code></p> <ul> <li> <p><code>save_and_register_file(file: UploadFile, db: AsyncSession) \u2192 int</code></p> <ul> <li><code>hdfs_sync.py</code></li> </ul> </li> <li> <p><code>push_file_to_hdfs(file_name: str)</code></p> </li> </ul> </li> </ul> </li> <li> <p>Controladores (routers)</p> <ul> <li><code>auth.py</code>: <code>/auth/register</code>, <code>/auth/login</code></li> <li><code>files.py</code>: <code>/files/upload</code>, <code>/files/push/{file_name}</code>, <code>/files/download/{file_name}</code>, <code>/files/</code>, <code>/files/{id}</code>, <code>/files/{id}</code> (GET, PATCH, DELETE), <code>/files/logs</code></li> <li><code>health.py</code>: <code>/health</code> (healthcheck)</li> </ul> </li> </ul> <p>(Ver <code>docs/Img/Backend/</code> para la imagen completa.)</p>"},{"location":"03-diseno.html#34-diagramas-de-secuencia","title":"3.4 Diagramas de secuencia","text":"<p>Se incluyen los diagramas de secuencia m\u00e1s relevantes para cada componente. A continuaci\u00f3n se describen de forma textual; ver <code>docs/</code> para los PNG o PlantUML.</p>"},{"location":"03-diseno.html#341-secuencia-subida-de-fichero-frontend-backend","title":"3.4.1 Secuencia: Subida de fichero (Frontend \u2192 Backend)","text":"<ol> <li>Usuario \u2192 Dashboard (Frontend): selecciona CSV y pulsa Subir Fichero.</li> <li>Dashboard \u2192 Axios (<code>POST /files/upload</code>): env\u00eda <code>multipart/form-data</code> al backend.</li> <li> <p>Backend (<code>files.py</code>) \u2192 <code>file_service.save_and_register_file()</code>:</p> <ul> <li>Guarda CSV en <code>uploaded_files/</code>.</li> <li>Inserta/actualiza registro en <code>file_configuration</code>.</li> <li>Backend \u2192 Dashboard: responde con <code>{ \"file_config_id\": X }</code> (201).</li> <li>Dashboard: muestra alerta \u201cFichero subido ID: X\u201d y llama <code>fetchConfigs()</code>.</li> </ul> </li> </ol>"},{"location":"03-diseno.html#342-secuencia-enviar-fichero-a-hdfs-frontend-backend-hdfs","title":"3.4.2 Secuencia: Enviar fichero a HDFS (Frontend \u2192 Backend \u2192 HDFS)","text":"<ol> <li>Usuario \u2192 Dashboard: pulsa Validar.</li> <li>Dashboard \u2192 Axios (<code>POST /files/push/{file_name}</code>): solicita al backend empuje a HDFS.</li> <li> <p>Backend (<code>files.py</code>) \u2192 <code>hdfs_sync.push_file_to_hdfs(file_name)</code>:</p> <ul> <li>Verifica existencia local en <code>uploaded_files/</code>.</li> <li>Llama a WebHDFS NameNode (<code>MKDIRS</code>, <code>SETPERM</code>).</li> <li>Llama <code>CREATE</code>, recibe 307 con header <code>Location</code> apuntando al DataNode.</li> <li>Ajusta URL para apuntar a <code>hdfs_datanode_host:hdfs_datanode_port</code>.</li> <li>Abre CSV y hace <code>PUT</code> a DataNode.</li> <li>HDFS DataNode \u2192 Backend: responde 201 Created si \u00e9xito.</li> <li>Backend \u2192 Dashboard: responde <code>{ \"message\": \"Pushed &lt;file_name&gt;\" }</code>.</li> <li>Dashboard: muestra alerta \u201cEnviado a validar\u201d y llama <code>fetchConfigs()</code>.</li> </ul> </li> </ol>"},{"location":"03-diseno.html#343-secuencia-proceso-de-validacion-en-spark-motor","title":"3.4.3 Secuencia: Proceso de validaci\u00f3n en Spark (Motor)","text":"<ol> <li> <p>Contenedor <code>validation-engine</code> \u2192 HDFS:</p> <ul> <li>Ejecuta <code>hdfs dfsadmin -safemode leave</code>.</li> <li><code>hdfs dfs -mkdir -p /data/bank_accounts</code>.</li> <li><code>hdfs dfs -put -f /local_bank_accounts/*.csv /data/bank_accounts</code>.</li> <li>Ajusta permisos <code>hdfs dfs -chmod -R 777 /data/bank_accounts</code>.</li> <li>Contenedor Spark (<code>spark-submit Main</code>) \u2192 HDFS.listStatus(): detecta CSV.</li> <li> <p>Main \u2192 ExecutionManager.executeFile(path, outputTable):</p> </li> <li> <p>Llama <code>Reader.readFile(...)</code> \u2192 devuelve DataFrame particionado.</p> </li> <li>Llama <code>FileSentinel.verifyFiles(...)</code> \u2192 si falla, <code>logTrigger(flag)</code> y salir.</li> <li>Llama <code>TypeValidator.verifyTyping(...)</code> \u2192 si falla, <code>logTrigger(flag)</code> y salir.</li> <li>Llama <code>ReferentialIntegrityValidator.verifyIntegrity(...)</code> \u2192 si falla, <code>logTrigger(39)</code> y salir.</li> <li>Llama <code>FunctionalValidator.verifyFunctional(...)</code> \u2192 si falla, <code>logTrigger(flags 40\u201349)</code> y salir.</li> <li>Si todo OK, <code>logTrigger(2)</code>.</li> <li>Escribe logs en PostgreSQL (<code>df.write.mode(\"append\").jdbc(...)</code>).</li> <li><code>HDFS.delete(path)</code>.</li> </ul> </li> </ol>"},{"location":"03-diseno.html#344-secuencia-listar-logs-de-validacion-frontend-backend-bd","title":"3.4.4 Secuencia: Listar Logs de Validaci\u00f3n (Frontend \u2192 Backend \u2192 BD)","text":"<ol> <li>Usuario \u2192 LogsPage (Frontend): al montar, llama <code>getLogs()</code>.</li> <li>LogsPage \u2192 Axios (<code>GET /files/logs?environment=&amp;from_date=&amp;to_date=</code>): solicita logs.</li> <li>**Backend (<code>files.py</code>) \u2192 consulta <code>trigger_control</code> filtrando por par\u00e1metros.</li> <li>Backend \u2192 LogsPage: devuelve array de objetos JSON con campos de log.</li> <li>LogsPage: recorre cada registro y formatea <code>logged_at</code> con <code>toLocaleDateString('es-ES')</code>.</li> <li>LogsPage: muestra tabla con columnas: ID, file_config_id, file_name, field_name, environment, validation_flag, error_message, fecha.</li> </ol>"},{"location":"04-implementacion.html","title":"Implementaci\u00f3n (GIT)","text":""},{"location":"04-implementacion.html#4-implementacion-git","title":"4. Implementaci\u00f3n (GIT)","text":""},{"location":"04-implementacion.html#41-diagrama-de-arquitectura","title":"4.1 Diagrama de arquitectura","text":"<pre><code>---\nconfig:\n  layout: dagre\n---\nflowchart LR\n  subgraph Usuario[\"Usuario\"]\n        Browser[\"User Browser\"]\n  end\n  subgraph Frontend[\"Frontend\"]\n        React[\"React SPA&lt;br&gt;(Web App)\"]\n  end\n  subgraph API[\"API\"]\n        FastAPI[\"FastAPI Backend&lt;br&gt;(Python)\"]\n  end\n  subgraph Persistencia[\"Persistencia\"]\n        Postgres[\"PostgreSQL DB\"]\n        HDFS[\"HDFS Cluster\"]\n  end\n  subgraph Procesamiento[\"Procesamiento\"]\n        SparkMaster[\"\ud83d\udd37 Spark Master\"]\n        SparkWorker1[\"\ud83d\udd36 Spark Worker 1\"]\n        SparkWorker2[\"\ud83d\udd36 Spark Worker 2\"]\n        ValidationEngine[\"Validation Engine&lt;br&gt;(Scala + Spark)\"]\n  end\n  subgraph subGraph7[\"Contenedores Docker\"]\n        Superset[\"Superset BI\"]\n  end\n\n  Browser -- HTTP requests --&gt; React\n  React -- REST/GraphQL&lt;br&gt;JSON over HTTP --&gt; FastAPI\n  FastAPI -- SQLAlchemy (INSERT/SELECT/UPDATE) --&gt; Postgres\n  FastAPI -- WebHDFS / HDFS Client --&gt; HDFS\n  FastAPI -- SELECT * FROM TABLE --&gt; Postgres\n  ValidationEngine -- Leer CSV/Parquet --&gt; HDFS\n  ValidationEngine -- INSERT logs en trigger_control --&gt; Postgres\n  ValidationEngine -- Spark job submission --&gt; SparkMaster\n  SparkMaster --&gt; SparkWorker1 &amp; SparkWorker2\n  SparkMaster -- HDFS FS Default --- HDFS\n  SparkWorker1 -- HDFS FS Default --- HDFS\n  SparkWorker2 -- HDFS FS Default --- HDFS\n  Superset -- SQLAlchemy --&gt; Postgres\n\n  style Usuario fill:#f9f,stroke:#333,stroke-width:1px\n  style Frontend fill:#ccf,stroke:#333,stroke-width:1px\n  style API fill:#cfc,stroke:#333,stroke-width:1px\n  style Persistencia fill:#ffe599,stroke:#333,stroke-width:1px\n  style Procesamiento fill:#f4cccc,stroke:#333,stroke-width:1px\n</code></pre> <ul> <li> <p>Frontend (React + Vite)</p> <ul> <li>Usuario interact\u00faa con la UI, llama rutas REST al backend y muestra datos.</li> <li> <p>Backend (FastAPI)</p> </li> <li> <p>Exposici\u00f3n de endpoints <code>/auth</code>, <code>/files</code>, <code>/files/logs</code>.</p> </li> <li>Usa SQLAlchemy Async + AsyncPG para comunicarse con PostgreSQL.</li> <li>Utiliza requests para llamar a WebHDFS (NameNode/DataNode).</li> <li> <p>PostgreSQL</p> </li> <li> <p>Almacena <code>users</code>, <code>file_configuration</code>, <code>trigger_control</code>, <code>negative_flag_logs</code>.</p> </li> <li>Superset (opcional) tambi\u00e9n se conecta aqu\u00ed para dashboards.</li> <li> <p>Motor Scala + Spark</p> </li> <li> <p>Se ejecuta como contenedor independiente, se conecta a HDFS (<code>hdfs://hadoop-namenode:9000</code>).</p> </li> <li>Procesa ficheros mediante Spark Streaming en modo batch y escribe logs en PostgreSQL mediante JDBC.</li> <li> <p>HDFS (NameNode/DataNode)</p> </li> <li> <p>Almacena CSV en <code>/data/bank_accounts</code>.</p> </li> <li>Motor de validaciones lee ficheros desde aqu\u00ed y borra tras procesar.</li> </ul> </li> </ul> <p>(Ver <code>docs/diagrama_arquitectura.png</code> para imagen detallada.)</p>"},{"location":"04-implementacion.html#42-tecnologias","title":"4.2 Tecnolog\u00edas","text":"<p>A continuaci\u00f3n se especifican todas las tecnolog\u00edas y dependencias utilizadas en cada componente:</p>"},{"location":"04-implementacion.html#421-motor-de-validaciones-scala-spark","title":"4.2.1 Motor de validaciones (Scala + Spark)","text":"<ul> <li>Scala 2.12</li> <li>Apache Spark 3.x</li> <li>HDFS (Hadoop 3.x)</li> <li>SBT (build tool de Scala)</li> <li>Kryo (serializaci\u00f3n para Spark)</li> <li>Docker (OpenJDK 11-slim, bitnami/spark)</li> <li>Docker Compose (orquestaci\u00f3n)</li> </ul>"},{"location":"04-implementacion.html#422-backend-fastapi","title":"4.2.2 Backend (FastAPI)","text":"<ul> <li>Python 3.12</li> <li>FastAPI &gt;= 0.100.0</li> <li>Uvicorn[standard] &gt;= 0.23.0</li> <li>SQLAlchemy &gt;= 2.0 (Async)</li> <li>asyncpg &gt;= 0.28</li> <li>Pydantic Settings &gt;= 2.0</li> <li>python-jose &gt;= 3.3</li> <li>passlib[bcrypt] &gt;= 1.7</li> <li>requests &gt;= 2.31</li> <li>python-dotenv &gt;= 1.0</li> <li>Docker (para ejecutar contenedores de backend en producci\u00f3n si se desea)</li> </ul>"},{"location":"04-implementacion.html#423-frontend-react","title":"4.2.3 Frontend (React)","text":"<ul> <li>React 18+</li> <li>React Router v6</li> <li>Axios</li> <li>Context API (AuthContext)</li> <li>Vite</li> <li>CSS puro</li> </ul>"},{"location":"04-implementacion.html#424-base-de-datos-y-otros","title":"4.2.4 Base de datos y otros","text":"<ul> <li>PostgreSQL 13</li> <li>Kafka (opcional)</li> <li>Zookeeper (opcional)</li> <li>Superset (opcional, para dashboards)</li> <li>PlantUML (diagramas de referencia)</li> </ul>"},{"location":"04-implementacion.html#43-codigo-explicacion-de-las-partes-mas-interesantes","title":"4.3 C\u00f3digo (Explicaci\u00f3n de las partes m\u00e1s interesantes)","text":"<p>Se destacan las implementaciones clave de cada componente.</p>"},{"location":"04-implementacion.html#431-motor-de-validaciones-scala","title":"4.3.1 Motor de validaciones (Scala)","text":"<ol> <li> <p><code>SparkSessionProvider.scala</code></p> <ul> <li>Configura la <code>SparkSession</code> con par\u00e1metros personalizados:</li> </ul> <p><code>scala   val spark = SparkSession.builder()     .appName(\"ValidationEngine\")     .master(\"spark://spark-master:7077\")     .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")     .config(\"spark.kryo.registrator\", \"com.mycompany.KryoRegistrator\")     .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://hadoop-namenode:9000\")     .getOrCreate()</code> * Ajusta particionamiento y directorios locales a trav\u00e9s de <code>SPARK_LOCAL_DIRS</code>.</p> </li> <li> <p><code>ExecutionManager.scala</code></p> <ul> <li>Orquesta todas las validaciones:</li> </ul> <p><code>scala   def executeFile(path: String, outputTable: String): Unit = {     val df = Reader.readFile(spark, path)     if (!FileSentinel.verifyFiles(df, metadata)) {       logTrigger(flag, metadata, path)       return     }     if (!TypeValidator.verifyTyping(df, metadata)) {       logTrigger(flag, metadata, path)       return     }     if (!ReferentialIntegrityValidator.verifyIntegrity(df, metadata)) {       logTrigger(flag, metadata, path)       return     }     FunctionalValidator.verifyFunctional(df, metadata) match {       case Some(error) =&gt;          logTrigger(errorFlag, metadata, path)         return       case None =&gt;          logTrigger(2, metadata, path)     }     // Registrar en PostgreSQL     Writer.writeToJdbc(df, outputTable, dbConfig)     // Borrar de HDFS     spark.sparkContext.hadoopConfiguration       .delete(new Path(path), false)   }</code> * <code>logTrigger(flag, metadata, path)</code>: escribe un DataFrame con columns (<code>logged_at</code>, <code>file_config_id</code>, <code>file_name</code>, <code>field_name</code>, <code>environment</code>, <code>validation_flag</code>, <code>error_message</code>) y hace <code>df.write.mode(\"append\").jdbc(...)</code>.</p> </li> <li> <p>Multi-Stage Dockerfile (<code>docker/Dockerfile.engine</code>)</p> </li> </ol> <p>```dockerfile    # Stage 1: Build con OpenJDK y SBT    FROM openjdk:11-slim AS builder    WORKDIR /app</p> <p>RUN apt-get update &amp;&amp; apt-get install -y curl gnupg &amp;&amp; \\        echo \"deb https://repo.scala-sbt.org/scalasbt/debian all main\" &gt; /etc/apt/sources.list.d/sbt.list &amp;&amp; \\        curl -sL \"https://keyserver.ubuntu.com/pks/lookup?op=get&amp;search=0x99E82A75642AC823\" | apt-key add - &amp;&amp; \\        apt-get update &amp;&amp; apt-get install -y sbt &amp;&amp; rm -rf /var/lib/apt/lists/*</p> <p>COPY ../project   project/    COPY ../build.sbt build.sbt    COPY ../src       src/    COPY ../db.properties db.properties    COPY ../src/main/resources/application.conf src/main/resources/</p> <p>RUN sbt clean assembly</p> <p># Stage 2: Runtime Spark    FROM bitnami/spark:3.3.1    WORKDIR /app</p> <p>COPY --from=builder /app/target/scala-2.12/Fin_de_Grado-assembly-0.1.0-SNAPSHOT.jar app.jar    COPY ../db.properties    db.properties    COPY ../src/main/resources/application.conf application.conf</p> <p>ENV INPUT_DIR=/data/bank_accounts \\        OUTPUT_TABLE=trigger_control \\        POLL_INTERVAL_MS=10000 \\        SPARK_LOCAL_DIRS=/tmp/spark_local</p> <p>ENTRYPOINT spark-submit \\      --class Main \\      --master spark://spark-master:7077 \\      --deploy-mode client \\      --conf spark.driver.host=validation-engine \\      --conf spark.hadoop.fs.defaultFS=hdfs://hadoop-namenode:9000 \\      /app/app.jar    ```</p> <pre><code>* **Stage 1:** instala SBT, compila fat JAR con todas las dependencias.\n* **Stage 2:** parte de una imagen oficial de Spark; copia JAR, `db.properties` y `application.conf`; expone variables de entorno y lanza `spark-submit`.\n</code></pre> <ol> <li>Script de reconstrucci\u00f3n (<code>scripts/rebuild_and_run.sh</code>)</li> </ol> <p>```bash    #!/usr/bin/env bash    set -euo pipefail</p> <p># Construir el JAR con SBT    sbt clean assembly</p> <p># Crear red si no existe    docker network create superset-net || true</p> <p># Reconstruir y ejecutar solo el contenedor de validation-engine    cd docker    docker-compose build validation-engine    docker-compose up --abort-on-container-exit validation-engine    ```</p> <pre><code>* Compila el JAR, crea la red `superset-net` y levanta \u00fanicamente el contenedor del motor de validaciones (asume que los dem\u00e1s servicios est\u00e1n corriendo).\n</code></pre>"},{"location":"04-implementacion.html#432-backend-fastapi","title":"4.3.2 Backend (FastAPI)","text":"<ol> <li>Configuraci\u00f3n (<code>app/core/config.py</code>)</li> </ol> <p>```python    from pydantic import BaseSettings</p> <p>class Settings(BaseSettings):        postgres_user: str        postgres_password: str        postgres_host: str        postgres_port: int        postgres_db: str</p> <pre><code>   hdfs_host: str\n   hdfs_port: int\n   hdfs_dir: str\n   hdfs_user: str\n   hdfs_datanode_host: str\n   hdfs_datanode_port: int\n\n   upload_dir: str\n   jwt_secret_key: str\n   jwt_algorithm: str\n   access_token_expire_minutes: int\n\n   class Config:\n       env_file = \".env\"\n</code></pre> <p>```</p> <pre><code>* Carga variables desde `.env` y construye `database_url = f\"postgresql+asyncpg://{user}:{password}@{host}:{port}/{db}\"`.\n</code></pre> <ol> <li>Seguridad (<code>app/core/security.py</code>)</li> </ol> <p>```python    from jose import JWTError, jwt    from passlib.context import CryptContext    from fastapi import Depends, HTTPException, status    from fastapi.security import OAuth2PasswordBearer    from sqlalchemy.ext.asyncio import AsyncSession    from .config import Settings    from app.db.models.user import UserModel</p> <p>pwd_context = CryptContext(schemes=[\"bcrypt\"], deprecated=\"auto\")    oauth2_scheme = OAuth2PasswordBearer(tokenUrl=\"/auth/login\")</p> <p>def verify_password(plain_password, hashed_password):        return pwd_context.verify(plain_password, hashed_password)</p> <p>def get_password_hash(password):        return pwd_context.hash(password)</p> <p>def create_access_token(subject: str, expires_delta=None):        to_encode = {\"sub\": subject}        # agregar expiraci\u00f3n...        encoded_jwt = jwt.encode(to_encode, Settings().jwt_secret_key, algorithm=Settings().jwt_algorithm)        return encoded_jwt</p> <p>async def get_current_user(token: str = Depends(oauth2_scheme), db: AsyncSession = Depends(get_db)):        credentials_exception = HTTPException(            status_code=status.HTTP_401_UNAUTHORIZED,            detail=\"No autenticado\",            headers={\"WWW-Authenticate\": \"Bearer\"},        )        try:            payload = jwt.decode(token, Settings().jwt_secret_key, algorithms=[Settings().jwt_algorithm])            user_id: str = payload.get(\"sub\")            if user_id is None:                raise credentials_exception        except JWTError:            raise credentials_exception        user = await db.get(UserModel, int(user_id))        if user is None:            raise credentials_exception        return user    ```</p> <pre><code>* Define hashing de contrase\u00f1as, creaci\u00f3n de JWT, y dependencia `get_current_user` que valida el token y extrae `sub`.\n</code></pre> <ol> <li>Servicio de ficheros (<code>app/services/file_service.py</code>)</li> </ol> <p>```python    import os    from sqlalchemy.ext.asyncio import AsyncSession    from app.db.models.file_configuration import FileConfigurationModel</p> <p>async def save_and_register_file(file, db: AsyncSession) -&gt; int:        upload_dir = Settings().upload_dir        os.makedirs(upload_dir, exist_ok=True)        file_path = os.path.join(upload_dir, file.filename)        with open(file_path, \"wb\") as f:            content = await file.read()            f.write(content)        result = await db.execute(select(FileConfigurationModel).where(FileConfigurationModel.file_name == file.filename))        existing = result.scalar_one_or_none()        if existing:            # actualizar par\u00e1metros si vienen en request            existing.some_field = ...            await db.commit()            return existing.id        else:            new_cfg = FileConfigurationModel(                file_format=\"csv\",                path=upload_dir,                file_name=file.filename,                has_header=True,                delimiter=\",\",                quote_char='\"',                escape_char=\"\\\",                date_format=\"yyyy-MM-dd\",                timestamp_format=\"yyyy-MM-dd HH:mm:ss\",                partition_columns=None            )            db.add(new_cfg)            await db.flush()            return new_cfg.id    ```</p> <pre><code>* Guarda el CSV en disco local y crea/actualiza el registro en `file_configuration`.\n</code></pre> <ol> <li>Servicio HDFS (<code>app/services/hdfs_sync.py</code>)</li> </ol> <p>```python    import requests    from fastapi import HTTPException    from .config import Settings</p> <p>def push_file_to_hdfs(file_name: str):        settings = Settings()        local_path = os.path.join(settings.upload_dir, file_name)        if not os.path.exists(local_path):            raise HTTPException(status_code=404, detail=f\"Local not found: {local_path}\")</p> <pre><code>   # 1) MKDIRS\n   mkdir_url = f\"http://{settings.hdfs_host}:{settings.hdfs_port}/webhdfs/v1{settings.hdfs_dir}?op=MKDIRS&amp;user.name={settings.hdfs_user}\"\n   resp = requests.put(mkdir_url)\n   resp.raise_for_status()\n\n   # 2) SETPERM\n   perm_url = f\"http://{settings.hdfs_host}:{settings.hdfs_port}/webhdfs/v1{settings.hdfs_dir}?op=SETPERM&amp;permission=777&amp;user.name={settings.hdfs_user}\"\n   resp = requests.put(perm_url)\n   resp.raise_for_status()\n\n   # 3) CREATE (NameNode \u2192 307 \u2192 Location \u2192 DataNode)\n   create_url = f\"http://{settings.hdfs_host}:{settings.hdfs_port}/webhdfs/v1{settings.hdfs_dir}/{file_name}?op=CREATE&amp;overwrite=true&amp;user.name={settings.hdfs_user}\"\n   resp = requests.put(create_url, allow_redirects=False)\n   location = resp.headers.get(\"Location\")\n   if not location:\n       raise HTTPException(status_code=500, detail=\"HDFS create failed: no redirect\")\n   # Ajustar URL al DataNode\n   upload_url = location.replace(f\"{settings.hdfs_host}:{settings.hdfs_port}\", f\"{settings.hdfs_datanode_host}:{settings.hdfs_datanode_port}\")\n   with open(local_path, \"rb\") as f:\n       resp2 = requests.put(upload_url, data=f)\n       resp2.raise_for_status()\n</code></pre> <p>```</p> <pre><code>* Gestiona la secuencia WebHDFS (MKDIRS, SETPERM, CREATE \u2192 307 \u2192 PUT a DataNode).\n</code></pre> <ol> <li>Rutas de archivos (<code>app/api/files.py</code>)</li> </ol> <p>```python    from fastapi import APIRouter, Depends, UploadFile, File, HTTPException    from sqlalchemy.ext.asyncio import AsyncSession    from app.services.file_service import save_and_register_file    from app.services.hdfs_sync import push_file_to_hdfs    from app.db.session import get_db</p> <p>router = APIRouter(prefix=\"/files\", tags=[\"files\"])</p> <p>@router.post(\"/upload\", status_code=201)    async def upload_file(file: UploadFile = File(...), db: AsyncSession = Depends(get_db)):        try:            file_id = await save_and_register_file(file, db)            return {\"file_config_id\": file_id}        except Exception as e:            raise HTTPException(status_code=500, detail=str(e))</p> <p>@router.post(\"/push/{file_name}\")    async def push_to_hdfs(file_name: str, user=Depends(get_current_user)):        try:            push_file_to_hdfs(file_name)            return {\"message\": f\"Pushed {file_name}\"}        except HTTPException as he:            raise he        except Exception as e:            raise HTTPException(status_code=500, detail=str(e))</p> <p>@router.get(\"/\")    async def list_configs(db: AsyncSession = Depends(get_db)):        result = await db.execute(select(FileConfigurationModel))        configs = result.scalars().all()        return configs</p> <p>@router.get(\"/{id}\")    async def get_config(id: int, db: AsyncSession = Depends(get_db)):        cfg = await db.get(FileConfigurationModel, id)        if not cfg:            raise HTTPException(status_code=404, detail=\"Not found\")        return cfg</p> <p>@router.patch(\"/{id}\")    async def update_config(id: int, payload: FileConfigUpdate, db: AsyncSession = Depends(get_db)):        cfg = await db.get(FileConfigurationModel, id)        if not cfg:            raise HTTPException(status_code=404, detail=\"Not found\")        for key, val in payload.dict(exclude_unset=True).items():            setattr(cfg, key, val)        await db.commit()        return cfg</p> <p>@router.delete(\"/{id}\", status_code=204)    async def delete_config(id: int, db: AsyncSession = Depends(get_db)):        cfg = await db.get(FileConfigurationModel, id)        if not cfg:            raise HTTPException(status_code=404, detail=\"Not found\")        await db.delete(cfg)        await db.commit()</p> <p>@router.get(\"/download/{file_name}\")    async def download_file(file_name: str):        file_path = os.path.join(Settings().upload_dir, file_name)        if not os.path.exists(file_path):            raise HTTPException(status_code=404, detail=\"File not found\")        return FileResponse(file_path, media_type=\"text/csv\", filename=file_name)</p> <p>@router.get(\"/logs\")    async def get_logs(environment: str = None, from_date: date = None, to_date: date = None, db: AsyncSession = Depends(get_db)):        query = select(TriggerControlModel)        if environment:            query = query.where(TriggerControlModel.environment == environment)        if from_date:            query = query.where(TriggerControlModel.logged_at &gt;= from_date)        if to_date:            query = query.where(TriggerControlModel.logged_at &lt;= to_date)        result = await db.execute(query.order_by(TriggerControlModel.logged_at.desc()))        logs = result.scalars().all()        return logs    ```</p> <pre><code>* Endpoints para CRUD de configuraciones, subida, push a HDFS, descarga y consulta de logs.\n</code></pre>"},{"location":"04-implementacion.html#433-frontend-react-vite","title":"4.3.3 Frontend (React + Vite)","text":"<ol> <li>Configuraci\u00f3n de Axios (<code>src/api/axiosConfig.js</code>)</li> </ol> <p>```js    import axios from 'axios';</p> <p>const api = axios.create({      baseURL: 'http://localhost:8000',    });</p> <p>// Interceptor de petici\u00f3n: a\u00f1ade token si existe    api.interceptors.request.use(config =&gt; {      const token = localStorage.getItem('access_token');      if (token) {        config.headers.Authorization = <code>Bearer ${token}</code>;      }      return config;    });</p> <p>// Interceptor de respuesta: si 401, borrar token y redirigir    api.interceptors.response.use(      response =&gt; response,      error =&gt; {        if (error.response &amp;&amp; error.response.status === 401) {          localStorage.removeItem('access_token');          alert('Tu sesi\u00f3n ha expirado. Por favor, inicia sesi\u00f3n de nuevo.');          window.location.href = '/login';        }        return Promise.reject(error);      }    );</p> <p>export default api;    ```</p> <pre><code>* Gestiona token en cada petici\u00f3n y maneja 401 autom\u00e1ticamente.\n</code></pre> <ol> <li><code>AuthContext.jsx</code></li> </ol> <p>```jsx    import React, { createContext, useState, useEffect } from 'react';    import { useNavigate } from 'react-router-dom';    import api from '../api/axiosConfig';</p> <p>export const AuthContext = createContext();</p> <p>export const AuthProvider = ({ children }) =&gt; {      const [token, setToken] = useState(localStorage.getItem('access_token'));      const navigate = useNavigate();</p> <pre><code> useEffect(() =&gt; {\n   if (token) {\n     localStorage.setItem('access_token', token);\n   } else {\n     localStorage.removeItem('access_token');\n   }\n }, [token]);\n\n const login = async (email, password) =&gt; {\n   const res = await api.post('/auth/login', { email, password });\n   setToken(res.data.access_token);\n   navigate('/dashboard');\n };\n\n const register = async (email, password) =&gt; {\n   await api.post('/auth/register', { email, password });\n   navigate('/login');\n };\n\n const logout = () =&gt; {\n   setToken(null);\n   navigate('/login');\n };\n\n return (\n   &lt;AuthContext.Provider value={{ token, login, register, logout }}&gt;\n     {children}\n   &lt;/AuthContext.Provider&gt;\n );\n</code></pre> <p>};    ```</p> <pre><code>* Gestiona estado global de autenticaci\u00f3n y m\u00e9todos `login()`, `register()`, `logout()`.\n</code></pre> <ol> <li><code>AppRouter.jsx</code></li> </ol> <p>```jsx    import React, { useContext } from 'react';    import { BrowserRouter, Routes, Route, Navigate, useLocation } from 'react-router-dom';    import { AuthContext } from '../contexts/AuthContext';    import LoginPage from '../pages/LoginPage';    import RegisterPage from '../pages/RegisterPage';    import Dashboard from '../pages/Dashboard';    import LogsPage from '../pages/LogsPage';    import MainLayout from '../layouts/MainLayout';</p> <p>const RequireAuth = ({ children }) =&gt; {      const { token } = useContext(AuthContext);      const location = useLocation();      return token ? children : ;    };</p> <p>const AppRouter = () =&gt; (       } /&gt;          } /&gt;          \\\\}&gt;            } /&gt;            } /&gt;            } /&gt;           } /&gt;             );</p> <p>export default AppRouter;    ```</p> <pre><code>* Define rutas p\u00fablicas (`/login`, `/register`) y privadas (`/dashboard`, `/logs`) con `RequireAuth`.\n</code></pre> <ol> <li><code>Dashboard.jsx</code></li> </ol> <p>```jsx    import React, { useState, useEffect, useContext } from 'react';    import { AuthContext } from '../contexts/AuthContext';    import api from '../api/axiosConfig';    import FileDetailModal from '../components/FileDetailModal';</p> <p>const Dashboard = () =&gt; {      const [configs, setConfigs] = useState([]);      const [selectedFile, setSelectedFile] = useState(null);      const [uploading, setUploading] = useState(false);      const [showModal, setShowModal] = useState(false);      const [currentConfig, setCurrentConfig] = useState(null);</p> <pre><code> const fetchConfigs = async () =&gt; {\n   const res = await api.get('/files/');\n   setConfigs(res.data);\n };\n\n useEffect(() =&gt; {\n   fetchConfigs();\n }, []);\n\n const handleUpload = async () =&gt; {\n   if (!selectedFile) return;\n   setUploading(true);\n   const form = new FormData();\n   form.append('file', selectedFile);\n   const res = await api.post('/files/upload', form);\n   alert(`Fichero subido ID: ${res.data.file_config_id}`);\n   fetchConfigs();\n   setUploading(false);\n };\n\n const handleValidate = async (fileName) =&gt; {\n   await api.post(`/files/push/${fileName}`);\n   alert('Enviado a validar');\n   fetchConfigs();\n };\n\n const handleDelete = async (id) =&gt; {\n   if (!window.confirm('\u00bfEst\u00e1s seguro?')) return;\n   await api.delete(`/files/${id}`);\n   alert('Configuraci\u00f3n eliminada');\n   fetchConfigs();\n };\n\n return (\n   &lt;div&gt;\n     &lt;h2&gt;Dashboard&lt;/h2&gt;\n     &lt;input type=\"file\" onChange={e =&gt; setSelectedFile(e.target.files[0])} /&gt;\n     &lt;button onClick={handleUpload} disabled={uploading}&gt;\n       {uploading ? 'Subiendo...' : 'Subir Fichero'}\n     &lt;/button&gt;\n     &lt;table&gt;\n       &lt;thead&gt;\n         &lt;tr&gt;\n           &lt;th&gt;ID&lt;/th&gt;\n           &lt;th&gt;Fichero&lt;/th&gt;\n           &lt;th&gt;Has Header&lt;/th&gt;\n           &lt;th&gt;Delimiter&lt;/th&gt;\n           &lt;th&gt;Acciones&lt;/th&gt;\n         &lt;/tr&gt;\n       &lt;/thead&gt;\n       &lt;tbody&gt;\n         {configs.map(cfg =&gt; (\n           &lt;tr key={cfg.id}&gt;\n             &lt;td&gt;{cfg.id}&lt;/td&gt;\n             &lt;td&gt;{cfg.file_name}&lt;/td&gt;\n             &lt;td&gt;{cfg.has_header ? 'S\u00ed' : 'No'}&lt;/td&gt;\n             &lt;td&gt;{cfg.delimiter}&lt;/td&gt;\n             &lt;td&gt;\n               &lt;button onClick={() =&gt; handleValidate(cfg.file_name)}&gt;Enviando\u2026&lt;/button&gt;\n               &lt;button onClick={() =&gt; { setCurrentConfig(cfg); setShowModal(true); }}&gt;Detalles&lt;/button&gt;\n               &lt;button onClick={() =&gt; handleDelete(cfg.id)}&gt;Eliminar&lt;/button&gt;\n             &lt;/td&gt;\n           &lt;/tr&gt;\n         ))}\n       &lt;/tbody&gt;\n     &lt;/table&gt;\n     {showModal &amp;&amp; (\n       &lt;FileDetailModal\n         config={currentConfig}\n         onClose={() =&gt; setShowModal(false)}\n         onRefresh={fetchConfigs}\n       /&gt;\n     )}\n   &lt;/div&gt;\n );\n</code></pre> <p>};</p> <p>export default Dashboard;    ```</p> <pre><code>* Subida de fichero, lista de configuraciones con acciones (Validar, Detalles, Eliminar).\n</code></pre> <ol> <li><code>FileDetailModal.jsx</code></li> </ol> <p>```jsx    import React, { useState } from 'react';    import api from '../api/axiosConfig';</p> <p>const FileDetailModal = ({ config, onClose, onRefresh }) =&gt; {      const [delimiter, setDelimiter] = useState(config.delimiter);      const [quoteChar, setQuoteChar] = useState(config.quote_char);      const [escapeChar, setEscapeChar] = useState(config.escape_char);      const [hasHeader, setHasHeader] = useState(config.has_header);      const [dateFormat, setDateFormat] = useState(config.date_format);      const [timestampFormat, setTimestampFormat] = useState(config.timestamp_format);      const [partitionColumns, setPartitionColumns] = useState(config.partition_columns);      const [saving, setSaving] = useState(false);      const [validating, setValidating] = useState(false);      const [deleting, setDeleting] = useState(false);</p> <pre><code> const save = async () =&gt; {\n   setSaving(true);\n   await api.patch(`/files/${config.id}`, {\n     delimiter, quote_char: quoteChar, escape_char: escapeChar,\n     has_header: hasHeader, date_format: dateFormat,\n     timestamp_format: timestampFormat, partition_columns: partitionColumns\n   });\n   alert('Configuraci\u00f3n actualizada');\n   setSaving(false);\n   onClose();\n   onRefresh();\n };\n\n const validate = async () =&gt; {\n   setValidating(true);\n   await api.post(`/files/push/${config.file_name}`);\n   alert('Enviado a validar');\n   setValidating(false);\n   onClose();\n   onRefresh();\n };\n\n const downloadFile = async () =&gt; {\n   const res = await fetch(`http://localhost:8000/files/download/${config.file_name}`, {\n     headers: { Authorization: `Bearer ${localStorage.getItem('access_token')}` }\n   });\n   const blob = await res.blob();\n   const url = URL.createObjectURL(blob);\n   const a = document.createElement('a');\n   a.href = url;\n   a.download = config.file_name;\n   a.click();\n   URL.revokeObjectURL(url);\n };\n\n const remove = async () =&gt; {\n   if (!window.confirm('\u00bfEst\u00e1s seguro?')) return;\n   setDeleting(true);\n   await api.delete(`/files/${config.id}`);\n   alert('Configuraci\u00f3n eliminada');\n   setDeleting(false);\n   onClose();\n   onRefresh();\n };\n\n const overlayStyle = {\n   position: 'fixed', top: 0, left: 0, right: 0, bottom: 0,\n   backgroundColor: 'rgba(0,0,0,0.5)', display: 'flex',\n   justifyContent: 'center', alignItems: 'center'\n };\n const modalStyle = {\n   background: '#fff', padding: '1rem', borderRadius: '8px',\n   position: 'relative', width: '500px'\n };\n\n return (\n   &lt;div style={overlayStyle} onClick={onClose}&gt;\n     &lt;div style={modalStyle} onClick={e =&gt; e.stopPropagation()}&gt;\n       &lt;h3&gt;Detalles Fichero&lt;/h3&gt;\n       &lt;label&gt;Delimiter:&lt;/label&gt;\n       &lt;select value={delimiter} onChange={e =&gt; setDelimiter(e.target.value)}&gt;\n         &lt;option value=\",\"&gt;Comma&lt;/option&gt;\n         &lt;option value=\";\"&gt;Semicolon&lt;/option&gt;\n         &lt;option value=\"\\t\"&gt;Tab&lt;/option&gt;\n         &lt;option value=\"|\"&gt;Pipe&lt;/option&gt;\n       &lt;/select&gt;\n       &lt;label&gt;Quote Character:&lt;/label&gt;\n       &lt;select value={quoteChar} onChange={e =&gt; setQuoteChar(e.target.value)}&gt;\n         &lt;option value={'\"'}&gt;Double&lt;/option&gt;\n         &lt;option value=\"'\"&gt;Single&lt;/option&gt;\n         &lt;option value=\"\"&gt;None&lt;/option&gt;\n       &lt;/select&gt;\n       &lt;label&gt;Escape Character:&lt;/label&gt;\n       &lt;select value={escapeChar} onChange={e =&gt; setEscapeChar(e.target.value)}&gt;\n         &lt;option value=\"\\\\\"&gt;Backslash&lt;/option&gt;\n         &lt;option value={'\"'}&gt;Double&lt;/option&gt;\n         &lt;option value=\"\"&gt;None&lt;/option&gt;\n       &lt;/select&gt;\n       &lt;label&gt;Has Header:&lt;/label&gt;\n       &lt;input type=\"checkbox\" checked={hasHeader} onChange={e =&gt; setHasHeader(e.target.checked)} /&gt;\n       &lt;label&gt;Date Format:&lt;/label&gt;\n       &lt;select value={dateFormat} onChange={e =&gt; setDateFormat(e.target.value)}&gt;\n         &lt;option value=\"yyyy-MM-dd\"&gt;YYYY-MM-DD&lt;/option&gt;\n         &lt;option value=\"dd/MM/yyyy\"&gt;DD/MM/YYYY&lt;/option&gt;\n         &lt;option value=\"MM-dd-yyyy\"&gt;MM-DD-YYYY&lt;/option&gt;\n       &lt;/select&gt;\n       &lt;label&gt;Timestamp Format:&lt;/label&gt;\n       &lt;select value={timestampFormat} onChange={e =&gt; setTimestampFormat(e.target.value)}&gt;\n         &lt;option value=\"yyyy-MM-dd HH:mm:ss\"&gt;YYYY-MM-DD HH:mm:ss&lt;/option&gt;\n         &lt;option value=\"ISO 8601\"&gt;ISO 8601&lt;/option&gt;\n         &lt;option value=\"Timestamp MS\"&gt;Timestamp MS&lt;/option&gt;\n       &lt;/select&gt;\n       &lt;label&gt;Partition Columns:&lt;/label&gt;\n       &lt;input\n         type=\"text\"\n         value={partitionColumns || ''}\n         onChange={e =&gt; setPartitionColumns(e.target.value)}\n       /&gt;\n       &lt;div style={{ marginTop: '1rem' }}&gt;\n         &lt;button onClick={save} disabled={saving}&gt;\n           {saving ? 'Guardando...' : 'Guardar'}\n         &lt;/button&gt;\n         &lt;button onClick={validate} disabled={validating}&gt;\n           {validating ? 'Enviando\u2026' : 'Validar'}\n         &lt;/button&gt;\n         &lt;button onClick={downloadFile}&gt;Descargar&lt;/button&gt;\n         &lt;button onClick={remove} disabled={deleting}&gt;\n           {deleting ? 'Eliminando\u2026' : 'Eliminar'}\n         &lt;/button&gt;\n       &lt;/div&gt;\n       &lt;button onClick={onClose} style={{ position: 'absolute', top: '8px', right: '8px' }}&gt;\u00d7&lt;/button&gt;\n     &lt;/div&gt;\n   &lt;/div&gt;\n );\n</code></pre> <p>};</p> <p>export default FileDetailModal;    ```</p> <pre><code>* Modal para editar par\u00e1metros, validar, descargar y eliminar configuraci\u00f3n.\n</code></pre> <ol> <li><code>LogsPage.jsx</code></li> </ol> <p>```jsx    import React, { useState, useEffect } from 'react';    import api from '../api/axiosConfig';</p> <p>const LogsPage = () =&gt; {      const [logs, setLogs] = useState([]);</p> <pre><code> const fetchLogs = async () =&gt; {\n   const res = await api.get('/files/logs');\n   setLogs(res.data);\n };\n\n useEffect(() =&gt; {\n   fetchLogs();\n }, []);\n\n return (\n   &lt;div&gt;\n     &lt;h2&gt;Logs de Validaci\u00f3n&lt;/h2&gt;\n     &lt;table&gt;\n       &lt;thead&gt;\n         &lt;tr&gt;\n           &lt;th&gt;ID&lt;/th&gt;\n           &lt;th&gt;Config ID&lt;/th&gt;\n           &lt;th&gt;Fichero&lt;/th&gt;\n           &lt;th&gt;Campo&lt;/th&gt;\n           &lt;th&gt;Entorno&lt;/th&gt;\n           &lt;th&gt;Flag&lt;/th&gt;\n           &lt;th&gt;Mensaje&lt;/th&gt;\n           &lt;th&gt;Fecha&lt;/th&gt;\n         &lt;/tr&gt;\n       &lt;/thead&gt;\n       &lt;tbody&gt;\n         {logs.map(l =&gt; (\n           &lt;tr key={l.id}&gt;\n             &lt;td&gt;{l.id}&lt;/td&gt;\n             &lt;td&gt;{l.file_config_id}&lt;/td&gt;\n             &lt;td&gt;{l.file_name}&lt;/td&gt;\n             &lt;td&gt;{l.field_name}&lt;/td&gt;\n             &lt;td&gt;{l.environment}&lt;/td&gt;\n             &lt;td&gt;{l.validation_flag}&lt;/td&gt;\n             &lt;td&gt;{l.error_message}&lt;/td&gt;\n             &lt;td&gt;{new Date(l.logged_at).toLocaleDateString('es-ES')}&lt;/td&gt;\n           &lt;/tr&gt;\n         ))}\n       &lt;/tbody&gt;\n     &lt;/table&gt;\n   &lt;/div&gt;\n );\n</code></pre> <p>};</p> <p>export default LogsPage;    ```</p> <pre><code>* Muestra los logs de validaci\u00f3n y formatea la fecha a espa\u00f1ol.\n</code></pre>"},{"location":"04-implementacion.html#44-organizacion-del-proyecto-patron","title":"4.4 Organizaci\u00f3n del proyecto. Patr\u00f3n","text":"<p>Se propone la siguiente estructura monol\u00edtica (tres carpetas principales), aunque cada componente puede colocarse en repositorios separados seg\u00fan conveniencia:</p> <pre><code>FileMonitoringSystem/\n\u251c\u2500\u2500 backend/                            \u2190 C\u00f3digo FastAPI\n\u2502   \u251c\u2500\u2500 app/\n\u2502   \u2502   \u251c\u2500\u2500 api/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 auth.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 files.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 health.py\n\u2502   \u2502   \u251c\u2500\u2500 core/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 config.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 security.py\n\u2502   \u2502   \u251c\u2500\u2500 db/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 base.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 session.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 models/\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 user.py\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 file_configuration.py\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 semantic_layer.py      \u2190 metadatos de campos\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 trigger_control.py\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 negative_flag_logs.py\n\u2502   \u2502   \u251c\u2500\u2500 services/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 file_service.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 hdfs_sync.py\n\u2502   \u2502   \u2514\u2500\u2500 schemas/\n\u2502   \u2502       \u251c\u2500\u2500 auth.py\n\u2502   \u2502       \u251c\u2500\u2500 files.py\n\u2502   \u2502       \u2514\u2500\u2500 logs.py\n\u2502   \u251c\u2500\u2500 main.py\n\u2502   \u251c\u2500\u2500 .env\n\u2502   \u251c\u2500\u2500 requirements.txt\n\u2502   \u2514\u2500\u2500 test_api.sh\n\u2502\n\u251c\u2500\u2500 frontend/                           \u2190 C\u00f3digo React + Vite\n\u2502   \u251c\u2500\u2500 public/\n\u2502   \u2502   \u2514\u2500\u2500 index.html\n\u2502   \u251c\u2500\u2500 src/\n\u2502   \u2502   \u251c\u2500\u2500 api/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 axiosConfig.js\n\u2502   \u2502   \u251c\u2500\u2500 components/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 FileDetailModal.jsx\n\u2502   \u2502   \u251c\u2500\u2500 contexts/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 AuthContext.jsx\n\u2502   \u2502   \u251c\u2500\u2500 layouts/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 MainLayout.jsx\n\u2502   \u2502   \u251c\u2500\u2500 pages/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 Dashboard.jsx\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 LoginPage.jsx\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 RegisterPage.jsx\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 LogsPage.jsx\n\u2502   \u2502   \u251c\u2500\u2500 routes/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 AppRouter.jsx\n\u2502   \u2502   \u251c\u2500\u2500 styles/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 index.css\n\u2502   \u2502   \u2514\u2500\u2500 main.jsx\n\u2502   \u251c\u2500\u2500 package.json\n\u2502   \u2514\u2500\u2500 README.md\n\u2502\n\u251c\u2500\u2500 engine/                             \u2190 Motor de validaciones Scala + Spark\n\u2502   \u251c\u2500\u2500 docker/\n\u2502   \u2502   \u251c\u2500\u2500 Dockerfile.engine\n\u2502   \u2502   \u251c\u2500\u2500 Dockerfile.superset\n\u2502   \u2502   \u2514\u2500\u2500 docker-compose.yml\n\u2502   \u251c\u2500\u2500 docs/                           \u2190 Diagramas PNG\n\u2502   \u2502   \u251c\u2500\u2500 diagrama_casos_de_uso.png\n\u2502   \u2502   \u251c\u2500\u2500 ERD.png\n\u2502   \u2502   \u251c\u2500\u2500 diagrama_clases.png\n\u2502   \u2502   \u251c\u2500\u2500 diagrama_secuencia.png\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 scripts/\n\u2502   \u2502   \u2514\u2500\u2500 rebuild_and_run.sh\n\u2502   \u251c\u2500\u2500 src/\n\u2502   \u2502   \u251c\u2500\u2500 main/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 resources/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 application.conf\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 db.properties\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 files/                \u2190 Scripts Python para generar CSV de prueba\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 scala/\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 Main.scala\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 config/\n\u2502   \u2502   \u2502       \u2502   \u251c\u2500\u2500 DbConfig.scala\n\u2502   \u2502   \u2502       \u2502   \u251c\u2500\u2500 DBConnection.scala\n\u2502   \u2502   \u2502       \u2502   \u2514\u2500\u2500 SparkSessionProvider.scala\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 models/\n\u2502   \u2502   \u2502       \u2502   \u251c\u2500\u2500 FileConfigurationCaseClass.scala\n\u2502   \u2502   \u2502       \u2502   \u2514\u2500\u2500 SemanticLayerCaseClass.scala\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 services/\n\u2502   \u2502   \u2502       \u2502   \u251c\u2500\u2500 ExecutionManager.scala\n\u2502   \u2502   \u2502       \u2502   \u2514\u2500\u2500 TriggerIdManager.scala\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 utils/\n\u2502   \u2502   \u2502       \u2502   \u251c\u2500\u2500 Reader.scala\n\u2502   \u2502   \u2502       \u2502   \u251c\u2500\u2500 Writer.scala\n\u2502   \u2502   \u2502       \u2502   \u2514\u2500\u2500 FileManager.scala\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 validators/\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 FileSentinel.scala\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 TypeValidator.scala\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 ReferentialIntegrityValidator.scala\n\u2502   \u2502   \u2502           \u2514\u2500\u2500 FunctionalValidator.scala\n\u2502   \u2502   \u251c\u2500\u2500 test/scala/                 \u2190 Pruebas unitarias (ScalaTest)\n\u2502   \u2502   \u2514\u2500\u2500 build.sbt\n\u2502   \u2514\u2500\u2500 README-cluster.md               \u2190 Gu\u00eda de cluster Hadoop/Spark/Kafka/Superset\n\u2502\n\u251c\u2500\u2500 docs/                               \u2190 Carpeta MkDocs (documentaci\u00f3n \u201clibro\u201d)\n\u2502   \u251c\u2500\u2500 01-introduccion.md\n\u2502   \u251c\u2500\u2500 02-requisitos.md\n\u2502   \u251c\u2500\u2500 03-diseno.md\n\u2502   \u251c\u2500\u2500 04-implementacion.md\n\u2502   \u251c\u2500\u2500 05-resultado.md\n\u2502   \u251c\u2500\u2500 06-conclusiones.md\n\u2502   \u2514\u2500\u2500 img/\n\u2502       \u251c\u2500\u2500 diagrama_casos_de_uso.png\n\u2502       \u251c\u2500\u2500 ERD.png\n\u2502       \u251c\u2500\u2500 diagrama_clases.png\n\u2502       \u251c\u2500\u2500 diagrama_secuencia.png\n\u2502       \u2514\u2500\u2500 ... (otros PNG)\n\u2502\n\u251c\u2500\u2500 mkdocs.yml                          \u2190 Configuraci\u00f3n de MkDocs\n\u251c\u2500\u2500 .github/\n\u2502   \u2514\u2500\u2500 workflows/\n\u2502       \u2514\u2500\u2500 mkdocs-deploy.yml           \u2190 GitHub Action para desplegar en Pages\n\u2502\n\u2514\u2500\u2500 README-cluster.md                   \u2190 Gu\u00eda de cluster (si quieres)  \n</code></pre> <ul> <li>El patr\u00f3n es feature-based: cada carpeta agrupa archivos seg\u00fan responsabilidad (API, servicios, modelos, diagramas, etc.).</li> <li>Cada componente puede versionarse por separado y luego combinarse en un monorepo si se desea.</li> </ul>"},{"location":"05-resultado.html","title":"Resultado (Manual de usuario)","text":""},{"location":"05-resultado.html#5-resultado-manual-de-usuario","title":"5. Resultado (Manual de usuario)","text":""},{"location":"05-resultado.html#51-requisitos-previos","title":"5.1 Requisitos previos","text":"<p>Requisitos comunes:</p> <ul> <li>Docker Desktop (Linux, macOS o Windows) con 160 GB libres en disco.</li> <li>Docker Compose (versi\u00f3n \u2265 1.27).</li> <li>Node.js v16+ y npm (para frontend).</li> <li>Python 3.12, pip (para backend).</li> <li>Clonar el repositorio:</li> </ul> <p><code>bash   git clone https://github.com/JuanraDAM/FileMonitoringSystem.git   cd FileMonitoringSystem</code></p>"},{"location":"05-resultado.html#511-motor-de-validaciones","title":"5.1.1 Motor de validaciones","text":"<ul> <li>Espacio en disco para contenedores Hadoop/Spark y datos HDFS.</li> <li> <p>Puertos:</p> <ul> <li>9000 (NameNode)</li> <li>9870 (NameNode UI)</li> <li>9864 (DataNode)</li> <li>7077 (Spark Master)</li> <li>8080 (Spark Master UI)</li> <li>8081, 8082 (Spark Workers UI)</li> <li>SBT instalado localmente si se desea compilar sin Docker.</li> </ul> </li> </ul>"},{"location":"05-resultado.html#512-backend-fastapi","title":"5.1.2 Backend (FastAPI)","text":"<ul> <li>Crear entorno virtual Python 3.12:</li> </ul> <p><code>bash   python3.12 -m venv backend/.venv   source backend/.venv/bin/activate   pip install --upgrade pip   pip install -r backend/requirements.txt</code> * Archivo <code>.env</code> en <code>backend/</code> con credenciales y configuraci\u00f3n de HDFS.</p>"},{"location":"05-resultado.html#513-frontend-react","title":"5.1.3 Frontend (React)","text":"<ul> <li>Desde <code>frontend/</code>:</li> </ul> <p><code>bash   cd frontend   npm install</code> * No hay variables de entorno adicionales, asume que el backend corre en <code>http://localhost:8000</code>.</p>"},{"location":"05-resultado.html#52-pasos-de-uso","title":"5.2 Pasos de uso","text":"<p>A continuaci\u00f3n se describen los pasos para levantar cada componente y usar el sistema completo.</p>"},{"location":"05-resultado.html#521-levantar-infraestructura-con-docker-compose","title":"5.2.1 Levantar infraestructura con Docker Compose","text":"<ol> <li>Ir a la carpeta del motor:</li> </ol> <p><code>bash    cd engine/docker</code></p> <ol> <li>Crear la red Docker (si no existe):</li> </ol> <p><code>bash    docker network create superset-net || true</code></p> <ol> <li>Levantar todos los servicios:</li> </ol> <p><code>bash    docker-compose up -d</code></p> <p>Esto arrancar\u00e1:</p> <pre><code>* PostgreSQL (para Superset y tablas de validaci\u00f3n).\n* Superset (opcional, UI de dashboards).\n* Zookeeper + Kafka (opcional).\n* Hadoop NameNode + DataNode.\n* Spark Master + Spark Workers.\n* Contenedor `validation-engine` (motor de validaciones).\n</code></pre> <ol> <li>Verificar estado:</li> </ol> <p><code>bash    docker-compose ps</code></p> <pre><code>* Esperar a que contenedores se vuelvan `healthy`.\n* Si HDFS arranca en safe mode, `validation-engine` ejecutar\u00e1 autom\u00e1ticamente `hdfs dfsadmin -safemode leave`.\n</code></pre>"},{"location":"05-resultado.html#522-inicializar-el-backend-fastapi","title":"5.2.2 Inicializar el backend (FastAPI)","text":"<ol> <li>Entrar a la carpeta <code>backend/</code>:</li> </ol> <p><code>bash    cd ../../backend    source .venv/bin/activate</code> 2. Ejecutar migraciones o crear tablas manualmente (no incluidas en el proyecto, usar tu propia estrategia). 3. Levantar el servidor local:</p> <p><code>bash    uvicorn main:app --reload --host 0.0.0.0 --port 8000</code></p> <pre><code>* La API estar\u00e1 disponible en `http://localhost:8000`.\n* Endpoints principales:\n\n    * `/auth/register`\n    * `/auth/login`\n    * `/files/upload`\n    * `/files/push/{file_name}`\n    * `/files/download/{file_name}`\n    * `/files/` (GET, POST, PATCH, DELETE)\n    * `/files/logs`\n</code></pre>"},{"location":"05-resultado.html#523-inicializar-el-frontend-react","title":"5.2.3 Inicializar el frontend (React)","text":"<ol> <li>En otra terminal, entrar a <code>frontend/</code>:</li> </ol> <p><code>bash    cd ../frontend</code> 2. Instalar dependencias (si no se hizo antes):</p> <p><code>bash    npm install</code> 3. Levantar el servidor de desarrollo:</p> <p><code>bash    npm run dev</code> 4. Abrir el navegador en <code>http://localhost:5173</code>.</p>"},{"location":"05-resultado.html#524-flujo-de-usuario","title":"5.2.4 Flujo de usuario","text":"<ol> <li> <p>Registro/Login</p> <ul> <li>Acceder a <code>http://localhost:5173/register</code> para crear cuenta.</li> <li>Luego <code>http://localhost:5173/login</code> para iniciar sesi\u00f3n.</li> </ul> </li> <li> <p>Dashboard</p> <ul> <li>Subir CSV: seleccionar un archivo y pulsar Subir Fichero.</li> <li>Aparecer\u00e1 alerta con el ID de configuraci\u00f3n.</li> <li> <p>La tabla mostrar\u00e1 todas las configuraciones; cada fila ofrece botones:</p> <ul> <li>Validar: env\u00eda el CSV a HDFS y lanza validaci\u00f3n en Spark.</li> <li>Detalles: abre modal para editar metadatos, validar, descargar o eliminar.</li> <li>Eliminar: borra configuraci\u00f3n de la base de datos.</li> </ul> </li> </ul> </li> <li> <p>Visualizar logs</p> <ul> <li>Acceder a <code>http://localhost:5173/logs</code> para ver resultados de validaci\u00f3n.</li> <li>La tabla mostrar\u00e1 los \u00faltimos logs, con fecha formateada a <code>DD/MM/YYYY</code>.</li> </ul> </li> <li> <p>Superset (opcional)</p> <ul> <li>Superset corre en <code>http://localhost:8088</code>.</li> <li>Iniciar sesi\u00f3n con credenciales creadas en <code>docker-compose.yml</code> (admin / 1234).</li> <li>Configurar una conexi\u00f3n a PostgreSQL (<code>superset-db:5432</code>) para leer tablas <code>trigger_control</code> y crear dashboards de calidad de datos.</li> </ul> </li> </ol>"},{"location":"05-resultado.html#53-configuracion-adicional","title":"5.3 Configuraci\u00f3n adicional","text":"<ol> <li> <p>Variables de entorno (backend)</p> <ul> <li>En <code>backend/.env</code>:</li> </ul> <p>```   # PostgreSQL   POSTGRES_USER=superset   POSTGRES_PASSWORD=superset   POSTGRES_HOST=postgres   POSTGRES_PORT=5432   POSTGRES_DB=superset</p> <p># HDFS   HDFS_HOST=hadoop-namenode   HDFS_PORT=9870   HDFS_DIR=/data/bank_accounts   HDFS_USER=hdfs   HDFS_DATANODE_HOST=hadoop-datanode   HDFS_DATANODE_PORT=9864</p> <p># Carpeta local de uploads   UPLOAD_DIR=uploaded_files</p> <p># JWT   JWT_SECRET_KEY=TuSecretoUltraSeguro123!   JWT_ALGORITHM=HS256   ACCESS_TOKEN_EXPIRE_MINUTES=60   ```</p> </li> <li> <p>Permisos en HDFS</p> <ul> <li>El contenedor <code>validation-engine</code> se encarga de ajustar permisos con <code>hdfs dfs -chmod -R 777 /data/bank_accounts</code>.</li> <li>Si se prefiere, desde cualquier contenedor con cliente Hadoop:</li> </ul> <p><code>bash   hdfs dfs -chmod -R 777 /data/bank_accounts</code></p> </li> <li> <p>Descargas y backups</p> <ul> <li>Para exportar la base de datos PostgreSQL:</li> </ul> <p><code>bash   docker exec -t superset-db pg_dumpall -c -U superset &gt; dump_$(date +%F).sql</code> * Para restaurar:</p> <p><code>bash   docker exec -i superset-db psql -U superset &lt; dump_YYYY-MM-DD.sql</code></p> </li> <li> <p>Personalizar Spark</p> <ul> <li> <p>Variables de entorno para Spark Master/Workers (en <code>docker-compose.yml</code>):</p> <ul> <li><code>SPARK_WORKER_MEMORY</code>, <code>SPARK_WORKER_CORES</code>, <code>SPARK_LOCAL_DIRS</code>.<ul> <li>Ajustar <code>spark.executor.memory</code> y <code>spark.driver.memory</code> en <code>spark-submit</code> si se procesan CSV muy grandes.</li> </ul> </li> </ul> </li> </ul> </li> </ol>"},{"location":"06-conclusiones.html","title":"Conclusiones","text":""},{"location":"06-conclusiones.html#6-conclusiones","title":"6. Conclusiones","text":""},{"location":"06-conclusiones.html#61-dificultades","title":"6.1 Dificultades","text":"<ol> <li> <p>ClusterID incompatibles (HDFS)</p> <ul> <li>Al cambiar o recrear im\u00e1genes de Hadoop, DataNode arroja error de \u201cIncompatible clusterIDs\u201d.</li> <li>Soluci\u00f3n: borrar vol\u00famenes <code>hadoop-namenode</code> y <code>hdfs-data-datanode</code> antes de levantar.</li> </ul> </li> <li> <p>Safe Mode en HDFS</p> <ul> <li>HDFS arranca en \u201csafe mode\u201d si detecta inconsistencias o falta de bloques.</li> <li>Se incorpor\u00f3 <code>hdfs dfsadmin -safemode leave</code> en el contenedor <code>validation-engine</code> para forzar la salida.</li> </ul> </li> <li> <p>Manejo de redirecci\u00f3n en WebHDFS</p> <ul> <li>WebHDFS devuelve un redirect 307 con URL del DataNode; ajustar host y puerto es cr\u00edtico.</li> <li>Se debi\u00f3 detectar y procesar manualmente el header <code>Location</code> antes de subir el contenido.</li> </ul> </li> <li> <p>Tama\u00f1o de ficheros grandes</p> <ul> <li>Al probar con CSV de varios GB, los DataNode se quedaban sin espacio y se exclu\u00edan.</li> <li>Se recomend\u00f3 montar <code>/tmp</code> de Spark Workers en vol\u00famenes dedicados (<code>docker_validation_tmp</code>) o usar <code>tmpfs</code>.</li> </ul> </li> <li> <p>Coordinaci\u00f3n entre frontend y backend (token y rutas)</p> <ul> <li>La gesti\u00f3n de 401 Unauthorized requiri\u00f3 interceptores en Axios para borrar token y redirigir a login.</li> <li>Asegurarse de que todas las peticiones protegidas incluyeran el header <code>Authorization</code>.</li> </ul> </li> <li> <p>Configuraci\u00f3n de permisos HDFS desde FastAPI</p> <ul> <li>Ejecutar comandos Hadoop desde contenedor Python (shell) generaba a veces problemas de path.</li> <li>Se opt\u00f3 por usar HTTP WebHDFS en lugar de comandos nativos en Python.</li> </ul> </li> <li> <p>SQLAlchemy Async y migraciones</p> <ul> <li>Cambios frecuentes en el modelo de datos (por ejemplo, <code>TIMESTAMP</code> a <code>TIMESTAMPTZ</code>) implicaron migraciones manuales.</li> <li>No se incluy\u00f3 un sistema de migraciones (Alembic), por lo que hubo que recrear tablas en producci\u00f3n.</li> </ul> </li> <li> <p>Paginaci\u00f3n y filtros en logs</p> <ul> <li>Al crecer el n\u00famero de logs, la consulta simple (<code>SELECT * FROM trigger_control</code>) se volv\u00eda lenta.</li> <li>Se implementaron filtros por fecha y entorno, pero falta paginaci\u00f3n y l\u00edmites por defecto.</li> </ul> </li> <li> <p>Interfaz de usuario b\u00e1sica</p> <ul> <li>Al usar CSS puro, cost\u00f3 dise\u00f1ar un estilo consistente y responsivo.</li> <li>Se recomend\u00f3 integrar un framework CSS (Tailwind, Material UI) para mejorar UX/UI.</li> </ul> </li> </ol>"},{"location":"06-conclusiones.html#62-mejoras","title":"6.2 Mejoras","text":"<ol> <li> <p>Migrar a Spark Structured Streaming</p> <ul> <li>Pasar de polling batch a procesamiento near\u2010real\u2010time con Structured Streaming y checkpoints.</li> </ul> </li> <li> <p>Orquestaci\u00f3n con Kubernetes</p> <ul> <li>Desplegar Hadoop, Spark y el motor de validaciones en un cl\u00faster Kubernetes usando Helm charts y StatefulSets.</li> <li>Facilitar elasticidad y escalado autom\u00e1tico de nodos.</li> </ul> </li> <li> <p>Monitorizaci\u00f3n con Prometheus &amp; Grafana</p> <ul> <li>Exponer m\u00e9tricas de Spark, HDFS y PostgreSQL.</li> <li>Crear dashboards de latencia, tasas de error y uso de recursos.</li> </ul> </li> <li> <p>Alertas autom\u00e1ticas</p> <ul> <li>Enviar notificaciones por correo o webhook cuando ocurran validaciones negativas (flags cr\u00edticos).</li> </ul> </li> <li> <p>Paginaci\u00f3n y filtros avanzados en frontend</p> <ul> <li>A\u00f1adir paginaci\u00f3n en listas de configuraciones y logs.</li> <li>Filtros por fecha, entorno, flag de validaci\u00f3n, nombre de fichero.</li> </ul> </li> <li> <p>Soporte para formatos adicionales</p> <ul> <li>A\u00f1adir validaci\u00f3n y lectura de CSV comprimidos, Parquet, Avro u ORC.</li> <li>Implementar validaciones basadas en schemas Avro o JSON Schema.</li> </ul> </li> <li> <p>Carga fragmentada (chunked upload)</p> <ul> <li>Para CSV muy grandes, usar multipart upload o chunking en frontend/backend para evitar timeouts.</li> </ul> </li> <li> <p>Integraci\u00f3n de notificaciones en tiempo real</p> <ul> <li>Usar WebSockets o WebPubSub para notificar al frontend sobre el estado de la validaci\u00f3n en tiempo real.</li> </ul> </li> <li> <p>Internacionalizaci\u00f3n (i18n)</p> <ul> <li>Permitir cambiar idioma en frontend (ES, EN).</li> <li>Formateo de fechas y mensajes localizados.</li> </ul> </li> <li> <p>Documentaci\u00f3n de API con Swagger/Redoc</p> <ul> <li>FastAPI ya genera documentaci\u00f3n autom\u00e1tica; extenderla con ejemplos de petici\u00f3n/respuesta y c\u00f3digo de errores.</li> </ul> </li> </ol>"},{"location":"06-conclusiones.html#anexo-glosario-de-flags","title":"Anexo: Glosario de Flags","text":"Rango Significado 30 Error de lectura (I/O, CSV mal formado) 32 Delimiter mismatch (n\u00famero de columnas esperado) 33 Header mismatch (encabezados distintos) 34 Column count per row mismatch 35 Tipo inv\u00e1lido 36 Nulo indebido 37 Longitud excedida 38 Formato texto inv\u00e1lido 39 Duplicado PK (integridad referencial) 40 Formato inv\u00e1lido (<code>account_number</code>) 41 Fuera de rango (<code>credit_score</code>) 42 Fuera de rango (<code>risk_score</code>) 43 Menor de edad (<code>date_of_birth</code>) 44 Negativo en \u201cActive\u201d (<code>balance</code>) 45 Balance \u2260 0 en \u201cClosed\u201d 46 Interest \u2260 0 en \u201cChecking\u201d 47 Overdraft inv\u00e1lido 48 Pocas transacciones en joint 49 Avg tx \u2260 0 con 0 tx 1.13, 1.21, 1.31, 1.41 Flags de validaci\u00f3n \u201ctodo OK\u201d (sin errores) 2 OK final (todos los validadores pasaron) 99 Sin configuraci\u00f3n en <code>file_configuration</code>"},{"location":"SUMMARY.html","title":"Summary","text":"<ul> <li>Introducci\u00f3n</li> <li>Especificaci\u00f3n de Requisitos</li> <li>Dise\u00f1o (Diagramas)<ul> <li>Casos de uso</li> <li>Diagrama entidad-relaci\u00f3n</li> <li>Esquema para BD no relacional</li> <li>Diagrama de clases</li> <li>Diagramas de secuencia</li> </ul> </li> <li>Implementaci\u00f3n (GIT)</li> <li>Resultado (Manual de usuario)</li> <li>Conclusiones</li> </ul>"}]}